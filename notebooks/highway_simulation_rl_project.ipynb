{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b48b6a92d8d88e15753faba58c47bae2",
     "grade": false,
     "grade_id": "cell-b0b3b6b4f891b031",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "## Graded Assessment: RL Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "\n",
    "For this reinforcement learning project, the challenge involves developing an RL agent that can proficiently navigate the HighwayEnv, a minimalist simulation environment tailored for autonomous driving decision-making. The primary objective is to construct an RL agent capable of effectively managing the simulated traffic dynamics of HighwayEnv, and subsequently, to benchmark its performance against a pre-existing agent implemented via the [Stable Baselines3](https://github.com/DLR-RM/stable-baselines3) library.\n",
    "\n",
    "### Environment Description\n",
    "**HighwayEnv** simulates the complex scenario where autonomous vehicles are required to navigate traffic on a multi-lane highway. The overarching goal for these agents includes optimizing travel time, executing safe lane changes, maintaining appropriate speeds, and adhering to traffic regulations to avoid collisions.\n",
    "\n",
    "### State Space (Observations)\n",
    "The state space comprises:\n",
    "- **Position and Velocity of the Agent Vehicle:** Longitudinal and lateral positions and velocities are critical for determining the agent's current and projected states.\n",
    "- **Position and Velocity of Other Vehicles:** The relative positions and velocities of nearby vehicles are essential for spatial awareness and collision avoidance.\n",
    "- **Road Geometry:** This includes the number of lanes and lane markings, which are crucial for proper lane adherence and navigation.\n",
    "\n",
    "### Action Space\n",
    "The agent can take several discrete actions:\n",
    "- **Accelerate:** Increase the vehicle's speed.\n",
    "- **Decelerate:** Reduce the vehicle's speed.\n",
    "- **Change Lane to the Left/Right:** Shift to an adjacent left or right lane if it is safe and possible.\n",
    "- **Maintain Current State:** Continue with the present speed and lane.\n",
    "\n",
    "### Transition Dynamics\n",
    "The dynamics of this environment are governed by both the physical laws of vehicle motion and the programmed behavior of other simulated vehicles, which follow basic traffic rules and patterns of lane adherence.\n",
    "\n",
    "### Reward Function\n",
    "The reward system is designed to promote safety, efficiency, comfort, and lane adherence:\n",
    "- **Safety:** Imposes penalties for near-misses and collisions.\n",
    "- **Efficiency:** Rewards are given for maintaining optimal speeds and minimizing travel time to goals.\n",
    "- **Comfort:** Discourages excessive and abrupt vehicular maneuvers.\n",
    "- **Lane Adherence:** Encourages maintaining lane discipline unless overtaking or avoiding an obstacle.\n",
    "\n",
    "### Hypothesis\n",
    "The hypothesis to be tested is whether a custom-implemented Deep Q-Network (DQN) can outperform an established RL agent from the Stable Baselines3 library in terms of cumulative rewards. This will be measured through the agent’s ability to adapt to varying traffic densities and complexities, ensuring both efficient and safe navigation.\n",
    "\n",
    "### Objectives for the DQN Implementation\n",
    "The project aims to implement a DQN that learns an optimal policy based on the defined state and action spaces, adhering to the specified transition dynamics and reward structure. The key objectives for the DQN agent include:\n",
    "- Efficiently navigating through traffic without predefined rules for specific situations.\n",
    "- Developing a balanced driving policy that optimizes speed, safety, and comfort.\n",
    "- Demonstrating superior adaptability and performance in diverse traffic scenarios when compared to the existing solution.\n",
    "\n",
    "This structured approach lays the groundwork for developing and evaluating the reinforcement learning model, focusing on fostering an autonomous driving strategy that can effectively operate within the realistic confines of highway traffic conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Reinforcement Learning (RL) has emerged as a powerful method for solving decision-making problems where an agent learns to act in an environment by performing actions and receiving feedback through rewards. In the context of autonomous driving, particularly in complex environments like those simulated by HighwayEnv, RL can be leveraged to enable vehicles to make intelligent decisions dynamically.\n",
    "\n",
    "**Deep Q-Networks (DQN)**, introduced by Mnih et al. (2015), have been pivotal in applying RL to environments with high-dimensional state spaces, such as video games and, relevantly, driving simulations (Mnih et al., 2015). DQN integrates deep neural networks with Q-learning, where the network approximates the Q-value function. The Q-value function quantifies the expected utility of taking a given action in a particular state, followed by following a certain policy. **Strengths** of DQN include its ability to handle environments with large state and action spaces and its robustness in learning stable policies in complex scenarios. However, **weaknesses** include its sample inefficiency—often requiring numerous interactions with the environment, which can be computationally expensive—and its tendency to overestimate Q-values leading to suboptimal policy decisions.\n",
    "\n",
    "**Proximal Policy Optimization (PPO)** and **Trust Region Policy Optimization (TRPO)** are policy gradient methods that optimize the policy directly. These methods are noted for their stability and efficiency, which come from limiting the steps in policy space to avoid destructive large updates (Schulman et al., 2015; 2017). For autonomous driving, the **strength** of these methods lies in their continuous action space handling, making them suitable for controlling the nuanced actions of a vehicle. The **weakness**, however, is that they can be sensitive to hyperparameter settings and require careful tuning to achieve the best performance.\n",
    "\n",
    "In autonomous driving simulations like those provided by HighwayEnv, several studies have demonstrated the effectiveness of these methods. For example, RL has been used to successfully navigate complex traffic scenarios, demonstrating significant potential in achieving human-like driving capabilities (Dosovitskiy et al., 2017). These environments often simulate realistic traffic conditions and provide a benchmark for evaluating different RL methods, including DQN and PPO, in terms of their ability to learn safe and efficient driving policies.\n",
    "\n",
    "Comparative studies, such as those by Liang et al. (2018), have shown that while DQN can effectively learn policy for discrete action spaces in driving simulations, methods like PPO tend to outperform in terms of achieving smoother control and handling continuous action spaces, which are critical in real-world driving scenarios.\n",
    "\n",
    "In summary, reinforcement learning offers significant promise for developing intelligent autonomous driving systems capable of operating in complex, dynamic environments. Both value-based methods like DQN and policy-based methods like PPO have their merits and limitations, making them suitable for different aspects of the driving problem. Continuous advancements in RL methods and computational resources are likely to further enhance their applicability and performance in autonomous driving tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Highway Environment\n",
    "\n",
    "The highway-env environment introduces an exciting domain focused on autonomous driving and decision-making in traffic scenarios. This environment is designed for experimenting with various aspects of vehicle behavior, such as lane following, overtaking, and navigating through intersections, making it rich in learning opportunities for reinforcement learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "env = gym.make('highway-fast-v0', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "# To visualise a initial/idle state\n",
    "action = env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "plt.imshow(env.render())\n",
    "plt.show()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Highway Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring and understanding the environment by analysing the observation and action spaces, the dynamics of the environment, and the reward structure. Here’s how you can start exploring highway-env:\n",
    "\n",
    "#### Understanding the Observation Space:\n",
    "\n",
    "The observation space in highway-env can vary based on the configuration but typically includes the positions, velocities, and other attributes of nearby vehicles relative to the controlled vehicle. Understanding this space is crucial for designing your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (5, 5), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Action Space:\n",
    "Actions in highway-env usually involve discrete decisions like changing lanes, accelerating, or braking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Structure Exploration\n",
    "\n",
    "Understanding how rewards are assigned is crucial for designing your RL model. Perform actions and progress through the game to see what actions increase the score, how much reward is given for different achievements, and identify if there are any penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward received:  0.7999999999999999\n",
      "Reward received:  0.7999999999999999\n",
      "Reward received:  0.8327113850086175\n",
      "Reward received:  0.7175567607980314\n",
      "Reward received:  0.7023121980100128\n",
      "Reward received:  0.8160795220498324\n",
      "Reward received:  0.8637845794187456\n",
      "Reward received:  0.7505909305552\n",
      "Reward received:  0.7013873830656888\n",
      "Reward received:  0.7002958141970411\n",
      "Reward received:  0.7333333333333334\n",
      "Reward received:  0.849111821699985\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.06666666666666665\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n",
      "Reward received:  0.03333333333333336\n"
     ]
    }
   ],
   "source": [
    "# Example: Perform an action and observe the reward\n",
    "env.reset()\n",
    "for _ in range(50):\n",
    "    _, reward, _, _, _ = env.step(env.action_space.sample())\n",
    "    print(\"Reward received: \", reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Environment - Simulation:\n",
    "\n",
    "You can visualize the environment in a Jupyter notebook or Python script to understand the dynamics visually. highway-env supports rendering directly to a Jupyter notebook using its render method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def generate_gif(frames):\n",
    "    # Save the captured frames as a GIF\n",
    "    gif_path = 'highway_simulation.gif'\n",
    "    imageio.mimsave(gif_path, frames, fps=10)  # fps controls the speed of the animation\n",
    "\n",
    "    # Display the GIF in the notebook\n",
    "    display(Image(filename=gif_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple environment demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/gif": "R0lGODlhWAKWAIIAAP///5P//0n/AGTI/zLIAGRkZDw8PAAAACH5BAAKAAAALAAAAABYApYAAAj/AAsIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKHEmypMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0qNGjSJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaNOqXcu2rdu3cOPKnUu3rt27ePPq3cu3r9+/gAMLHky4sOHDiBMrXsy4sePHkCNLnky5suXLmDNr3sy5s+fPoEOLHk26tOnTqFOrXs26tevXsGPLnk27tu3buHPr3s27t+/fwIMLH068uPHjyJMrX868ufPn0KNLn069uvXr2LNr3869u/fv4MOL/x9Pvrz58+jTq1/Pvr379/Djy59Pv779+/jz69/Pv7///wAGKOCABBZo4IEIJqjgggw26OCDECIFwIQUVmjhhRhmqOGGHHbo4YcghijiiCSWaOKJKKao4oostujiizDGKOOMFkZo44045qjjjjz2CJsBQAYp5JBEEslWkUgmGaSPQSm5ZFMGDCDllFRWaaWUAQRgwJFXdtllllsyGZOTQRJg5ploCiBAmExF6eWbU4LJJZxwyikmTAagqeeefBKgJptLuUmnl3aqJeigVxZ6Z0t59umonn86dSiiVwKK1qSUVmnpoio1+uinZm6KFKaZTilqWaSWOsCpnJrkKaiPsv9aVKqlyioWrZna2qpIr8LqqK5NqlqlopcKSyWxu6LUq69prgmlsXFqOSe0yCbrKrN9RtomtFhKuxaug1ZrLUnLYuuns22SqS6wX62r7ridwvouvDuuS++9+Oar77789uvvYiEaNKLAIhIM4kEFG/yhwgsXNLDDATPcIcIRE/SwxQljXPFAGWvcsMcfc9yxQBeLfLDEHFJ8MsgeopwyxCuz/O/MNNds880456zzzjz37PPPQAct9NBEF2300UgnrfTSTDft9NNQRy311FRXbfXVWKu0MckjF1Ay1zGbHLLYLcMcttddd4322WuP3bbbX79dNssvmz032XeDfXbcavehvbXcE9udN+Abqkxh1ognrvjijDfueG3ukvn4d+DSKe7k1lVep7eYb6f5m5d3Pt3ncLIrenOkv2n66cul7uXqrCfnupWhx+7c7MNybjt1uB+r++7SRe4k8MQXb/zxyCev/PKY0+j889BHL/301Fdv/fXYZ689icx37/334Icv/vjkl2/++einr/767Lfv/vvwxy///PTXb//9+Oev//74BgQAIfkEAQoABwAsAAA+AFgCRACC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4osCKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrQoygNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdy7at27dw48qdS7eu3bt48+rdy7ev37+AAwseTLjw3o2G5SJOzLix48dRG0J2utixgcuYM2vevDmyxsmgQ4uOW2AqZ80HDGjFzLdyYwMDYsueTbt27AABVEN1Pbq3799dC+jWSuCAgOFXYQ9gjZd3YuW2o9fGjbypc+DYs2tfKpxr8eNZocf/Zl73emHx0tNT95xxu/v32btzFQAeK3rZl8t/fnw/vfTqTJkH34AEGiafdwBK1d9s+Sm2n2X+RTgeexgVaOGFhR341oK0kdeWgINxKCF+FF6E4YkoAkdAccmJ2GGDbIEomIsRrrfbgynmqOOANNoGY1oyBtajermVaNGOSCaZ3ZA+JjhWkIAxGZ2NT0Gp5JVYjrXillwSQJ99Utb2Y5ZdnWbmmWSmqaZlZ45J1WUj2pabk2vWaeedeIIZ52xU5unnn4AGilmcfQZq6KGIYjnoiHQm6uijkL4Hp3+NRmpplhAFGJGmmXLXqacPWfdpUhJxGiqoDol60KTR6bYpqqnC/yqZrAyZOiutCtl6K6mvKlUqrgmpGiuwCOlaK7HFImuQsAKtxFRNz9IU7UzTxtSUtNXClK22S0HbrbSL3janS9dS+625SnmbLrbrylSuu9uSey687dKbFLv1cjuvvvfii5S6/b50KZKLFjrwwQgnDNplBivs8MMQ/+VmxBRXbPHFGGes8cYcd+zxxyCHLPLIJJds8skop6zyyiy37PLLMMcs88w012zzzTjnrPPOPPfs889ABy300EQXbfTRSCet9NJMN+3001BH7daoSP3qa6+8nqpsQcweezXVB1id9bBfkz32rlVjnTbYYq9ttttoh6223GzPPTfdb+Mdd9t67+oNdt/O7svvv/4eADDh9gZsbbwtMd644AJDLm++kUvOkuOXW5455ZUrPjjii2uuEuaBcz65552jfrrqj5veOuuvgx671LTXbvvtuOeu++68956jlb4Hf2ebxGcGvPDIpxnmdLkdn/zzSi5PG3XOQ299jtLz2TyO13dPZvYdVu/9+Dzu+Z/45KevHfgMoq/++76xPwD13MNvP4Hy09/e/fzjbz7zBnBf/wZomOIZkIAITKACF8jABjrwgRCMoARzN5IKWvCCGMygBjfIwQ568IMgDOFEjELCEprwhChMoQpXyMIWuvCFMLRJQAAAIfkEAQoABwAsAAA/AFgCRQCC////k////5OTZMj//2RkZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4oseKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrQoSgBIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNolxpdy7at27dw48qdS7eu3bt48+rdy7ev37+AAwseTLiw4cOIZ25MzLix48eQVw6MTDnl4sqYM2t+TKCz58+gQ3cWIACnwc1xDahezbq1a9cHLqOeTbs2XtG4cZO+mdC2UQMDggsfTrx48AABDMTW6Lu58+dCc0v/vNvmQug/gRvfbhy5ctnYw4v/H79yuvnO1huSx6mdu/vjyZdnXE+/PvTz5tM/tB+z/fv33zHH34AEYobfdDVRVOBK/v3HXYDzLSjhhIYdqFtpNFlEYUkNOmgchBhtKOKIe1koWnWKXURhhx4K5518IZIo44xxmRgaijIJWCCLLQ7wIng0BikkZjryx2OLLw6p5JIDxjjga1BGuRqTVFZJXkVWZqnlluJNxOWXYIa52X5ilmnmmYeph+aa9iFkWURvQhQnmSfBOaeaKNlZp54mSXSnehr+eV2efJbkJ6FyIuqQSoUamuiej/bZaKMw4gnpoopieqmmkjLEZpcHfSoqeValhJWppaJalUpXqbrqUa3C/5rqSbHKShVBJZ1qK1Wu8rqrVKzOSquwudZqkq7DvvprVMEqe6yxxRJ7ALLPOlttUqNeWUC23Hbr7bfghivuuOSWa+656Kar7rrsViblu7C1exJr8tYb3pEeJmmvAZ61Zu+/tOHroL718iuavwAnDJnA/xEsr8HSxavwxIQx7KBy/0KMn8QUd6yXxf9hXLCNoNHr8cl1gQxgxiSDRprIKMf8lsrbOcyuxiS/LPPOM/fYXXz7tuyZzjwX/ZvPxdm8Ls4tw2z009nBK7XTS7MmNAFUQ6311kW1ZmLWXIctNlBeXwj22GinzVPZowlwttpwv+VmppY6GqnddHbKqd514//d90KuuU23p4MPuinhhfeWuOKHI87333f77fjjhlNeueSTV5q55pdjrpCgnR/aeKgyTxn36XNJS220vi4LVbOtXxs767NPC63t1tIOrOuv8/5Ur1MBz6zwvfveFPHFyx688UzBvjzzakEffbK5r4577dcjhfr23Hfv/ffghw8wkOKXfzL55qc/7tRTRkmp+vB7S/MAKXn3fvz4izp//cndn///Z9ofSuwXOQAa0EwCTIkB/HfABmopgShZYAEdSMEsQXBeDKygBoV0wQMQMG8bDOGS5pe0AHDucyJMoZJISBylqfCFD2Tfu2BIwxra8IY4zKEOd8hD8I3kh0AMohAoh0jEIhrxiEhMohKX6KUeOvFMaYmiFKdIxSpa8YpYzKIWt8jFLmYlIAAh+QQBCgAHACwAAEAAWAJEAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiixYoKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtCjKA0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltq3bt3Djyp1Lt67du3jz6t3Lt6/fv4ADCx5MuLDhw4gT9zXAuLHjx5AhK548mYDly5gza7YsQADlz0x9gh7d1cCA06hTq159OkAAA6Rj/91Mm3Zn2Z9F494t1TTr36xdw+ZNnG7t45hvFz+se7nzA76BS2/9+rn1tcizW74+uDn33dGnT/8f/r18WO3Zzf/1rn50ePHAybefjxU9cvp72eNX/B4+a/n7BfiUfbZ5JqBd+h1YWH/+oSacghAuReBmykUYV4IWAsZggwM8mCGEE2pW4YdtYUgiXxs26OGJLLaol4ku3hXZjDQ2FuONOMIFY4489ujjj0AGKeSQRBZp5JFIJqnkkkw26eSTUEYp5ZSyRcSURFdauRSWW0LUlJZdPpSll2GKWaZDY6KZZkNrsnmmm0pxGSeZby70JZ1JyZknmHuaWadCd/rZp6BI6VnoQDOFRpOiMjW1KKMxQRrpUjVJCpOlLjmaKKWPKlUpp5t6GqqojWLakqmngloqqZOqeimqK8H/Gqurr7LaalKd2loAlbz26uuvwAYr7LDEFmvssciuVeOykiXr7LMupujfitBGuOOxJwEpLXzUVqvgtcaylOO24nXrrYDgFnvrh+TCB+C5AaZLrE0Ztiveu/DiJ++wOx1o73j5fttTvj/t9+9v5gY8377BFozfwcFVp3C8A8PLMHEQr5bwxOZd/KvH4DErMr4clwdyryeXrHJZKU/p8Mowx6yYTjLXbDNiud6s886BvcTzz0ADNmvQRBPG56F4Ik3oAYYynbTScM65dNNUH+300lerKfXUVleNtdVZRw212GGTDfbZT3vNtUCjIvWprpnKmpKmq+Ka8wFv29023nvzz12323fnDfjed/u97uCHC2544n0XHvjjhEN++OK7Fm355ZhnrvnmnHfu+edLjiwy6KRnnrFqG+/Wcums03V6aqnjtnrrtL/1uoMSPzd77byrdXtqJKtece/EL8ZhfNftXvzyYf2OWvCyD8/89K4fr3HuzilP/fZYOR+7bNpzL/5U3mO/XPjjp++U6MwmL7368PuKfvz012///fjnr//+/Pfv//8wG4kAB0jAAhrwgAhMoAIXyMAGOnAiRomgBCdIwQpa8IIYzKAGN8jBDtokIAAh+QQBCgAHACwAACsAWAJYAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiix4oKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtKjRo0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltq3bsgbiyp1Lt27dt3jz6t3Lt+9NAwMCCx5MuHDgAAEM+F3MuLHjx18BG55sGLFiyJgza97MWadkyqAPJ+5MurTp048/hw59GbXr17Bjg1W9mnJr2bhz695tlHZtw7d5Cx9OvDhL378FWzbOvLnz3MiTD1j+vLr165ijJ6eOvbv372ztiv8fLxe8+fPo06tfz769+/fw48ufT7++/fv48+sfWqC///8ABijggAQWaOCBCCao4IIMNujggxBGKOGEFFZo4YUYZqjhhhx2CKBAKUWkkogoSRQiiSeZWCJEI7J44kMvwriiiynSWKOMMzoUo4458ngjjiah+GNDLQJZkopDEtkjQ0X6mORCOyq535RUVmnllVhmiR95c2np5ZfEESDmmGSOCeaZRFmIZntcttnlmoypCad62v3G3Zx8yYnneXXWdueeeekJ6Hd9rvbnoG4JimhQBhCQWaG1BbcoW4pO6lOjOxlQAFmQriappWlVCmqmjuIkwAGaciqdbaO2JWqrOGH/aiqqm47VKWWHwlrWq7rWpFibLZ2aqq2rFpZrr2LxiuxMCMJUq1i3Tnbssl8pS61sbmZ77bbcduvtt+CGK+645JZr7rnopqtuqAum1KC7DMLbLkrxypugSvWe9C69+Zq0r77z8qugvfcKPLDBBQN8MMIH4huwv/2W9C/ECyucsMXNMtywxgbaC+KSUEYZ8pMjBynkkScfkLLKNppsJMstw/zyyjTHjKTLM6dc88syS4mzkyjbHHPPTIqskNFHr6v00kw37fTTUEcttVDWTm31fFVfla2bV3fNVdZWRVvZaF6XfRXYVYltLNlmty0V2lSpTdi0btd9FNxTyV3Yp3b3/10U3lLpTRjffhcOFOBRCT4Y4YY3vhPiUCk+HduOV94T5E9JTrflnMuEuVOaU9756LptDSzpqKeu+uqst+7667DHLvvsxUV8wMQS24777Q9TfLHvGWMcPPDD59477xUT3zHHBRJcPPLJQ/+89BszP6Dz1Su/vPDbc9889t97H7724xtfwEjop6/++uy37/778Mcv//z0T0T7/fjnr//+/Pe/7uf+C+BTACjAAiqFgAZM4N8qpMAGZgWBDowgTyAowQrehIIWzKDnGKjBDiIFgx4MIfgcJMISBgWEJkyhClfIwha68IUwjKEMZ0jD83johjjMoQ53yMMe+vCHQAyiEAWH6KCAAAAh+QQBCgAHACwAACoAWAJEAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiixYoKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtCjKA0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltq3bt3Djyp1Lt67du3jz6t3Lt6/fv4ADCx5MuLDhw4gTXzXAuLHjx5AhK55MubLly5gzHzAwoLPnz6BDdw4Q4ABOzahTq17NmjVn0bBBIyVt4HTr27hz694t93Xs37MD1L7Ju7jx48iTP/X9G7bS4TaVS59Ovbpl5s1DP7dtvbv37+DnYv/P/nk78fDo06tfv3U8+c7BoddkT7++ffru38fnfr+////H5UfefucBaOCBCGoW2YKSPcZfghBGKKFqPk1o4YUYZqjhhhx26OGHIIYo4ogklmjiiSimqKJmBzElkYsRwfhQUzHK6JCNNy71oo4Q0dgjjgwBGSSPPyq1o5FFIjkjkUsq2WRSR0KZpJRPUpmjk1damSVSNTK5UElM0dSUmEvNVyaZSpmZ5kxjshmmm2vCmZSac8pZp0xt4nmmnaaheaeeccaUp6BvAvonoYEieuhLg8LUKKN7GrripJRWaumlmGaqaaQtberpp/8x6FhfopYqGaiopoqcgO+R1ueDqsb/KqtfrJJHm06z5qqrXrVmd2tOuwYrbFy9NoeUfNENqyyIBRigWLHAbYbrstRy2KxXBDj7FrSxHTttteBeeO1WBBwggLZucQvbr7CG6+6B42pV7rlwqSsauwW+q6+BBRxAwL8AB/wvVPOi25a9oeGb7L4Mh2rqVAazZWqpmzH2bcMYZ4xVhRp37PHHIIcs8sgkl2zyySinrPLKLF/VopcNCbmQzAn5WCWXUx4QJc45d4nlkDDPHLTQPwOtZcxD10wzQksznbTSRRN9tNRTK2Tzljr7zPPNWm/9Zb+cQhq2S4VK+qrZdCLl56Jkl+3o2G0n+rbccdPdqdtis1232mufvK0o32b7nbfeLOE9uOCH94343XAX3rjjLUcu+eSUV2755ZhnjtjEnGvuuYoIg+YqT5+XDmDonymcr+mss4e6Z6ov3Prs6b3+GbKr0657d7Z7hrvsuwdPXe+d/Z628Miv+l5ssR+f/PO79d684tBX35r0wpFu/fa4cU6x8dRzL351HI9v/vnop6/++uy37/778Md/2Uj012///fjnr//+/Pfv//8AnIhRBkjAAhrwgAhMoAIXyMAGOvCBNgkIACH5BAEKAAcALAAAFQBYAlgAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AAEIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKLHigpMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0qNGjSJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaNOqXcu2rVueBuLKnUu3bt23ePPq3cu3700DAwILHky4cOAAAQz4Xcy4sePHXwEbnmwYsWLImDNr3sxZp2TKoA8n7ky6tOnTjz+HDn0ZtevXsGODVb2acmvZuHPr3m2Udm3Dt3kLH068OEvfvwVbNs68ufPcyJMPWP68uvXrmKMnp469u/fvbO2K/x8vF7z58+jTq1/Pvr379/Djy59Pv779+/jz62dboL///wAGKOCABBZo4IEIJqjgggw26OCDEEYo4YQUVmjhhRhmqOGGHHYIoEMpSRRiRCNCVCKIJzaUooooidgiiS8+pBKMMaJ4kos30miSjjuauOJCPwJZo409+pijjEEmNKORRSJ5pJNNErTflFRWaeWVWGap5ZZcdunll85ZCOZ35JV515iaiXmVmWyiGZV2v3Hn5mNqWgVnbQHM+dSdq8mpJ2N1VsVnaHn+ydSgoPlpaF+BUoUoZYUumtSjoAUn6V6NTkXpZJdOKp1tnfqVqVSbGhbqUaUCdypfo775qamrEv+V6mCKxspfhVjNqpytQ+kqmqW8qtUqVL5OF2xQxdZ67FnD7vkqYZEuCxebbUpbJbVlWqvtttx26+234IYr7rjklmvuudUJmFKD6zLYroIquftugvPSixK79y4Yr771Htivv/nyexK+AwtcMLwBI3ywwiYR3LDBDzMcsb0LUzyxxSXJm7CBRJaEY5QsPtnxATx6zKTJJ5Nc8scoj6wylCALKXLIMctcs5JJIrQkzC/zzHLPI5cMNEM7B73y0Qahq/TSTDft9JTUPi11pwRUbfXVVmvV7NRc47e1U9ia2fXYwuKa67O0jkb22mV93VSyarMtN1huH4q2cnHPzbQBBDD/a/aadw8GrN7m8r2TAQU0VfdSxQY2OOHkGo6TAAcgrvjfdgYu2OOQiyv5TZRbztTiSsHNeefgKiY2S6EnPjrmgmquLOriIgiT60uR7mngs9Pu+3NhZ/v78MQXb/zxyCev/PLMN+/880Gpu7Ht0wNcsfXXG7ivxAc4nLHGF2Mfvvb/Fli++dWTn73646OfPoHnw/++/Ou73z799Q+4Pcbdg/89xP3jXgAPNJICGvCACEygAhfIwAY68IEQjOBEoEfBClrwghjMoAanprsNelAsHfygCLsSwhGaECslPKEKp5LCFbrQKS18oQyTEsMZ2pAoNbyhDn+Swx36UCc9/KEQLWsSxCEa8YhITKISl8jEJjrxiVCMYhI9RMUqWvGKWMyiFrfIxS568YtgdFBAAAAh+QQBCgAHACwAABQAWAJEAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiixYoKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtCjKA0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltq3bt3Djyp1Lt67du3jz6t3Lt6/fv4ADCx5MuLDhw4jfGljMuLHjx48TS55MubLly5ifGhjAubPnz6A5BwhwQGfm06hTq17NWunm0LBDjzZgurXt27hz64b7OrZv0QFo59xNvLjx48ib9v4dG6lwnMmjS59OnfJy5qGd167Ovbv373GvY///rH04+PPo06vPKn5859nb18ufT/98e/cD4Juvz7+//+L3uacfdP8VaOCBmUGmoIIHMBYfghBGKKFqPk1o4YUYZqjhhhx26OGHIIYo4ogklmjiiSimqKKBETXVIlMvKiURjDEmNeNSNdoIkYs74pjjATfK2KOPD/FYJJFHCjmkjksi9SOQTUIZZZBMJlllQ0Y6lCWWRMbEVE1f0hTmTGN6WSZMZ6K5FJhritmmTE25+aaZSrFZp5xJ4ZknmWm61Kefc9K5J593wvknS3ESOqihhTK66EkrRirppJRWaumlmGaq6aaXLugpg5yGqtmnpDImql8BjjfgTqeSmCp2szn/xVOrW73K3KoP0tqhrb/Fmmiuuk7Fq2+47hfsrvjJFpysrB5r1bC/PdessxxC65sBzAJLrVPWxiattttK2C1s2P5qbLiaJUtuhehmOG5o5R6qaLvKqftZsQTSa+G7nvkqr6P6LsXve8GxG7C49va7rLn5HuxawgR/e67DBg4MXLyB3kSxwKWWavDGFXdMarYTg+zVxyZTOmvKLLfs8sswxyzzzDTXbPPNOOdM6ZM8T/kjlU7+HKWUWtJoZdA+J100klwyzZDRRxO99JVNU73Qlk87nbWSUQMt9dZcV221QkYLipSdj6rZqNmlzXu222i/DbeebQMsN9t1sx133nq76833SwwDnrHga6udNuGHtxQ4oIMr3rjjhSN+N6Q6V2755ZhnLiremncebqnyiTyy56RzanF+pPVU+uoanj7a3xqzLvuErqe+8uy4H3g6Z7DblHvpBWBcMcSe9b7375cH7xUBwh+3+wDG04285cpvRcABAjRvXO3R+z29ztVrdX3203Gv+veZF3AAAey37z77UI2vPYDEo9693ejrDLpU8xMnuqf341z+BighlBHwgAhMoAIXyMAGOvCBEIygBCfIl5FY8IIYzKAGN8jBDnrwgyAMoQgnYpQSmvCEKEyhClfIwha68IUwjKFNAgIAIfkEAQoABwAsAAAUAFgCUwCC////k////5OTZMj//2RkZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4oseKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrSo0aNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdy7at25Ib38qdS7eu3bs34+Ldy7ev379e9QIeTLiw4cM5BSNezLixY76KH0ueTLlyYI2WM2vezDlp5M6gQ4sevfIz6dOoU0s2rbq169d7WcOeTbu2WdmhAejezbu379/AgwsfTry48ePIkytfzry58+fQo0ufTr269evYs2vvbbu79+/gw4v/H0++vPnz6NOrX8++vfv3pw3In0+/vn373iOq1J+S/0mJ/flnEoAoCTggRPshWKCBBxD4n4ILPpSghBFS+CCEB2IIl4YbNmTAACCGKOKIJIIYQAAGVOhQgBwy2CCHL1qYIXw01ljShyXmWOKJKdro44+q4ajjkCaiCOSRSIImJJFE9pjkk1A+tiSTOjoZZY24XUnWlFSWaKVYWWoJVZhigsVllyHyeBtmZWJFZptdnYnmAGqW9SacnrGJp1lyolknWXfuaVSgglp136GIzrdmRoVORWij6z0K6U+STnpepZbuhGmm421qaaKgpkifRZwe6emkBhCg6qqstuqqqgII/2AAqaX+eCqkqb6q66uxxjhRrbbqKZRyKRGLUnIqIVsscssye2xyue4qLayymqTss8c1my22255krLfOcltcsuFae6255ZZ0LrrGkdstu+2KO668xGk7L7ZiRjvttF8C6++/Qum77679AmzwwTgJPPCrBSPs8MMuKbzwqrE2DPHFGB8g8cQEVJyxe6GGLJ9VLpbcIoMOZihjhw5tPLHHKjfE4sq+rqgiQxPafPNCM+scs8w78xx0QjkDfSHNKbPsc59d8ggjygLlK3KoH7fHNJV/Vt2UolqjdzWTWXddVH0wi13e11RabLZO9LVa9trioc2k2nD3TBBK0vbK2K0Hy//dZN0yFVQfx43xbbDfOYYNOEsFtM0xq4ULu7hJiO9o5OQvPa5r5IxiflLlJCruOd6au10tYoYDPDXVo7NUuul0QyZ567QP9TqrbxeWeu28/+fywCPHHtvsvRef2O8Mjxr1YrsbX3vjwM+H0N7EO2994AfsevT0zFd/fVjfwnuv+MO5Gy+9wtk7vrrrRltf+ugHZ/76B4TPfrr1r5v/u+TDHz9w6qvX/wA4QN/MT4AF5E4CFQgu/Nlvf+dr4J7o870KWvCCGMygBjfIwQ56UC6rA9UHFzWSEprwhChMoQoVAroROW2FMIyhDGdIQ++NcGtzspzwbshDybRQRKLroRBeKfPDEe1wiEgsTBFFdMQkOtEvSwxRE59IxbtEMYhVzGJfrng5LXrxL1yc4hfHiJYQJoqMaEyjGtfIxja68Y1wjKMc50jHOo5mO3jMox73yMc++vGPgAykIAdJyOUEBAAh+QQBCgAHACwAACQAWAJJAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wALCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiix4oKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtKjRo0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltq3ajW6vAphLt67du3jz6t3Lt6/fv4ADCx5MuLDhw4gTK17MuLHjx5AjS55sN67ly5gzb4WrubPnz6BDv9UourTp06hTH+WsurXr17Bfs45Nu7bt22ln497Nu7fvp7p/Cx9OvHjN4MaTK18uHDnz59Cjp3Yuvbr169iza9/Ovbv37+DDi/8fz5M6+fPo07c0r769e/Ls38svLjFlfZQRVea3D5F/f/wjJWTSfgA+5J+BBSJ40n0L/pdgQ/o5OCCBE0pYEoUVOhShghlq+CCEHzJ0IIgFzjdTfCamqCJzKK7oImgGxPiiTS3OaKNbMRoQQAAG3HgiaT4GiVmOAxS5Y49CvlRjkkwesORLRBYp5QBHrgRASoJhmSVKg2kJmEpbchmYl4AZ1KWYY6L5F5hpnnSmm2Ga9KacbcL5JZl+4Zmnmmvy2Rebd/rJl5mCDURnnVFOqegABtTVZIgUPSrpekDelOiiiyI5aVCGbuqpk5XOdCmmpGr66U0InbrpkqOSimmVqv7/qFCskrbYqquv8kjrATn2KuNJvgZr6q5cCWvssTmyZSuuzE7J47CeGkDAtNRWa+210wogALFfGdAss7COlpFM3n5b6q+0Sovtuthqy61X5ZqbK7Rm1RivvIwm+y67/Fbr7rvF4juvsqG6dO+3+gJ8QL8MT2tAAQpndbDAUtJbFqsIoxvxwg33+/DGV01MMaPLiVyxxiBz3DG7H6dMlckCW/ybyAm7XJK6K1/rLsQ2SwWzueEmF2/NPd+cc7sCyFx0Uj9/G7RxvS6tEs5HU6ut0lIb1XSzTxeHddE9Hgsssl9nPRTZaEetIoMdktj2Qht6CKmAc9PdoIUTjSii3nDX/52q3wfx3ffdchPu9tuzAm6Q4IkbvrfiBMV9OKgWUs7hhRhaPjnln0Zu9ueghy766KSXbvrpqKdO7JOqx3WloHvp+SfssdOeF6B92o6X7HzxXrudgR6aO/B7Eu8XoXWWNKfyyTMfvPDFQx+9888fEKf0vft+u+67c3+X9tu3jhnr4pefIvnmp+8e+uq3fx777scPXkW+8pq2sPLnzx39JY1spK76C+B1+HcA/1EJgAJMIHQIaMCuKfCBxKlIAQ1YMQhaMDxbQ9gFN9idDDarbBwMIdQoeEAQivCEMyOhA1HIQuJ40FUrbKEMe3O/tM2wKQHKoQ53yMMe+vCHQAyiEC6HSMQikuSGSEyiEpfIxCY68YlQjKIUVUSZKlrxiljMoha3yMUuevGLYAwjYQICACH5BAEKAAcALAAAFgBYAlcAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AAEIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKLHigpMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0qNGjSJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaNOqXcu2rdu3cOPKnUu37kwDePPqzWu3r9+/gAN/3Us474DDiBMHCGBAsOPHkCNL3mkgseXLmAcsbjy5s+fPoAFXzkza8ubQqFOrXi12dOnXhzmznk27tu2jrmGXln27t+/fwFvm1p2Zd/DjyJOrHk5cMWPl0KNLf8y8+eHT07Nr3962unXs3MOL/x/P1bvlwoTJq1/P3in69O3jy59Pv779+/jFF9jPv7///wAGKOCABBZo4IEIJqjgggw26OCDEEYo4YQUVmjhhRhmqOGG/uXn4Ycg1pSQShGRCFFKEqFYIkopsniiig/BGKOLL57Uoo014jgjjQ7J2COPP+oYpJANmbijSTciuSKRRQLJkJFDKnlkSU2GaOWVWGap5ZZcdunll2CGKeaYRr1nJnpkpqkmXeYRB96acMaJVpu6vSnnnXh6RSdsdubp559U7ambcYAWauhSgsJG6KGMNhpUoq8t6uiklN4EaWZ9Vqrppi9dilmmnIYqakmeXgbqqKhWeuaqeqXq6quwxv8q61D8zWqrrP/dquuoAu7qK6UGrlRASgoSWyxKCxqLoErHIpugsss6+6y00Z7UrLXTYputSclSeyC0wYJbILPbltStttVyW6656x5wLbvrnusug7/WGye9UJLk45P7LtSvv07yy6TAUkZJZY4HI3xAkglPubDCDzscscEMT0wxxEsOrNC/GwcMsMYdg4xQvgQX3NB/VdqrMpYT4smqpCvjWaHLBNRs8802wzRszDlRCJyFdxpAQE4CHGDAzjzf5PPPEuYpNNFGI/3by6wKtvRvEf75NE5FH31cqaY9F9jVvjlo6NY5eR0c2M7BPBfZZbfrJ9o4qQ0c24id+nbLwX3/OyndlkrtG96JuS0X3L0RuCngNHUteG+EI2Z4XIgnnmuojM/k+NfWkTY5XJXf1l+qja3a0uZrd26q2ICFbtvjvI77Euy2Ra734XwnrftQtrO+++/4UW068MQXb/zxyCev/PLMN+98TY+/O2+80ks/vd/eyo4u9upS7z333YN/vfjywpuu+eFmr7i462+ffvjkt2t99fR/L/7z+OM30v789+///wAMoAAHSMACGvCAE8mfAhfIwAY60HJNe6AEUeO6CVrQarm7oAYfU8ENenBvEfygCPvSwRGaUC0lPKEKy5LCFboQLC18oQy3EsMZ2tAqNbyhDnfIwx768IdADKIQHYdIxCIaMVYcSqISl8jEJjrxiVCMohSnSMUqMiggACH5BAEKAAcALAAAFABYAkUAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AAEIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKLHigpMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0KMoCSJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaJcaXcu2rdu3cOPKnUu3rt27ePPq3cu3r9+/gAMLHky4sOHDiOkaWMy4sePHjxNLnky5suXLmGkaGMC5s+fPoDkHCGAgs+nTqFOrXl10c+jXoUeXZk27tu3buC27hs1bdIADUHMLH068uPGeu3vDLmkg+PHn0KNLz50ctMzmT6dr3869u+Hqn687//dOvrz582vBe4Ypezz69/Djy2+pvjN70u7n69/Pn3t9zvdh51R/BBZo4HCQMXZAggnmd+CDEEaYWlgSVmjhhZJRiOGGHHaYl4YehijiiERRReKJKKao4oostujifBGpFCNKEqVUI40znpSjjhDJ2COOP/IYpEk7EjmkkQ/ZWOQBNwqZJJAO+fikk1FCWSWVDSk5JZYLSXklkluWRNCLZJZpZkvZpXSVSmse1eZJWKn5pklz0mmVnHe6mSecdQK3J59VsflnSXHqGaihUwl6KKJS4bmonYP6GWmfkproaKKMNnXmppx26umnoIYq6qikzsXgqQmWquqq1CnX22iVpv/J6qy0fudqS+01WuuuvPr133os5RpVr8QWa9ev9rkkoIPGNuvsUMgCqCymz1ZrrU/RDvDSsrJe6+23mt0aLH66gmvuuSxliyu5w6Lr7rsLiruSsMzCa++zqC62YL7U3uuvvSD+K/DABBds8MEIJ6zwwgw37PDDOy3J5JETU9wkmF+KSXHFGWscJscdX+xxxyAz5GWWVqLMZZcpm9yyQie7vDLML9M8c0JakizxkiJzDPHP6HYLqaWZOlgooI8S2ielSzc9qdNEI93v0FPHWvXRVJcrtdZZt7s111aDjbXSSZMddddGgw302mxfeHbbcINaQL6zKfsg3XjHDfTcORHPcIAAdReYLW+y6f0z3zj5DTh/TB3gKuGkGQ4x4jcpHrh8TF3+EmiFS+4w5YlrDl/mNL0muucHg1756eiRPpPpqDesuk0EsH6e66V/ZnvsAs8+k+WML7X7Sp51znvCvssE/H64v96Z8cenfgAB1FdvPfUtLa9f8zBxHnn0CdMN0/DkZY63Y/s2Bv76BAbM/vvBfwX//PTXb//9+Oev//78dzfS/wAMoAAHSMACGvCACEygAhc4kf458EBpiaAEJ0jBClrwghjMoAY3yMEOZiUgACH5BAEKAAcALAAAGQBYAjcAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AA8IHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3MjQgEePFguIFMmxpMmTKFOqXMmypcuXMGPKnGlggE2bEgMEMDCS5MyfQIMKHUq0qNGjSF/WvDkg586eBZJKnUq1qtWrWLMGXXrTKc+eWsOKHUu2rNmzLLninPh1JNq3cOPKnUu3qNqmbKHW3cu3r9+/gA/cpdjWZ+DDiBMrXhxzcESdhaMynky5suXLBh1DhKwXs+fPoEO/1fyQM1jRqFOrXi30I8iKnVnLnk27tu3buHPr3s27t+/fwIMLH068uPHjyJMrX868ufPkAAZClWwQgPXr0Q9Mp14Qu3Xpsat7/wd/+qD37NrDdx8vcDrC89m3vz/f3r159unLi8dO3u18/vXpRxB8AQo4EIH5GSgQfQUatt91/Tm4HoAJSjggfvY9CGGFFi6IYYYTbshhQgyO+J+IIF7Innz3rZjigR8q6CGF3D1n44045qjjjjz26OOPQAYp5JBEFmnkkUi6puSSDG3nJJJQRonYXUxViZdBpjmpnpRcdgkXlVZ2dVCWWvrn5ZlokgVmmFcWRGaZNaYp55xSrRmmQpGVSeeefBplp5V4wiljn4QWqtKfVQYqaIeGNuqoRogyhdCbej5q6aUXRSomlk8tGiemoIaakKZrcZqnlqKmqmpmS7ZqQJOertUq66y01mrrrbjmquuuvPbq66/ABitscOi9OCOALGpY7JYw0sjssd81+Cm0y8qIYLIhboitiig+e0CJxlIb4bTXhlvus+B6m+6g65qp7LgnamtujNbSy+i39k6Lr7P1uogug9s22y27GA5r8MEIJ6zwwgw37PDDEP86qEOuRWyxcz29KhFTkF3ssXIZT8TxTh+XbFzIG9/UscksB4eyyDdp3PLMvL2csk0y06zzbTZHxFTOOwctW88OjQy00EijRnRDRiftdGpLM9T001R/NnFDFVctUUAAIfkEAQoABwAsAAAZAFcCNwCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rMaKCjx48fGxYYSbJkyY0oU6pcybKly5cwY8qcSbOmTYMGBujcyZNnwgABDJgcSvKm0aNIkypdyrSp06cqc/acqvNnUKJEoWrdyrWr169gw8KUSrWnVaFYTYpdy7at27dw42okW3bnQrRpi8rdy7ev37+AZ9KtO+Bu3pOBEytezLhx38F1DR8e6biy5cuYMwsm7BMhULyHNYseTbq06YGQy56dTPm069ewY7tNTXU1a9m4c+vefROkb48ib/MeTry48ePIkytfzry58+fQo0ufTr269evYs2vfzr279+/gMSL/BkAeQPDWB8qbZzi+vMP25N/rVS+/NX32etO7x2///kL46/FXgED+/TfffgaiV2BCiOkXn4AEIpjggA4GOGGED05I4YIINcjhQQBayOCBGV5YYX0bSjiigip2SGKJK6YIY4zh1WjjjTjmqOOOPPbo449ABinkjr8VaSRwQyapJHK0ccbTZ6xFueSUVALWpJM6QRnlZFV26eVbV2Kp5ZZ5fWnmmV6FieUAoJGZFZpwxrmUmli26aZacuapJ010OmnnnfntKeigKfVZ15iA4knoooxWZGhZiCYaaKOUVqrQo1RFKimFlnbqKWpHhgrSpop+auqpFJHaIKqsturqq7DGwCrrrLTWauutuOaq66689gjgeZx+aNCvwGIoIo0nFpusicsqFKKywg77IorGKtssste62N+M2sp4bLfVCuittdES9CyE2YKbLojTortuQeeaWK65k/Zq77345qvvvvz26++/AONqpERpBWwwd006pGm9BzcsXcINLYyewxRHBzFDEnNa8cbNXdzQnxNzLDKTkT0EssYjp0ycxwydrPLLK5eM8VVYwWyzbizb9ubNPL+Ws2c079zz0KQNHFHBREsUEAAh+QQBCgAHACwCABkAVgI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzamRooKPHjwYgFhhJsmSBjShTqlzJsqXLlzBjypxJs6ZNhAYG6NzJcwDDAAEMmBx6s6jRo0iTKl3KtKlTljl79vwZdKjJp1izat3KtavXrzKjSt1JVahVkmDTql3Ltq3btyjFjvXZ0OzZk3Dz6t3Lt6/fmnLHOrR79q/hw4gTK/YbWOrguyMXS55MubJlwHPJLgRK2Orlz6BDix5dsPHUzVUhk17NurXrt6Z5loWM97Xt27hz3wTJW6Rq3cCDCx9OvLjx48iTK1/OvLnz59CjS59Ovbr169iza9/OvXtLAOABOP9EKzC8+IbkD4R/mH49+vbgx5M3L782/ff249cf6J5hSf76+QffeQLmF+BCAxKIYGTlHajQfw0q+OCA+0UoYUIJVqiegxMaWJ+H+OF1X4EAcogQhRr2tyCDG3rn4oswxijjjDTWaOONOOao447G8ebjjyAdQNuQ6fFo5JGKxZaZVEAJSeSQSEYpJWNLVqlTk09COeWWXLKlpJVXBuBklnd1aeaZXH0Jpk5jkukZmnDGqZSaa7bp5lVy5qknTXSCaeedRe4p6KBxrZkZloASReiijFrU55KIJgpho5RWypGhc0UqKYuWduqpQECG6uOfiX5q6qmopqrqqqy26uqrsMbEKuustNZq66245qprRCOuWOKFJ7Kooq8RfliisRuaeFCG+B2borIGTToshiiSaOGz0BZU7YogFtgtsckCu6yw0wb7LbjljkuuuNHOl267BrKr7brs0RsigLvmq+++/Pbr778AByzwwIyGGtFvBCc83ZcNcYawwhA7x/BsZUZsMXMTo9YZnhd3fFzGHD3s8cjBgbzQxpOSrHJuJiuEcqArx9xaywc5XLHMOLNGs0E2F5bzz6PtXFDPbwJt9GUG++bz0RoFBAAh+QQBCgAHACwAABkAWAI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzanRYoEBGAwY2ihxJsqTJkyhTqlzJsqXLlzAndgxJccCAAAFoxtzJs6fPn0CDCh1K9OXMijZx6izKtKnTp1CjSp3a82jNmzmpat3KtavXr2BbWkU6YGnYs2jTql3LFujYq2bbyp1Lt65du28n2ox7t6/fv4ADVy3A12HSrIITK17MuPHDvBAPF3ZMubLly2ghP5SMubPnz6Cbdvw4ObTp06hTq17NurXr17Bjy55Nu7bt27hz697Nu7fv38CDCx/eEwAAgR2Te0xovPlx5ckVOjd+APpy5tOrQ5c+/fl27M6Rf/9H2F37+IPlrV9Hn9069/bnDXb3rvx9c/P1wd9Xb5+6e/37xVfQfPiNBuBxBa4nH3wCEpRegw4yGN2B9E1IXnYJ9lehheyFx9+BGVJYHXEklmjiiSimqOKKLLbo4oswxngaSDTWWCNH6kEo4448LmaATUAGGaRCSuWoY49IJmnXj0I2OQCRORmZn5JUVkkXk04OmVCRUhpo5ZdgnoVlljYxZECXXoap5ppTjUmmmWgqyOacdArlZpZwolnnnnz6dKeTUJ6pZ5+EFrrSn00GGqehjDY6EqJCKjqoo5RWKpGNmIKEY5eWdurpp6CGKuqopJZq6qmopqrqqqy26uqrsMby6hR1IV7o4X+2Bjhlrv4dSSCuHd5Xa7C9+iohhwveumuyyiI74IPLPtussxFOmyazxVI70K/GHitntcICi+2G10qrq7bbeqshuRqKFy242aJ7ALTvpmvtuiPKqu++/Pbr778AByzwwAQH9e1EpTl28KUFN1wQYRYRcIAACTcGMVIHIOawwxdTJDHFoXWsV8YVb8yvyBJ9XPJiKEf0pMYmE9xyyisrNrPLNccM680QEZCzYDxv9rPOrQbtkM8hD21QWUTLrPRAKidt0ctPN42q0QtFDRrWClFttcBcJ6T1Z2Ej5PXXAC8cUdV9qQ0R22hjFBAAIfkEAQoABwAsAAAUAFgCVgCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4oseKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrSo0aNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdy7bsxrZw48qdS7du0bd28+rdy7evWrx+AwseTLjwXY2GEytezLgx4MaQI0uenPYx5cuYM2t+anmz58+gQ9fsLLq06dOeSaNezbp1YdWuY8ueDRc27du4c+vezbu379/AgwsfTpwmgOPIkytfzry58+fQo0ufTr269evYs2vfzr279+/gw4v/H0++vPnzyourX8++vfv38K8STBlRZX2UEunfP5kfP0T7/+n3kIAD+hcgfwciWKCBDhHYIIMPKrigSftJ2JCDF0KYoYUbUpighxOCGKGII8Zn4okopqjiWQa06OKLMMYY44o01riZAQPkqOOOPPaYYwABGGDjkERChqOPSPoIpJBuIVbkkzYemeSUPwZplm1QZtmelFRSySRZWGopJnFcdpnkl2OFOeaav5Vppo9oiqUmm3Tq5uabOi55pZN19hncnXgOoGeTGflpaJuBKmkloRgd6uhuMkYqqYt7FvropZgyNWemnHZq06aehirqqKSWauqpqKaq6qqsRjWdStbB/1pdStfRGitKteI6q63UydqrrruelKuwwRL7K7CvIpussceaNKyzxULbbEnPUnsrs8tKm6220vm67QHNtSruuOSWa65M82m4EIAhHtAfhwxhGK+6CrFbYkkV4puvu/vuy2+7/977Lokd6vthwAUbLHC/DB/s78MO+3vuxBRXbHFSk2Y848UcEwkonoN2rJfGcYqM1cdvhmxyWSWrRMDLMMcsgADuagbqaSibqfLKYxlAQE40G1CAzXzamSiSLfP8lc9AHyA00ZZCejScSrP8c04EPJ3Zzabl/GbSVXPFtE5aY8Z1aV5TuXPYXo2dU9mXnS1a2lOuzbbYV980M9yUyf8dGt1J2n23Vm7bFPTQWxedG8mMDw5W4TUdDnWjjg8OuUokI2624pUrDTa8B01+Ueekl2766ainrvrqU30LbrTWTvs67NXOLvu12EbnbbfK8p677r1DtzvwwT/H6+24x4687Lb7zq3zyn9be/LRE/+78cWzrv323He/1Ejghy/++OSXb/756Kev/vrsT+T9+/C3yjjJ8de/GOBICm7//nvhr+jn/AvgXPzXI/0J8IBwISDVEMhAuiiwRwBsoATJ8kAeRXCCGFza1HZkwAx6sCsVrNIFP0hCq4RQUIsqoQpBOL+MrfCFMIyhDGdIwxra8IY4zKEOd7gm9Pjwh0AMohAOh0jEIhrxiEhMohKxExAAIfkEAQoABwAsAAAnAFgCRQCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4oseKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrQoSgBIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNolxpdy7at27dw48qdS7eu3bt48+rdy7ev37+AAwseTLiw4cOId25MzLix48eQI0umuXiy5cuYM2veTPSggc+gQ4sOfWC0Ac6oU6tezRqx55kDYsseECDA6da4c+vezfvna5qzadvuTby48eO7f8OeXfs28ufQo0s/rFxm8NjOp2vfzr172+ovr8v/zu69vPnz6F+Cdykee/r38ON7X99SfHP5+PPr533wQPv/192334AEFoiaAQAmGJuABjbo4IOImSbhhKBBaOGFGGao4YYcdujhhyCGKOKIJJZoonwOqRSRihClJJGLK6L0oowtwvgQizfSWONJM/K4o4856piikEMCGaRJPSL5o5JHlpSkkzEaWSSTU1LZEI5VHoDQiVx26WVKABx1FZhWqTQmmVWhmaaYZbJJlZltujmVmm/KOaedUtGZp55R8dknnn+ehJWfT8G5JqCFIpqooGcyeqijdSra1JeUVmrppZhmqummnJZXWaeghrrap6I2eGWpvE1YGoWjJYRqegXA/8TqaeS9uhqC7cXUnKu2mldArTMRcIAAwPbKGa7i6Wobr8Z292tOwhLbLGvIXqesAcxOq92zOQkgrbapVRvcTNhuCe62xQab7rmSiTsbudmy+xy30K4r72PuygavuffOa69LBAT8b7+J5RvbtfESTBy9NkU7sMKGGaygbLVpyS/ECz+8ksMYTybxxMJp3HFuDG8c8MkoE+CtyCP/NevLn7UsXawvwSzzzTjnrPPOPPfs889AF5elxU1CWfSTRB8dpZUMYXkqkU1DHbWUTzO9kNNTU3211Ftr3bXVXxs99NJiZ4n02UuWnTXYCV8c9NtvhwlppCY1OveekjJF6KJ129Vdkt9/x3l3oH0LHrjhBwyat1p7T7o444MTfvijk9/5eFKG0l255YVTnjjin3seuuZwl2766ainrjpqpK7uuqWtvy57lwLBTOGqs+fOZe04LTic7sCPyHtODAZvfIfD41T88cxjmPxN7jUv/YXPAzfe9Ng/WP1y0Wfv/YDbWxfy9+TrJ5B/ILe3fPnsp/fxxOu3L793ts86//3456///vz37///AIzMSAZIwAIa8IAITKACF8jABjrwgRMJoAThk5YKWvCCGMygBjfIwQ568IMgDGFWAgIAIfkEAQoABwAsAQAuAFcCNwCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcKLCAx48FKBoYSdIAx5MoU6pcybKly5cwY8qcSVMmyI8mHw7YyXNAgAA5awodSrSo0aNIkypd2vKmx6ANe/L8CZWp1atYs2rdyrVrTKcFqi6UupOq17No06pdy7btSrBiGZIdENet3bt48+rda9NpXYVz//IdTLiw4cN34UoMjLix48eQIxNVrJOsWcmYM2vezDkhZYdzL3ceTbq06cR+IYYGerq169ewkYIVWXJk7Nu4c+vezbu379/AgwsfTry48ePIkytfzry58+fQo0ufTp0jgOvXBzpNiB279psIu3v/PzD7oPjs5LebF/8dJHf2HcGHh5/e/Xr66g2eBxBfvn78/v3XXXsfvTdgfwXOd2B9CQp4YHkOosdggwWdR6BHBo43YUgKaphfhRZuyGGE6EEI4oIfnuhhgCpKmOJA+104YoslvigQfdXlqOOOPPbo449ABinkkEQWaeSRSCap5Fa1kTQRWDMuKeWUkRkw1wAPUQUllVx2iZiVc2UJ1JZelmkmXmCSJaYBZJ7p5ptnpSlVRGyaCOedeC4lZ090tpnnn4AOtSdPfdoZ6KGIsjToTmv6meijkGq0KJYOaWlopJhm+tCkjV6q6aegFtSkbRI5GuqpqKaq6qqsturqq7DGyCrrrLTWauutuN7nIosEhbhhh+N5GqOIwPKHIIbFypjhrvaRaOyvugZr4wG+TkstgM2SqGyyx0YJY4jCYpttr+IiG+2z1lbL643gTjtsuCiuy26845JLL4X23msujcbCK2GuAAcs8MAEF2zwwQgnrHCe3oq08MPUhWURlqxBbPFzEldEsWAXdyxcxhRt7PHIyIGsMcckp5ybySGjrPLLr7E8EV0w18ybzKodULHNPMeGs046u9zz0Jj9DFrQRCdtWsMTCa00RAEBACH5BAEKAAcALAkALgBLAigAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AA8IHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3HiggMePBSgaGEnSAMeTKFOqXMmypcuXMGPKnEmTI8iPJh8O2MlzQIAAOWsKHUq0qNGjSJMqXXrxpsegDXvy/AmVqdWrWLNq3cq160anBaoulLqTqtezaNOqXcu2bUWwYhmSHRDXrd27ePPq3WvTaV2Fc//yHUy4sOHDXOFKDIy4sePHkCOzVKyTrFnJmDNr3pyZssO5lzmLHk26dFrPUS0DNc26tevXNcGKLDkStu3buHPr3s27t+/fwIMLH068uPHjyJMrX868ufPn0KNL3w2guvWBThNavy4w+8Ht3Dve/0QIvnp37wbLYx9PHvx6kNrdn4fffvv7j/Xti2efXv5++v3p959H8QmIXoDhHUhQeQDMB2BBDDqI33f+KbhghfwhaJ6EIVGI4YMXyiebhwlaKJB6HBZYIoghrjihhhuOON2MNNZo44045qjjjjz26OOPQAYp5JBEukRbbRPJWOSSTDZmwFwQUaVkk1RWmdeTZEUJ1JRWduklWlhKpaUBXH5p5plWhdmTRGSaiOabcA6lJk9slhnnnXgaCWVEbWaY55+AojTnTmPaGeihiPK5p0NSupnoo5AiNOgAhToa6aWRHimYQoZi6umnoIYq6qiklmrqqaimquqqrLbq6qkoDlXYIYkbypqfiwTSGqObsTraq58t1urrhy9CiKKdvwJ7IrG56npfszA+O6uxIvJ6rLUGKrtstiwOlGyxwQqrbYSyThuutOZ6y2y66nIL7rkp6tpguQEBACH5BAEKAAcALAAAFgBYAlYAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AAEIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKLHigpMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0qNGjSJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaNOqXcu2bVADcOPKnUuXrtu7ePPq3cvXp4EBgAMLHkwYcIAABvoqXsy4seOyfwtLLnw48ePLmDNr3gw08uTPhhFzHk26tOnNnkGDtny6tevXsCGrns06tu3buHMvTT1bcm3dwIMLHx6Td2/BlYkrX848uPHjoX83n069Oubn0JNb3869e9664MPH//VOvrz58+jTq1/Pvr379/CHF5hPv779+/jz69/Pv7///wAGKOCABBZo4IEIJqjgggw26OCDEEYo4YT2xWfhhRhmqOGGHF5FUkoRgRgiShKJCJGJDqk04kklkriiSS2yeCKKDak4o4w3wvhiSTvymKOOD9FYo4s/HhAjkCkKuZCNQRLZJI5JdijllFRWaSVmDF6p5Za5Zcnll2Ca5mWYZJb52JgpiaemXWa26eZTaKKE3XHavWnnnUXFedKcvdWJ55+A7qSnSXzO5megiCYa06AlFdqbdIpGKqlJjB7gKG2TZqpppZeqBqmmoOLJKXSUiRbqqajq1Olkh6bq6qstrf8qWauw1mrrmrjCZeuuvPbq66/ABisssAOmVCxKx54koErLItusss9SGm1J01IbILPXOpsttNtKCyC23xrbrbXjHpCst+Fy+x+464qbLrrtqhsvvP2xO+y9+Oar768fOjkklFEiGbCPRfZoZME9HknwkwIzxOTAC//bsMP+ShwxxRUvqaRCG3OcscYAW3wwwxN7/HFCHaN8MkL7tuzylLm+TFWlMjdmAAE456xzzjAVUDNRNP+s2M05CWCpz0IHFXTSexGNk9EGIE1crrietjTT3xFQ9NHLyVrqp5ldjfVdTucUddekEkarZmKP3VbZOJ2tnNdqm1pa226vBfdNck//nXZhYGO5YN42a/0013P/TVjgl+FNOFp70wS11MPRDbjVgz8+tKVqtjQ52opHh7mCmi/mX8+gh7526axbZzlydrcu+3ZUrzn77bjnrvvuvPfuO731avsuucObW67xxZ9L/LzLny68888HHz1/7iZfbbXIM9889dNzLy/0wHsfvn7sgj9+ftVrr3z25hP/+/vwxy//TSPVb//9+Oev//789+///wAM4ETmR8ACuspxBkygWhCowAaWhYEOjCBYICjBCm6FghbMoFUwqMEORoWDHgwhU0AowhIehYQmTKFQUKjCFvaEhS6MoQxnSMMa2vCGOMyhDnfIwx7yhUJADKIQEYdIxCIa8YhITKISl8hEAgUEACH5BAEKAAcALAAAFABYAkUAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AAEIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKLHigpMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0KMoCSJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaJcaXcu2rdu3cOPKnUu3rt27ePPq3cu3r9+/gAMLHky4sOHDiPMaWMy4sePHjxNLnky5suXLmGkaGMC5s+fPoDkHCGAgs+nTqFOrXl10c+jXoUcfUMu6tu3buHMndt255mgDtHULH068uHGgvDnfBK70uPPn0KMfTz5geXDp2LNr346YuvXm3MOL/x9PPq53mr+vl1/Pvr37mOdnpgf/vr79++Xjy5yfFL///wA659gBkBX4mHoBJqjggqmFxeCDEEYomYMSVmjhhRhmqOGGHHbo4YcghijiiCSWaBwAKkWUkkQrqoiSiy9ClKKMMdJ4Eos1PtSijjs61KOPOQJ5I4wmEVmkjUPymKSSRyJZEo5LNjQjk08aeQCUTVJp4pZcdnkSgrNZlRJWY15VpphHoZlmVWeyuaabX5r55lQqyTmnVG1SlSedd0ZVp5pxwmkSmX36WShUexp6qFN/CloSoYHq6eWklFZq6aWYZqrppn8Z6GmBnIYqqm7UwQbbb6OmqmpqpZoaWwBh2v+56qy02tWqq9WZxB+gtfbqK1u3upoSc7L+auyxPwVr6rBZIevsszopCxuzkEJr7bUuSfvaSbs6iu234GobGrekNQvuueHiOq6u5VaL7rvOfgoqZObCa++9kXaF7773UsjvvwAHLPDABBds8MEIJ5wpij8y1LDDQUoZ8UJTChklxBdjnKXFVTp5pccfa4llxyJbaWXIHJMsccYUT9wyywo9HLPMCdFcs8szw3yzwjz3S1++eC7KqNBNJarooLw+mrS7sXrLdLFKe9s0n0AHXfXRUUtKNJhQP93111JDPbXVSC+ddM9op62h1mq3HWoB8pb2ktzaxW2323jTBHdOBBzLIADd2YlrKqp5F/7S3jj1/Td3gp8KK9WGq434TYoDjp24LXU7dOSSW24TAZ5Hh3m2kHPe8+SJhw7d6C0Ri6jpaaNOuerPsc6S60/BHjvtMFXOuLqZt4u17gjLTpPv29mekuZFE89zAQcQIP301EvfEvJ1A88S80w533PcMPE+nbwEylu69+j/52/67OO3fvvwxy///PTXb//9+Ocf3Uj89+///wAMoAAHSMACGvCACJyI/hYYoLQ48IEQjKAEJ0jBClrwghjMoAazEhAAIfkEAQoABwAsAQAZAFcCNwCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcONCAx48gQ35MWKCkyZMoTXJcybKly5cwY8qcSbOmzZs4VxoYwLOnz589CwYIYCClUZQ5kypdyrSp06dQo0rVuBOoVZ9CiR7dWmCq169gw4odS7asy6pXr2YtytWo2bdw48qdS7euRbRpgR5k2xap3b+AAwseTDgm3rxYDfLtq7Kw48eQI0uuexgxz72M/U7ezLmz588vK1tem/kk6NOoU6tWLRox6dIlV8ueTbs23dZ5X8O2zbu3799JRQoPSRJ2V+DIkytfzry58+fQo0ufTr269evYs2vfzr279+/gw4v/H0++/FIA6NMDcJtQPfoD7BG6BwBfs3z39U23n5+/8X38KSnEX3wHDRjgfur1F5uAANpn0Hz0HfhfegouOOF7BD7YoH4IUihhgQZyCOKG/o3o4YcaJohiiieKaCKGDhYEYYUMqrgiQSG6KCOJFv5n3o9ABinkkEQWaeSRSCap5JJMNunkk1BWNNyUVBJkHFdRZqklYLhZxtNQBlh55VFblmnmW116CaaYY9545ptwNpWmZWsO1GaGceap501zehmmnXfGuOeghLbUp2V/ChSooIU26qiUXiLK5qLHPWrppREdmladilLaI6aghqpYpJsSNemioqaq6gGaXsVphXeu5yorplTWKtKpgc6q66689urrr8AGK+ywxBZr7LHIJqvssp5BiCeOPH66o406TttiiSxeKy200dao7bYD5YittTBWy+233qLbYbnmCiQuuO52m26EjMYr74XswnvAu5W+SG+79qq77r/jniuwv7DOm/DANNbI7MMQRyzxxBRXbPHFGGcMp60ch+lpvxqH3FyrVg0F65gipzwyqa4GcPKVKsecHMlAmfyxzDj7RrNVLxuX88+17azXzUAXzRrLafW8m9FMgyZ0TzZ72vTUnT39pctEU601ZFYPEDWlW4ftWMe2Kl2a2DgFBAAh+QQBCgAHACwBABkAVwI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzYjTAsaPHjyBBFixAsqTJkyhRalzJsqXLlzBjypxJs6bNmzhzEjQwoKfPn0CD9gwQwMDIlEiTktTJtKnTp1CjSp1KtepFnkKzCiVqlKDSryqtih1LtqzZs2jTasSqte3QokfBylVLt67du3jz6oXI1q3brgPlCt5LuLDhw4gTt+zrVytggYLnKp5MubLly2oZNxb6+EBksJhDix5NuvRKzZt9co37Ganp17BjyxaNOvWA1V5bJ53Nu7fv32hrp8YdWLdr4MiTK18uM6Tz5x1ZGzfJvLr169iza9/Ovbv37+DDi/8fT768+fPo06tfz769+/fw48vvrRKA/fsAFNbHb1//yQP85ZdQSgDy599/AR5YkkAJDohggwjtZ2CEEt6n4IIQHhRWhgZVaKGD1AUoIIUhTkhiifhdWACDJmr4X4Epgoghh9KtSGNuD8Z44ow6ukgdjB/uaOONxaHYY4c5BunjivM16eSTUEYp5ZRUVmnllVhmqeWWXHaJJXRgihTRdAR6aeaZdQm3GXEPkRkWmnDGKZaajbHpkJsvyqnnnk7R6ZedDeH5I5+EFkqTn411FqigSxnq6KMsIeqXogwx2iikmGYqkaR/SWQpk5qGKmpCnGYFaKWWjqrqqgKVuhVcY6b/yuqsmroa1KkLfUrrrpCG6StHnsrK67DEFmvsscgmq+yyzDbr7LPQRivttNSO6mF/Mi4loopAYivkkEfWCK6SSP5IJGRJkltjt96Wa2S77vKoLo4LssvtttmOO2+R8sIrrr35shguvY2e61m6/hKs74hL9stwvNq2CLHA+6L7bsL8RizxugYPWu3HIIcs8sgkl2zyySinjOavvi7KqMqZsQwmzJva9iqlEJNJc3A23wrrzg7ZChSu4k4HtFlC/0T00QUlrdrPKurM9FhOv4Xz1Dv1zJnLgmI9p9ZBXe31AVX7JHbGbo5dVdk9nY012wO4ffDLak8Ft9xMl7002lLXKh2V3lD73SrYT8utq+BQAY730YpzjSfiiRNuNeQDyTwzqnRTzpTl0AkUEAAh+QQBCgAHACwAABMAWAJEAIL///+T////k5NkyP//ZGRkZGQ8PDwAAAAI/wABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiixYoKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtCjKA0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltq3bt3Djyp1Lt67du3jz6t3Lt6/fv4ADCx5MuLDhswYSK17MuHHjw5AjS55MubLlrwYGaN7MubNnzQECGIDq87Lp06hTq16tNPPn159Dj35amrXt27hz63brGrZv0KJJ99xNvLjx48iX9v79e7bT2smjS59O3fBy5rCdN4Vevbv37+DZXv/H/lk7U+7h06tfz17qePKbZQvn2b6+/fvh38MfIJ/2cPwABiigbvrB199z/w2o4IIMSubYgxAqNt9ODVZo4YXIoYfhhhx26OGHIIYo4ogklmjiiSimqOKKLLbo4oswntgQUxLRGJGNEOHoUFM3LlWjjz0q9aOQOeo4o5EMIZkkkEUS2WRSQTq5I5MPKbmQlQphiRCPT0LZ5QFDevllQgWcR5OZM6EZ03ZpqgmTm28uVROcLrEpk51rynmmnm0qNSefeQIap597EtpnUn8aGqiigzL6Ep11CtooUokiemiMmGaq6aacdurpp6CGKuqo+EVo6mOkpqpqcgWSd2BWGq7/KuusY7WK3atYxUrrrrxmZStzuF6la6/EFvvUr9iZJ2yCxjbrrHL7ZcfVsM9WuyuyzClrFbXWdqsqtrAFuy2z3pYrK7iviVsVt+a22ym6sQW3Fbvu1ovpqfgmNi259vbr71T0/ivwwAQXbPDBCCes8MIMN+zwwxADeKSUU1I8sZhVUnkxxhuDGSVSYYL8sccZa7ykyVeinLLFJ3Pc8cgkV+xyyyKPOTLMONusc8kzC1SmpI8CHamjQxPdEp6THlAppYVaeiekLCEdtNFHC1011VFbvZLURTN9qdJNe/001ipxfTXZKUGdNdppa721221HLPfcdNdt991456333uLl/3sq32L5/bd/9AGeF7yeqStVwKAS4PjjkEcuueMCCHCAAZNnnnnl2jpNoeF4Id6Z4lEx/qnmqENe+eWpt875hDqBHnq0icubK7+ytp766pjrrvnrhH8ue12ie9b54riv6nvqrC+v+fFgFz48XcV3Bj3sORHrvObNby859KZPX1b1nF0ffOy9ej959+o/Dn7y4otH++i2Lyv9ru0/znv+lAvw/v3xewv5+FO/cQFwVvwjwP74BzwEHTCA8ptffAq4LvilKoELzF8DzWYTCMZFcKbSSvi8BcIIYQ8nHjTOCFPIwha68IUwjKEMZ0jDGtqQbiPJoQ53yMMe+vCHQAyiEByHSMQiTsQoSEyiEpfIxCY68YlQjKIUp0hFmwQEACH5BAEKAAcALAAAEwBYAnAAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AAsIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKLHigpMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0qNGjSJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaNOqXcv25ca2cOPKnUu3btG3dvPq3cu3r1q8fgMLHky48F2NhhMrXsy4MeDGkCNLnpz2MeXLmDNrfmp5s+fPoEPX7Cy6tOnTnkmjXs26dWHVrmPLng0XNu3buHPr3s27t+/fwIMLH068uPHjyJMrX868ufPn0Gk3TCmRekTrELE7VHkdZXXv3U9+/xefXft08wzRpwdfnnx7k+Hdb2f/UP1C+wrxI+T+Hn7/A+P5919C0RVo4IEIJqjgggw26OCDEEYo4YTA2UbhhRhuZWGGHHYY1YYehijiUSCOaOKJPZWI4ooszqRiizDGqF98MtZo40ov3qijiTnu6GOHPf4oJIVBDmnkkUgmqeSSTDbp5JNQRinllFRWaaWT58k3n5ZZClgffV16GSaANJI5YJllmrkll/eBOWaAJcGp5phzrsdmm3fm56adYvLZJ5557rcnoHGiiRAAiCaq6KKMNuroo5BGKumklFZq6aWYZqrpppx26umnoIYq6qiklmrqqagueuWqrLbq6quwxv8q66zOFSmkASna+pQBvPbq66/AAmulrj4aQMCxyCar7LLHCiCAARyRZcAA1FZr7bXYUhtAALhWSeyOxjIrLrPOQvstU9Nmq26223ZL5bk3hjvuvM0+G+1Y6a6rr7bcDovYlfLSS6+5/4qV7777ujslvDYGLPC4BGckLcIUKywlwzU6/DCzEWM0McUJ+ysxwBuPW+69BoO8brsie0xyyeTaizFSB6tsLcveFlylxjATcPLMR9VsM78WRwk0jEXfhHJYwTbtdK+0Ri311FRXbfXVWGet9dZcd13bSGCHLfbYZJdt9tlop6322mxLNGlKmMJ9qdyVqjQ33ZTinTdKcfP/ffdJfwNuqd5v+z244XUjXrjiktp9uOCPm9Q35IlTvrjljTOeOeabSx54SY96LfropJdu+umop6766qy37vrrsMcu++y012777bjnrvvuvPfu++/ABy/88MQXb/zxyCev/PLMN+/889Avdjnonx8wueeVcw6p49lTH7n33VtfffXih389+Htrv73m67MfOuGRwh+/+++rXz/26dvfqPzt458/+tMrH6OiR8ACGvCACEygAhdopac5UFi8eaAEecXA1ghtaDjbzQVtlsEKnmaDKutgbkAIMhF6UDQkpJgJb5NChK3whJ9pYcV6I0OEJQ2GnqlhyCI4NHXdEIea0aG+XX4oGyGui4hAvIwRsfXC2SzxWk1MohJ7CMV+8ZCK1YqiFCfzxJtZUYNYzOIXt7iZCUqQhmZ0IBnXyMY2uvGNcIyjHOdIRyal6o54zKMe98jHPvrxj4AMpCAHmamAAAAh+QQBCgAHACwHAEUAUQI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaJFiAYyatQosYDHjx8vihxJsqTJkyhTqlzJsqXLlzBjojQwoKZNmw4DBDAAsqfMn0CDCh1KtKjRo0iTLqR582bOnT1BKp1KtarVq1izap3KtGnNpzyjetxKtqzZs2jTqv3Z1StGsWPXyp1Lt67du0rbNn0LF6/fv4ADCx5MUK/Th2HFEl7MuLHjx0IN42yoM3FUyJgza97MGaHkr5Sh9u1MurTp03M/DwALtwDq17Bjyy66sXbH0bNz697Nu7fv38CDCx9OvLjx48iTK1/OvLnz59CjS59Ovbr15VEBaN/OMPt27d17fv8Hr/DyeAALzY9PL349e5Dnw8N3X7799/ch49effx+/R/37fQRgQt71R6BPAx6knoEIFcidfwUkqKB9Dwb4H30NIojhhPxVeGCH5FkY4YYcCkjidSimqOKKLLbo4oswxijjjDTWaOONOOao4448pljbjxn1KOSQkAEJpFc3VUbkkkwGphqSSCrZ5JRUyvUklE1JWeWWXGp1JZZNGdDlmGRyBeaZN4lZ5ppsBvUlmgOo2eacdK70Jpha1qnnnhfdiWWefAYqaEN+QgnooIgmOtCbRtqm6KOKNuoopJRWaumlmGaq6aacdurppxY5GOKHJjKYIYjoiXiAhAUt6GGJF5rBCuuIshokaqoiskqQq6POquuuFPbq64mtBovrqfkROxCvxw5ba7Goylfqq85SC22yzy5rrLSxWnttt8Laui2o5JZr7rnopqvuuuy26+67a7UmL27QhdQnvJDOq69U0n0kZ0Q2BYDvo/sWPJ2/FAU8sKIF73uwR/9CpPDCiDas78MFRAzwahQPavG8GGssMccdB/qxvCFbBFrJfJ7cWsoTTczyni7T+xzCMdck8Mw013xZvxAnrDPPPfvMb3T2WmRAQAAh+QQBCgAHACwBAEUAVwI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIseJCAxgzatzIkaPBAiBDihxJkqTFkyhTqlzJsqXLlzBjypxJs6bNlgYG6NzJs6dPnQECGPhYsqhRkDeTKl3KtKnTp1CjSp1aMOfPqz+DDi14tKtJqmDDih1LtqzZs0ytYl0LVChRr3DRyp1Lt67du3hfqmXLditBuIDzCh5MuLDhw0v38sXqdyDguIgjS55MubJhxYt/Nhb42Kvlz6BDix5tE3PmnVrfdi5KurXr17BjCzR9ekBqrquNyt7Nu7fvurRP3/6bm/Xv48iTKy/dsblzjKqLi1xOvbr169iza9/Ovbv37+DDi/8fT768+fPo06tfz769+/fw48s/ThKA/fsM69+3n3/kfv4KmfQfAAvp91+B/h2IoEgD9segggmV1GCAIx0wYYQJ7udgSBdi+KCGFGaIX4ghWQghQgaC6CGHJ6IoIoAkgtThQRK2GF0BJqro4ocjxljAjDS+SKCPP9qIG1LzJankkkw26eSTUEYp5ZRUVmnllVhmaeVzXHqk5ZdgZhdcZsPBBFmYaKZJ1ZiLlRmRdCWpKeecTbHJl5sQwfkVnXz2GZOdi22Wp57T+WnooSkBypegDxFaKKKQRvqQon1V5GiJkmaq6UGUXoVno5fiuOmom3aalVsUhSoqqaxCaqpPnzrppGqrtB7a5a3QpRpqrbz22tKsvgYr7ESBDWvsscgmq+yyzDbr7LPQRkufkBvKaOSRLOoY5HRAYmvttY5Ru2C2Pa74bbnb8gjjjiV2S5y6Q/ro7rvkrptuvfGyi9S8nAmZ771Fagswv/3COy6+B+NIcIr2DgxuuAYTuTCm0lZs8cUYZ6zxxhx37PHHIIcsF663QqynyCiH92pPw12a8svdrcxTy47CbLOYtZ3ql8s390ydzD3tXLPPRCMHNE9CE1r00r4dvVPSJzMtdWxO06z01Fi3VjWqB/Cc9degbQ01nGCXbRnJXZoMZ0AAIfkEAQoABwAsAQBFAFcCNwCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQI0qcaNCAxYsYM2rUOLGAx48gQ4oUSbGkyZMoU6pcybKly5cwY8qcSbNmQwMDcurcybNnzgABDHQcSbSoR5tIkypdyrSp06dQo0oliNOnVZ9AhUo0ypXk1K9gw4odS7asWaRVr6r9GXRo17dn48qdS7eu3bsq065dqzXi2794AwseTLiwYZp6917tC/Ev3MOQI0ueTPluYsU+GT903LWy58+gQ4t+eRmzzqxuORMdzbq169ejS5segHqr6qKwc+vezVuubNO1/d5e3bu48ePIZ25czvxi6uEgk0ufTr269evYs2vfzr279+/gw4v/H0++vPnz6NOrX8++vfv38OPTBUAfgMDoDevbP4CfYf2B/SmkH4BH5fcffx85NCCCBfp3YIAC0kdggxFKeF+CDlrIoIIPYpjhfhAmdCCDBRioYYgHLbihiSB6uNCCKKbYIYUVTljihxNyeCKNIs54I44k6mgji0MCKd+RSCap5JJMNunkk1BGKeWUVFZ5XHNYcmQle1l2adFM0I20ZVi/YRbcmOiVqdiZL4XpFZpSqbkXm3CSJ+dadLbkZkh1xjkbVm31ad6dauXJ0p4xCorWn1Zppih4hPIFJqI8Proooz05aml3kaql6aGU/rhpUp0uNmp4pTY6KaWnKpUqT4a2/3rdqzvFqlKoosqqHKa1BqrrdrSe5itMuP5aU7BsfWrsdMjSNmyboS6rnJdZSpsdtdXKVKy13Hbr7bfghivuuOSWa+656Kar7rrstuvuuyvB6GKFLVYq435BGjlvjxYmWpC89hoEcK78FvniiP4OpOK+CA2sY70EN7xjwAQtzLDACFNcsY8PX6ixwhybaDC9ORLpccf5ApkwyPiufIDFH788Irw012zzzTjnrPPOPPfs889AA4ttcxRtG/TR2jVrq0JGI+10dUo/u1m0T1fNLK/CKrtQ01Z33VuzOWnNNNVel70b2AOInRDXZrfNGtpqI8S223R/FnXcB81d996S3RddNNl8B9431sn+zargiEM2NNHPuclSQAAh+QQBCgAHACwFAEUATwI3AIL///+T////k5NkyP//ZGRkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSoxooKKBiQQLaCyAsaPHjyBDihxJsqTJkyhTqlzJsiVGAjBjypxJE6YAAQIN1Ny586aBjRxdCh1KtKjRo0iTKl3KVCHPpzJv5oRK1SfQplizat3KtavXr0qpQpV6QKdYnlY3gl3Ltq3bt3DjpjwLdSpdnj/Vyt3Lt6/fv4CL3uVpdzDNvBoDK17MuLHjwIZrFo4cE3HQx5gza97MmSXlmGTNfk6buLPp06hTqz7w2SbOsq0JkL68urbt27jXxg69W4Dl3MCDCx8O1+JFj1eJK1/OvLnz59CjS59Ovbr169iza9/Ovbv37+DDi/8fT768+fPo038EwL490NIK27M/8J72Qfnz6y/ED0D/fvz+xQfgewzhRx+BAsp3YHIJ8bcgfA0OyGCE8gVIoXsIXphfhgg5yGGHEuqVIIYTgljhh/cpaGGKKqJYEH8rskiiiBr25+KLIUIo44YlGuRhj+oFKeSQRBZp5JFIJqnkkkw26eSTXlUUko5QMmfclcc9VB+QVVo5wAAdBRBAXl0+Z8CXaKYJJkNiWhZjmcudueZEbVIJp3ByqolmQ3Vuaeedw8kZ5ph/AopbnnrOqVCffhranKAekelooInu6ZCbN0566JcfSaopcIjq+RCmXH5qG6QdeWrqbaGqOaqfha7/qhqqEvUp66aVKpoQo1veCiqnGNnqa22tpsknobAOi+ugqip7WrGWLsTrm86aJiVIsVb7GJbGRQSrfdqGK+645JZr7rnopqvuuuy26+678MYr77z0FpgjuD62WKpAP+57AIyZDgSwv//ea297D2bbL407ApDwwTNmW/CJBA/McL76XoxjxvhuHHHHBFksscggC8xxyRMjTK3JFGvsMXsrs/wxxDAHnDKPLsuMM8r19uzzz0AHLfTQRBdt9NFIJ71Wlh/xrPTTQZ7pUQBlOQ311eZJHWbVWHddpNbBcu312OqBnarVZKedndkY/aT22+OxPZHbcNftndwQUU233XyvFa1rRHqj3ffglE4tNuGIR8c0cn4FBAAh+QQBCgAHACwAACoAWAJZAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiix4oKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtKjRo0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql071IDbt3DjypXLtq5dmnPz6n17t6/fuwYGCB5MuLBhwQECGPjLmHHgw5APJ17cuLLlr48ja0as+LLntJk3R578ubTpqKFFS+58urXX1KoLk3ZNu3ZQ2LELU7bNeyru3IN39x5OHOZv4AOEF19+9Dhw5cyjF3eeG7r06z2px7aOvftp7Ztne/8fnxO8ZvHk03s2P5q1+vcx2UNGD78+YOSG6dvfb1L+au78BTjWXgTSJaCABSbo1oEMNujggxBGKOGEFFZo4YUYZqjhhhx26OGHIIYo4ogklshVASiiWFOKKdLEYoszvVjAii/SyKKNMMok44wxyuhijT/eGGSOMe04JJEw+Xgkjz0K2SSSLxn5pIpL4kjllEwWqaSOW2oJpYlghinmmGSWaeaZaKap5ppstummfTvGKeeXbSqY4Jt47kTAnnz26eefewogQElzFtolnv7l516ejNIE6KOPCkqooZRmiSh+si3a6KYvQeppn5IeUCmlmyaaKYCcpmrSp6zuOemoc5b/iqlhqKqaaqusvgprnLLOSlittm6K66e67nrom6bSGuyyJw0b6aCiGstro8kOph+ztzr7Z6jSTstotZwBi+2b2m4LbbdSfutruOO2qxO6x7ppZ4Hu1mvvvfjmq+++/Pbr778AByzwwAQXbPDBU6JkkEpdHpRSugc4jNKhCz8MpEkVT3xxSRmf1HDHxV4ZMcjRbkxyyUieTDFJFjvJMcsefwyzSRCrvPHIBDF8s80u4zyQzi5LHLPJJK+cc8spz0zzzkqHrHDTNRct889IW4rw1VhnrfXWXHft9ddgBwjvzWGfODadZWt1NtppW7W2yG2bvXbcXb1tNd1Y2Y33Vnrv9833235n1XfgeQNOuNuGH4743IpTNXjjUz0O+VOST0554pY7VXnmTG3OOVKefw465qInFXrpqKeu+uqst+7667DHLvvstNdue20ntyR0TLvDlLvuTb/0+0q9Cz+8SscjH7xLyafU/EnFM7888EfP9Dz00xN//cvZK1+9TNtzT7X13Tsffvi3p6/++uy37/778Mcv/4Gnz29/VPXfr3/npO/vv+b9+58ATRfAARqQKPk7oAJ3ksAFOtAmDXygBL10tglakIEFvKAGo5TBDXrwgyAMoQhHSMISmvCEKBSg3VbIwha68IUwjKEMZ0jDGtrwhjjMoQ5fFBAAIfkEAQoABwAsAAAVAFgCWACC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8AAQgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4oseKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrSo0aNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdi9KA27dw48qVy7au3Zpz8+p9e7evX7sGBggeTLiwYcEBAhj4y7hx4MOQDyde3LiyZa+PI2tGrPiyZ7WZN0ee/Lm0aaihRUvufLr119SqC5N2Tbs2UNixC1O2zZsq7tyDd/ceTvzlb+ADhBdfjvQ4cOXMoxN3nhu69Os+qce2jr27ae2bZ3v/H68TvGbx5NNfNj+atfr3MtlDRg+/PmDkhunb339S/mru/AUo1l4E0iXggQUm6NaBDDbo4IMQRijhhBRWaOGFGGao4YYcdujhhyCG+FUBJJZo4okopqjiiiy26OKLMMYo44w01mjjjTjmqOOOPPbo449ABinkkESeeFBKEiEJkUoRMbkkSklC2aSUT540JZUOKVmlSVFaeSWXW4L5kJNjYpmlmQ1pWaaXa7J5pptvitlmSV3KmaaaC4mo55589unnn4AGKuighBbqYI+GWoZoon8u6pmCCULoKKN8TmqZf/m516CllIrIqWP4yaYpg592+mGpf2EqKoD7oWoqh672/6WqYazaF+urGd56X6i6ScojrnrqWtesvT4oLLAUHrsWsZzVWp+yyEYIbVrMDqCfgNNGe+ivn1V7bYDZaiuuWZAWOO656Kar7rrstuvuu/DGK++89NZr770nFYASjSnNqJK/+wKcr8Am8RtwjP/KmDDCBzM8sMINv7gwjBNLHLHFDztcMMElcdwxxBlTfLGL/Xrs8QEnc3wkmgzh2bLLecKskMwzsxwznHfifLPONfPcs505A/2yzwiRGafQP9P5pdJhHlAn00dDHbTUUzu9tNVNX4310Ph27fXXYIct9lAKjm322SURoPbabK+N9kvhvi23oXHPbXegdd+td6Xc7vnt96t5/40gAYLLFHjh/BlAeHn6mn044vYpnpMABxjQ+NiPQw6f5DhRbvnZmWuu3mLmsuT55WKHLnp6GLeEetiqry777LTXbvvtuOeu++689+7778AD/7rBIZNcssYfg7yx8skjjzLzzbe+vMjFG199i8dTf/2KFVs/vfbRS/889CmTb/L55js/PvjhY0/0SPDHL//89Ndv//3456///vwnFPz/AAygAAdIwAIa8IAITKACF8jABjrwgRCMoAQnSMEKWvCCGMygBjfIwQ568IMgDKEIR0jCEprwhChMoQpXyEK6FemFMIyhDGdIwxra8IY4zKEOd0ijgAAAIfkEAQoABwAsAAAVAFgCVgCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4oseKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrSo0aNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdi3Ij27dwa7qNS7fu27l289rFq7evX658/wo2G3iw4cNLCyNerFUx48eQdTqOTNnp5MqYM6e8rLmzUM6eQz8GLbo0TtKmU/dFrbq1S9auY6+FLbv2Adq2c4fFrVs1797As/4OHno48ePIkytfzry58+fQo0ufThmA9evYs2vfzr279+/gw4v/H0++vPnz6NOrX8++vfv38OPLn0+/vv3s1PPr38+/v///rR20WUQDPqQSgQU6lKCCbSF4kkQLMnQgRBFK2CCFFxqYYUMTarihhQ86aBKEH4I4IoYlKlThQiuqmKKLIXoYo4kA1mjjjTj2ZcCOPPbo448/5ggZkEQWyaOQSL5kwABMNunkk1AyGUAABiS52JJRZhnllFVa6SVKWGopppRUfmlYmGNqyaWZbKKZ5pZl4mjcU26++eSabH5Zp51PdnnjnE7tyWeTfuaZpKCDDlBojYA2heigixqa46N8Rvpfo0xRaqelktqo6Zh4/qlRWZ+KGWqnkyYK5amMjkpWqWrG/4lqqqo6ySqAmC4Fa5a3zgqgkcAGKaerYwVr7I6+Jitprso266xwxD4r7bRoMUvttdhmq+223Hbr7bfgciVeSuaRS55K5Zl7Lkrlspuuu+Ohuy6849Jb70nt4juvvvGqe69J+QL8Lr/9EhyevAUbDJ6//5YUsMP7CpywxPiFa/HFGGescWkCvphQiwh1yKDHHc848okooixjSSKqzCHJJbv8sskzy8wiyDHbDKPOO7Pc8m0//wz0ykMTTSLNNBZdM88Gbez001BHTdOxxkodNYfBPrsrnJxaHW6VWRMg9thjCyDAbc5uvaqsXmdsAAE5nW1AAWnXaivbbV/8dtwHzP9dt91Odp13t3vnRIDfzaodpeCDb1t4TogrqziUjDduubSTN9nr5dsWULlLZn+uJ+Bkis75s54bfoAApluZeemng5s6TnCznjbVwMb+Ld1k9z5230b2rfvwWmedNPHIJ6/88sw37/zz0Ecv/fRfNXzAw9dHDPHE2WuP/ffad2+9+AfbW77CC5v/HcLjD0yx9eB7H77776eP/vrq439/d+yffz/1AAygAAfYmZEY8IAITKACF8jABjrwgRCMoAQnQsAKWvCCGMygBjfIwQ568IMgDKEIR0jCEprwhChMoQpXyMIWuvCFMIyhDGdIwxra8IY4zKEOd8jDHvrwh025jxAUh0jEIhrxiEhMohKXyMQmOvE8AQEAIfkEAQoABwAsAAAoAFgCRACC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4osCKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrQoygNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdy7at27dw48qdS7eu3bt48+rdy7ev37+AAwseTNjuxsKIE9c9rLix48dTDUieTLmyZckHJl+EzLkz0sugQ2c2gNGz6dOEDQxYzbq169erAwQgbRG17dSwc+eWTbv27d/A6arWTZw1783Bk+cdXpz4ceTKo0sny7z57tnQp2tvW9366+e+t4v/H1+1u/fWn7OTXy/W/HnWmUuzn08/83vY6cPX35/V/f346vEnYHL+nZdfRQMmSFWBBvaGoIIQ3sagdeBRFOGFTE3YXIUWYughZBoWx6FEH3oYonPYBVjiioGdqJtsB6jI4n4uXhejjDPmuFxoPF6m449P9SgkZUAWaeSRSCap5JJMNunkk1BGKeWUVFZp5ZVYZqnllpA5xBSJS0XUlJhhkqkUmGeamZSaa0I0pptlwpmmnG0+9KadX9KJFJp14jmnn302dKeXeQK6J5s36pkooXEy+idBKzVFk6QzMVWTpZMudammlWIqk6efctqpUpuSOqqpoYoaE6irqtoqqqkm/5Wpqy+xCpOttdKaK6y34toSpbEi9SqXxBZr7LHIJqvsssw26+yz0ELIWLTUKidotUlZNtqQlRmE7beJFRBZj5mBa95VI4Kr7mAFGMAVAQcI4O6351qV7rr4+tXuu/HOi229Vd2b78B57cuVAPKa+1p/3hLssGH+bkVAxNQCbJWDAj2s8VwGb7zgwlhhLK7HJBdGALwKu4ZuigSV7PLLdlk8lcAw12yzWjXCBmPDN/es1slAB00AwupyO6TPSLt1dNJMN+3001BHLfXUVFdt9dVYa+zooYoiuui1j4IdqNhcG/o1Q4OSXbbafK7NtqJnL5Q22o1uHbfcdb9t9t0JzcuNd9h0A/732IMTrpDfBUTqK0vADivsrLJC/vipkzteea+8Yh455QeUurnlnXMeerCjk+755Zqj7lLjqZdu+emuty656r8urnjmt2et++689+7778AHL/zwxBdv/LM8Hq/8xgUu7zzBzT8vfdG6TW/9v9Vfrz3y2W/vvbLRfy++seGPb76W5Z+vfpXpr+8+lMm/L//89Ndv//3456///vy7PNL/AAygAAdIwAIa8IAITKACFzgRozjwgRCMoAQnSMEKWvCCGMygBm0SEAAh+QQBCgAHACwIAC4ASgI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjFjRAsaJFig8LaNzIUaPEjyBDihxJsqTJkwIvqoTYsSXKlzBjypxJsyZMAwNy6tyZc2GAAAZaurRJtKjRozZx8uTJ8GdQoRuRSp1KtarVkEqX7vQJFCrHq2DDip2ZVeuApl29FhjLtq3btxHLmuX6VC3cu3jhyl2Kti7UvIADCz66ly9Dv38HK15Ms/DWhoiHMp5MubJCx48XRpZsubNnzWZ1Otz89bPp03kxiz6s1iPq159Vn4XcGrbt21Vlz07otPVa3MAHy+5bO7jx4y910/WNvLnb4cvtOp9OfXTo1bzTSq/OXSp0hb23d/8fz13lSofMyauvad4iS/Hr48ufT7++/fv48+vfz7+///8ABijggAQWaOCBCCao4IIMNujggzYBIOGEABzAmUEUTiiUQhlWuGFCHVrY0UIhXohhhiKWhlCHHo4IIoofrgijiy9SmGJUHM6o4kEs3uiajBnGyKOOO55oo4kFlUgjkBP6+BuTTSI5kJJLGhlllVZK6CSJRP44ZJdPfkmhkFm2iCWEaKap5ppstunmm3DGKeecdNZp55145pmXlJBdpOefgObXkgERLeVUoIgmut6ghfJ0qKKQRtocoxAZCpSkmGZqG6WN7kSopqCGWhmnlfL0qaioprpnR6eW6qmqsMbOyhapDVnaqqy45moUrQzZquuvwNLE60K+BmvssSTxeZifyDbr7LPQRivttNRWa+212Gar7bbcduutrj0qyyKZSYIZZplb5njkmQRRiSOUWoprrrpXFomusge462WZZr4rpobyrsuuQPqee+/ABINJb73+HoxwwVwK3LDDE7cLcY1jBsxwxd927PHHIIcs8sgkl2zyySinTBlGKresqVIuxxwpzDLXHCjNNueMJ8469zwnzz4H7SbQQhedJtFGJ90g0ko3jSDTTkc9IMsCBQQAIfkEAQoABwAsAQAuAFcCKACC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQIxqYSLHixIcFMmrcmDGix48gQ4ocSbKkyYcWK3rkuPGky5cwY8qcSbMmygE4c+rEuTBAAAMsWdocSrSoUaMGdup06BNo0AJHo0qdSrXqzKRKd/b8+bSl1a9gw9bEmpUp16di06pdy/Yl2aw8FTbtqrGt3bt432ptOBct3r+AA4vVW5ahU7qCEysmSjgnxMNCF0ueTHlk470LIfutzLnzTbiPu3oeTbrz5aWG6XYszbr06dCbW8uevfa0Y7lnEdPevfg139yReQsfjhQu6oR9dRNfztY3w+TBmUufTtJ2XOTAY1PfLtX5Vs1euYv/H68wZUqMqsmrH2r+YkTR6+PLn0+/vv37+PPr38+/v///AAYo4IAEFmjggQgmqOCCDDbo4IP0RVcQABRWKFBQCFWo4YUSDqShhQd06OGHA4ko0IcAlMhRQiiqGJ5BLYZoIoopyrjiQTHaWFeGJHL44oQ96rgajj1iSGSRM8Zo4gE5LtnkjUeCKCRUPG44JZUwKgllllZOyWKQToK5JZBWGsllmUkiOSSEbLbp5ptwxinnnHTWaeedeOap55589qmfdg615+eghAL4lAEPwdVUoYw2et+hiWa1qKOUVjoepA4p+pOlnHa6HKaRKoWop6SWyhqomWY1qqmstioZqg3BmbWqq7TWahesCmk6q6289moVrgnp6uuwxEYFLELCFqvssjIB2pCgzEYr7bTUVmvttdhmq+223Hbr7bfghivuuDGFieaYI6r5Y7rn7hglhS66eya8PspLppTmgmjmvfqmiS+67NLrZZX/rhtwjfsSRGO99h6MMMAnimlwxF3mK7DFD0O88MDv1sjxvB5jHO+a/GY88cYJO1xvQAAh+QQBCgAHACwAACkAWAJXAIL///+T////k5NkyP//ZGRkZGQ8PDwAAAAI/wALCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiix4oKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtKjRo0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTot2otq3bnmzfyp1L90Dcunjzmryrt69frXz/Cl6rcbDhw04DI17cVTHjx5BzOo5MOerkypgzo7ysuXNRzp5DMwYtuvRO0qZT60WtunVM1q5jq4Utu/ZJ2rZzg8WtOzbv3sCx/g5uejjx48iTK1/OvLnz59BjG5hOvbr169eja9/OXaoBAuDDi/8fTx68AAEGuqtfvxOA+/fw48ufT7++/fv48+vfz5/+9/IAlneeAf0VaOCBCCao4IIMNujggxBGKOGEFMLHnkr/Baiheehd6OGHLxWQUkQqkTgiRCeiuJmJt0WU4YYbplcSiy0+lKKNK6pYo0Ml6rhjQzfymCOOPwI5pJFHMhQkknvROKOTdvnYJJFFLgQihjBmKeOVXHZ55YtZArilTGN6aeaZtYEZpngDkkkdmoxhJ+ecb8J5oZprclhmStURcF6HdhpmwACEFmrooYgSGkAAewaqHZ55tskndeRJ6uhNxs00aKKcJrpoo5c+R+eoN5qk4Xl2hWpTpmR26qqhn6r/GqpBfYZpgIiyzsRqTJu+6iuouXYpUK15ghesroUd1auvrgJ77IfpQbqmAM++lqxRyzLLqbPVsifttNR269KuMGWrLayMiutlesVWGq66K5H7krnnKpouvF22W+kB3FYrr0v01hsrvlzqC151uBJcJUVIjepwdQpfWSzCBEW88EQWZzxUAd+GR11CGkeZUcgkw3VAgBcPVPLKLEcFJsQtxyzzVf/VOfPNOHvXb848VwUASgWm1J9KQwvNn9FHA130SUErvR/RSTO9tElNSx011Vdj/TTS+nHdtdNbWx221l+DnZ/XZ5udtthlk7022/j1LPfcTyU8pZBJWrmk3ikj/7Q333fjHbiSf4NcuN95A/6klCILPrjiizveuOQSHY543wb1SGXklENZeeKGg3455iTRbfrpqKeu+uqsU/Xw663H3nLA5w4s++0a066t7RoTjjvOujPLe8a+/z5z8Nru3G3xxseMPLPK+wt58ys//2vJzFNffb2e3hty9tqTbH2nw1sMfvi5c49o+RGfj77Frz+M/fTv1788/fbnn6v7+vfv//8ADKAAB0jAAv7vZ3CLm9ruA7WxuU2BCWTgAiUYQfugjYIVpM8FMViSqj0Qgh+04Abr08C2dXBqJ8zaATyYQhO28G0vhOEKUThDFdKwhi40oA4dNZIe+vCHQAyiEGKHSMQiGvGISEwixnbIxCY68YlQjKIUp0jFKlrxiljMoha3yMUuevGLYAyjGMdIxjKa8YxoTKMa18jGNrrxjXCMoxznSMc62vGOeIxJhfbIxz768Y+ADKQgB0nIQhrykBQKCAAh+QQBCgAHACwAADsAWAJIAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxARFphIsaLFixcjatzIsaPHjyBDihxJsqTJkyhTqlzJsqVBjDBjUnRJs6bNmzhz6tzJk6DMn0CDCh1KtKjRo0iTKl3KtKnTp1CjSp1KtarVq0N7auX5dKvXr2DDih1LtqxZkl3Pql3Ltq1bsADiyp1Lt67du3jz6t3Lt6/fv4ADCx5MuLDhw4gTK17MuLHjx5Aj031LeWFaiAUqa97MubPnz2QvM6wIurRpy05Pq17tVfTBmKxjg3Ytu7btk7QPBL3N223u3sCDO0xq0IBxAwOPK19+ALnw52B/a11Ovbp159CzszRAoLv37+DDd/8XIAC79vM4pfc0MKC9+/fw47cPEMA8+vsiuYvfL548/v80qccTe/IVKB999gGoIGYH8Ofgd/4tKKFJAu5EoIEYzlffhBw2VIB+D4bY4YgfVajThRlmmCCJJGYW4ossxhiRiTmhmKKBK8rY4YsiZqbjjxKlNpaNN8qXI5AL8sgfeUciKSONOBFZpHsIOkmikvsxaeWWtUk55QBVcikhiFhCWJ6YaJ7m5ZRhpvkfctcRdN1xbtbZ2Zx4UmfnfzLt6eefgJaWUaCEFmpoWxMdquiiJRZk1EtEvRYppFk5WhSllfp0qaWZatqpQI9y+qlum3oqlKSjkjpqqKaeKqqrrQL/hSqsA7Faa6m3ThrrT7PuxuivwAYr7LDEFlsaYAYFliyyBSnb7F8HMUuQs9NKOxC110K7rF/Ralutt9mCKxC244pbLrfPmkvuAetaG25f3aL7rbzvwtussfjmq+++/GoHZb8AB4zfvwIXbDBwBB+s8J55NmycbQkvLDGaaxbZZmwRT6yxkxXfeDFrGW8ssowdp/jxaiGPrHKHJd/Y5GxCriwzxV/iCHHMM+fMcc0FvvxZyjoHnV3LGJ6sGtBCJw0c0QYarfTTUK/EdIFOR2311SE53DDWXHft9ddghy322GSXfZOPua6q6659Yuor223DDdurb6ddt92y0p233DC58323qrTizavfg/ONEeGFC544qGsrPrfhg0JuEeJxO9435ZebrfnmnHfuuUrusht6u6ObKzq956J+uuqkq746X/HaO6/s9cKeLuuml66u7ri7Hvrre23bO+ufF2/88cgnr/zyzDfv/PPQF6t1ntFXn+/UB25o/fbDYh9f1dyHH6j38IEv/vl2kh+fz+i3z6X68LHv/vxAwv+e/PTnz6L9YGqv//9W4p/5AEjACQnQfwVMIImmhycFOvCBEIygBCdIwaNh5YIYzKAGN8jBDnrwgyAMoQhHSJoKmvCEKHSgZFbIwha68IUwjKEMZ0jDGtrwhoIJCAAh+QQBCgAHACwAAEUAWAI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaNHiQZCihxJsmTJgQVSqlzJsmXLjzBjypxJs6bNmzhz6tzJE+SAn0CDCh36M0AAAyhdKl2asqfTp1CjSp1KtapVqAaIatVqFKlApmBfXh1LtqzZs2jTql2YdatboF2Thp27tq7du3jz6t07sO3bv14PzB3Mt7Dhw4gTK/b5F7DcwUwXS55MubJlq34bbw0MOezlz6BDix7NWDPRuF87RybNurXr14kzmw6KWrDqpbBz697Nu6rs2UWPPr7Nsrfx48iTczTJvLnI4cRVKp9Ovbr169iza9/Ovbv37+DDi/8fT768+fPo06tfz769+/c9Aci3LX2h/Pv06yu8P1/sfv75FdAQfwAEyBCBBRZnH4H+JURggAIuiF+DCD2o4H8AXuighSsdmKGGFX7YoYT9jUhigiZuKGJTJwJA4UEcphjihDLOWKJ+KtKIo403sohhjz7miGKNBlkI35FIJqnkkkw26eSTUEYp5ZRUVmnllZg5xxxCxGHp5Zei/dYYQV11CeaZaC4m5l9kHmVmmnDGmdeab7VpwJty5qmnWXS6ZdCdt+0p6KC+AffTn3gSquiiOPW5FaKBMirppDI5qpWdiVKq6aYVWUoUppFyKuqoD3k6FKiqkarqqghpueVBmbLUKuustNZq66245qrrrrz26uuvwAYr7LDEXqRjkDxC6OGxyMK4YoQnGtiitD/K92KRz0IrpLLRgujss8syG+6QO34LpLbJemsuueiuy+64LqpbUIzlYivuuNQK+e608hJEb7P2notvu8UWbPDBCCes8MIMN+zwwxDX1ZxEuEVsMXp0OlSmUhd3XF7GDW3sksckhwcyQyJfW/LK1p3cEKAjsywzdi4zBLPKM+fcW81sVazzz8fxjFDK/QJtNGtCH0Q0kUc3PVrSBi1dr9NUfzZxRD6nGRAAIfkEAQoABwAsAABFAFgCNwCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjRgMgQ4ocSdJAwQIoU6pcybKAx5cwY8qcSbOmzZs4c+rcidPAgJ9AgwodOuBAgAAGWiptybOp06dQo0qdSrWqVYY+iWodahTp0q8pr4odS7as2bNo01LMurVt0aNJwX5VS7eu3bt48+pdyNat1oFx5SrdS7iw4cOIE3/02xaw4KWKI0ueTLky2r6MhTp+zNSy58+gQ4vOiDkz0K6BOasczbq169eeS5t+61X1Sti4c+vefVa2adS2b/MeTry48Zklk5c8GTzs8efQo0ufTr269evYs2vfzr279+/gw4v/H0++vPnz6NOrX184LID37xkKhx9/4W369RXep99wP3z5/uWXEEsH4Aegewbatxp+AByIUoH8OehSgvoF2KCCzlE44IIaImRhfxz+hyGCIlYYooAenojiQR9KCGGJJj7YIYstxjjhjAZZeKGNBbLn449ABinkkEQWaeSRSCap5JJMNulkU8pJBNmTVFaZm29FNQTXlFZ26SVoWDq05WBflmlmZGFqWVtnZ7bpZl5pOpSacG/WaWdvW0E052p39ulnVXE2tKdzfxZq6E6BKjQmm4c26mhMiSa0KIGPVmrpYn+pOeiDl3bqaURRRsTlp6SWauqpqKaq6qqsturqq7DGwyrrrLTWamtTJK6YI584Mperrr7KGOGIN8KYoooSFgssQTq6yKCzw/LYK7M1bphhtMcKiy2Nv4LYLbHK7mjtt9JOO1C144brrbbLnosut+y2K1CzxPZ467345qvvvvz26++/AAcscF58ViTSwAirp5JJEgUFV8IQm7fwRA4jFfHF4k3cMFAPY+xxdxpTDBTDH5eMXcgb/0SyySxPh3JEQa3c8szHvexQxTLTrDNvNjeE885AD9czQz8HbTRuBa8Vkp0BAQAh+QQBCgAHACwAAEUAWAI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIseNCAyBDihzJsIDJkyhTelzJsqXLlzBjypxJs6bNmzgHGhjAs6fPnwgDBDCQsijKnEiTKl3KtKnTp1CjItz5s2rPoEONapXKtavXr2DDih0rkarVqliJai1Ktq3bt3Djyp2r0OxZn3XXsqXLt6/fv4ADZ7R7l2devUcFK17MuLHjsIQLH0Zs8rHly5gza94Y+W5ayic3ix5NuvTlzmc/gy5gurXr17DForaqGnTs27hz65Y5srfIkqt3Cx9OvLjx48iTK1/OvLnz59CjS59Ovbr169iza9/Ovbt3twDCA/84sBeh+PNGFZ4PT778wfXj3b9f3z6xefj1QyeEDyD9fvr+3XdefpWpB6BKBg4oX0H8Ecjaf+sFOJ+CCxKEX4UWHmjfhBRuaBB/En6ooYcMjlgghOI5uNCFCArYoX4ushdiiS+eGON43+Wo44489ujjj0AGKeSQRBZp5JFIJrnRgxgZoOSTH4UEJWYFOFnRAAcMNeWWBZnF5WNVWoSlll9y6WWZjIV5ZZZWognlmW4KpqaYbcaZJJx2/jXnlXXmaSSefvK1J0UD9BnokIAeKtegEY1pqKJAJgrpW4xC5OikREqKKVmVPnTppkFqCmpYTF706Kg5/obqqqy26uqrsMbhKuustNZq66245qrrrrwiVWOpIkaIoUAstsihjMYeG1+yNP6aIHrDHlAsiRk6+yyy1FabYrQNcgvfjNpimy2xJgIb7LbMhiuujeeKB+5A08KorIrXLjsuudbeaK+87e7Lb7Prmgswjr0WbPDBCCes8MIMN+zwwxDHuRq7Bvlm8akRZ2zdxJUWVpVQGGsscnQcP+rxTyCPrPLGE5t8ck8prywzyS0n9HJPIc+sM3El23xzoTsHrVzPCP0MtNBIF0d0QUbHnPTTui1NUNNkQm11bFIPRHXOV3dN5cRRXkySkQEBACH5BAEKAAcALAUARQBQAjcAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AA8IHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3MiRooGPIEOKZFigpMmTKDuqXMmypcuXMGPKnEmzps2bCg0M2Mmzp0+EAQIYQEn0JM6jSJMqXcq0qdOnUBfq9EmVJ1ChRbNG3cq1q9evYMOKnTi1KtWrQ7MSHcu2rdu3cOPKzWm2ak61a+fq3cu3r9+/GMvWtZowLV6TgBMrXsy4MVjBgwfcPYzYseXLmDNrDhy5J1rKlTeLHk26NGPIgz+DLmC6tevXsL+irqsadOzbuHPrfimyd0iSq3cLH068uPHjyJMrX868ufPn0KNLn069uvXr2LNr3869u3ewAMKL/xdYFKH48+TzGjw//oD69ewHvi/IHoD8lObjpzd6sL599/MN5N9+ocGHHoElJVTfffz1px+ADRrYXnkO6kehhBPiV2F7EBZI34MBCmhhiAcM2KGHIo4Y4YcHnshafi2SWCKIGmL434UsHijjggi++N2PQAYp5JBEFmnkkUgmqeSSTDbp5JMNJRiYAVBWWVBvVl5WEpUU7RQUl1lWiVqYjW1ZkZdCkSnmWWoqZmaXA3zZ5pNjzgnYm2cOAKadStbJZ194wrnnn0f6SahegU6006CHEmloo3ElChGajEIa5KOWuiXpQ5RmWiSmno61qUOdhiokqKaCJeVFH6V66UiuxuYq66y01mrrrbjmquuuvPbq66/ABitsTDziSJCJxqaoY405hscgiso626OCNK4YrbTJCoQss8dWC6223q5q47PiNnsjidtyC66K384Yo7rrcrgju9+ma228Gd6L738uUvuuvu7KC2/A2KJb7MA89jvswgw37PDDEEcs8cQUV2yxq6u1e4BvHLd68cfZZSxpZz7JCfLJ1YlcAKMk92QyyjBDpzLLLZca883MzYxQzYvi7LNyOh/Es54/F21c0AYNXanRTMOG9EBDv9z01E6LTHPNUlOtdWlPCxR1mluHTZrKOXXs23YBAQAh+QQBCgAHACwAAEUAWAI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzI8aCBjyBDigSZsIDJkyhTnuzIsqXLlzBjypxJs6bNmzhzPjQwoKfPn0B9FgwQwIDKoyl1Kl3KtKnTp1CjSp0KkWfQqz+HFkXKtQDVr2DDih1LtqxZjFaxYtVqtOvRs3Djyp1Lt67dnWrVenT79q7fv4ADCx7cMW1eoHv5JiXMuLHjx5DhGj4s1GBbxSsja97MubPnwpSDssWM8rPp06hTe54cejRpk6pjy55NW3JoxASJXiZdu7fv38BtjhwusuRrr8GTK1/OvLnz59CjS59Ovbr169iza9/Ovbv37+DDi/8fT74898UGAahfD0Bg3/Ts1btXiTB++/ml68cfSF8/e/7owfcffpkJOOB7BdkHYH4HKUggbP6tt2CBBsr3IHIVWoggQfbdt+FADh7QX4P7XZhQiCMmiGKAHDr4oUArMpjhhBCSOKCILLZ4Y4o6HsgjiCXimCOMQf5o3pFIJqnkkkw26eSTUEYp5ZRUVmnllVhmSVdxWnbpZXaTfSnmmNCFSeaZaP5mZppstonamm7GKSdkcM5p551/1YnnnnyepWefgAY61Z+CFmqoToQequiiMHHJ6KOQRirppJRWaumlmGaq6aacdurpp8kNeUCHF2KoYokvjlqkqKruyGqMFJ67KqGJNvooKqw1zkhrrRYKGWuPvRrZqq0yAquhsLiaKmuwr65aLJDO/kokqshGm+uy9/l6rbE0Ksvtrth6WK2rt1rrLajopqvuuuy26+678MYr77y9HXeuQMTlawC9/Fpn774E3faTbv0WLN2/BQnsE8EGN9wcwgErPADDDlccHMQJSwywxRzXe9zGESsMcsckq4ZxyAKPXPLKn518gMQTF8XyzKa5DDPFNOccmc0S46zzz4zZm5C+xE0XEAAh+QQBCgAHACwAACwAWAJXAIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDiix4oKTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtKjRo0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltu1NA3Djyp1Ll67bu3jz6t3L16eBAYADCx5MGHCAAAb6Kl7MuLHjsn8LSy58OPHjy5gza94MNPLkz4YRcx5NurTpzZ5Bg7Z8urXr17Ahq57NOrbt27hzL009W3Jt3cCDCx8ek3dvwZWJK1/OPLjx46F/N59OvTrm59CTW9/OvXveuuDDx//1Tr68+fPo06tfz769+/fw48ufT78+zAL48+vfz7+///8ABijggAQWaOCBCCao4IIMNujggxBGKOGEFFZo4YUY7mffhhx2GJNCKUkUIkQqRVQiiSiJmKKJK6J4EostOnTiQyO6aJKKL9p4o44lwbgjjzj+SGOMDc0oI5FFIslQjUPm2KSTCXko5ZRUVtmSeHJZqeWW1hHg5ZdgfsklXhGOaeaZpJWJ5pqOYelmluupGd+bc7H5GHbHaaeenPDhKZmdbUJHmWhxQjifn4UB2hiiqumZHp/vMTqYooxJqpp05kHqnqWBUboYp59hWp6m7YE6gKeKmeobe6SyZ6qib67/pepgjqLX6nqv2kmgrIISVut5t6qXK6p7zYococSmN2yyeNEZK7PoOTsetNRWa+212Gar7bbcduvtt2QaqNKB44qLEoIpoXsuuesWWK677cJ7krrzsluvvPfumq65+erb74D78muSvf8GGDC+JdE7MMEJC7wwwg1DDO7EFCsFopILMZkkxlFqvCTHHUN5pMgbC1myyR+DfJDHKaPcco88HuCjyxeT/LLMM+MMZMw5BwnzzkCPbDNJFRdt9NFIJ610bMEuDZvETkPVdNStQU11U1NPLC2Will99VJZg2tsYL+GC/DXUoX97djRde0v2k+p7S3bA5R9l9dwHyV3t3QD+iaqW3jnXdTe3PY9wN9tBS74UIRvazjibCm+eFCNa0u33YC/PXlSlWd7ObJ7Sb756E99DjnpqMu3tXipt+7667DHLvvstNdu+3IMR6y57mcXbHC8uyvMu4DvBp+78Acgn3vyDg//u+8AHmx888sfbz311+/OvMTL3+49lSOFL/745Jdv/vnop6/++uy3P9H38Mcv//z01/9V5/bn7xj++vffF//+C6DZHCTAAqbJUAZMYGYAqMAGjoWBDoygVyAowQpmhYIWzCBVMKjBDsYNgR4MoQhHSMISmvCEKEyhClfIwhYeMEMwjKEMZ0jDGtrwhjjMoQ53yMMDBQQAIfkEAQoABwAsAAAsAFgCVACC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ACwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4oseKCkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrSo0aNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdy7btzY1u48qdS7euXZ9w7+rdy7evX7J5/woeTLiw4Z+BDytezLjx4MSOI0ueTHks5MqYM2venPQy58+gQ4t+6Xm06dOoK5dOzbq1a7+rX8ueTTtt7Nq4c+venRuA79/AgwsfTry48ePIkytfzry58+fQo0ufTr269evYs2vfzr279+C8w4v/H0++vPnz6NOrX8++vfv38JEqTCmRPkSVEfHfR1mff37/+530H4AO6feQfQGa1J+ACSrYYEkDOliAARRWaOGFGGLIEIIHEtiQgQV6+KGIG5K4EIcjmkhQfCzSZcAAMMYo44w0whhAAAa0qCNYDmWI4Y5DvVjjkDXemCOQSGrl44UENOnkkwIIkGRQQhJppY04TqnllpNVeeWVR3IpJm63+eXll0SGOaZMC665U5NfldnXmWjWqKabpEWIp15y8kVnnTEauWdMbQ6KEwF3YtXnXn8COoCghrpUqJgT6hRlolctqlejgEIaKUuTclnpoQcIgKlVmt615Ko+ftpSqFuO/3oTAaWeWlWqrqIGq5YFHPDkr04ewKqwXeGaq2m7askqhQeUyJWxx4qWbLRnQUvtZ9NeC5hG2uaWbbfghsuVcyk1p5K55TKXrrookdsuuifBGy+789Jrkrv1Lreucufae6+/B+D7L8DyDszvvskhnPC7BBccMMAP61uuuBRXbPHFGGes8cYcd+wxtfOpmBCKzjLYocgHkVwyhHo223LLLp9scoglLWszhS8/GDPNM6coocwsPzgpzDkLXTTQOy90c4Yff8VpnZ42XdnTVkrdFdVfRm21ZFgPufVWXVup9deNhU0j2VmZbaWtaBemtoxtX/X2kGxnbC3YjsIdd1Vz2/+59d1K5h3j3nwLPuPYHQOetuEDEE5V3zIizrHiWEHu+FSQB5ql1ZTLzfjlUi297N/cxiX6haCnHlbnqre+G+uux04b7LLX3hrttueObOm69+77VAKX5HDEBzNcvMHHC+9w8MQvbLzzyEOvfMPUJz+99c0jp7D2z0uf/XH9Stw9+OOTn6/1zA+v/u/st+/++/DHL//89O850v3456///vz37///AAzgQU63KgGGpH4ILErmsFS3BDrQMQt81OYeSMGpMU5zDaygBgcTQRhlcIMg9NMFZfTBEJqwLh0cQAlPyEK3RFByLYzhXF44QRna0EUjlOAKb8jDsBBwST0MohAnh0jEIhrxiEhMohKXyMQmUuY7UIyiFKdIxSpa8YpYzKIWt8jF5wQEACH5BAEKAAcALAAAPQBYAkUAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AAsIHEiwoMGDCBMqXMiwocOHECNKnEixosWLGDNq3Mixo8ePIEOKLHigpMmTKFOqXMmypcuXMGPKnEmzps2bOHPq3Mmzp8+fQIMKHUq0KEoASJMqXcq0qdOnUKNKnUq1qtWrWLNq3cq1q9evYMOKHUu2rNmzaJcaXcu2rdu3cOPKnUu3rt27ePPq3cu3r9+/gAMLHky4sOHDiHtuTMy4sePHkCNLprl4suXLmCU3zBzZgOfPoEOLFq2yMufTJ0erXv0ZtevSMFl7PmDgdWIDA3Lr3s27d+4AAWqnNG07M27fyH0DF178dQHmNgkcEAC9ueDjybP/Dl5ao3Xj2sMv///u+nlO6dTJD8YeHvn44d7VT2bfvvd7+ZjN5xSQHv9f+vX1Vl1JxPnXGIAB7jaggY/pd96CDNqFYIK5LVhghIdNSCGEGCLmYIfzUYichfGBmKGIvnFoYnMESLfiXRrWd99JF74IWIztzWjjjjwKhaN43MGXUY/XobibjkQmqWRNP2qH5JKvNZndk1Ay1uKVWBLAX5VtyeYlaFw29+WYYUL2ZZlopqnmmmy26eabcMYp55x01mnnnWtKJKRD3T3UJ58o6RloRHsCSiNEf242KKKL+tmooSYRWihDiVL6qKKHMpoppJFquimmnXpKoKSfgjqqo5cuBBuerLbqKmJXpf8Uq6xWqTTrSVjRWutRu/Jala29mpSrr78SS5WuxyI7lbLLGtssrrcKG6y0xTobFbNSYXuttVABWy2035bE1Kvklmvuueimq+667NpVY7vwVgQvj+/Ou6689n5Hppec5kvnmK2tpqq/YtbH0nL9EhynlLqlhLBCChcn5cHBJRyxmwzn5nDFEF/s2sQrPWypxwtTuLEBA5PMGcgsoWyqymxmPMBKLicE88oGt1RzyjfHbLJKOx/UM2Ys02zx0FzKbHTHSEtWNEoi89x0mUpDzbHNU3dmpG7AHXB01knKbF8AXjMN9oFbb6fi2WuKzRuVbHcGsGxxzzk33XXnrffefPf/7fffgAcu+OCEVyloqVKfmjCpoaLaeL+HPz4y4mYrDjnjlr+MeeaJl+0454lHDjrWqVY+OkKTTn466qWbvrnnixcu++x0RSvutLeHm3uy4Oo+bO/PAp+ttk8RXzy33SJ/vPDbKt+U8U5B/7zz0zOfPPVqSZ+99ddT6zvuB9i+e/DjA0D7+einr/767LfPdr3ux683/PLXfzZBd4dGW2v29//+QGtbyZGC5L8CNg1/OFGbARc4NATeRIEMjCDJHGgTBUnwghGj4EwEhMEO5kuDMuGgB0fILhDGZIABJKEK7USQA6RtAHBboQxZ5TYUzvCG5crfanDIwx768IdADKIQNYdIxCK+ZCRITKISl8jEJjrxiVCMohSnSMWJGPGK5kqLFrfIxS568YtgDKMYx0jGMpoxKwEBACH5BAEKAAcALAEARABXAjcAgv///5P//0n/AGTI/zLIAGRkZDw8PAAAAAj/AA8IHEiwoMGDCBMqXMiwocOHECNKnEixosWLEw1o3Mixo0ePEwuIHEmypEmTGFOqXMkS48eXMDdKPEmz5siWOHPq3Mmzp8+fAw0MGEq0qNGjQwMEMBDSptOSQKNK/SkUqVWkSplGfMpV5NSvYMOKHRu16tWzSZc27eqUrNu3Cs2ivZp1Jtu2cPPq3cu37Fy0de3erdm3MFi5f48GhjjYpuHHkCNLLog48VGtWxvTnMy5ZWXLRTE/1Ly5s+nTqKmCvip6NGmUqWND/Lx6QOuGr2HL3s27N0Paq2/jzk3St3HKtZEKX0i8+PHn0FMDT7yYcXOv0X1P/1vd4XXs2cOL/++7fW5379/H7y4PWG3m6+rjyxfL/uz54fDnn65P1731/PoFKGBOMRUIkmDNDciZgQxqhCBxCkYo4YQUVmjhhRhmqOGGHHbo4YcghijiiCSWaOKJKKao4oostujiizD2BJVCANRo4wG6HWTjjjjOiNCONp5EI5A9+qgjkTkeyWOSBQFZY5E3JeQkAFCCp+SNQkqJpJFXPslkk1s696OTVRYw5JJfDkRmlmOiKWabWHIJZphRwhnnmwatmaZAeuKZJ51W/gmomVoOeuadgcao6KKMNuroo5BGKumklFZq6aWYZqrppoU1aGCZpHEKo6efghqqqMfxZ5VSpjaGqouqYv/l3neEvtpbrIoF0OpgtrKIq1GB0drrrckVxWp6w6b4q7GzIptsbMsatetdz6IYbWgCCVutdMVK6+y2I15LlFbagmuauENNy5a5JKJrW7bfsiuZuMcCKO+H9DZr772Q5atuV/yCmC+58Qbcabdp/cuVwR4ODO++DPNFakwKPxUxhxNTXLHFF3fs8ccghyzyyCSXbPLJKKes8sost+zyyybKOSeiiRI0ZZmHBrnnAX36abOhOVO5881D91yzmoPW2qXQMv/sZp12Mu0z0kBHDWrQVxdKs9JL46z11kFLDbWgW3M9s5dNUw3212hPzafRZjtNM8x012333XjnrffefPdi7fffgAce1EsPui344dUC51BWhCHuOLuKN8R4aY9X/mzkDE3OpuWcv4p5QwY03vnonH7+m+ikp36p6QuFTrnqsE/KOkKa7xz77aNetfhSqOPuu6KzH1R72r8Xryzh79luWkAAIfkEAQoABwAsAQBEAFcCNwCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQI0qcSLEiRAMYM2rcyJFjxQIgQ4ocSZKkxZMoU6pcibCjy5cZJZacSTMky5s4c+rcybOnz4QGBggdSrSoUaEBAhj4WLPpyJ9Qo0o9EPSo1aNJl0Z0yhXk1K9gw4od67Pq1bNIlTLt2pSs27cLzaK9mlUm27Zw8+rdy7fsXLR1Kd7F27fwV7l/jQaGOLim4ceQI0sWiDixUa0TG9OczBlnZctEMT/UPLOz6dOoe34GLVT0VtImU8uOuJq164awY8/ezbs3UNZWb4/OLdK3cYO1QQtfSLz48efQTSf/u9hu8wLRj0+fW93hda/Zw4v/37sdsFrB38fzLn+2O+706uPLn8qe7vnM8Oefrm/VPcPv2Okn4IA3wWSgR+hdR2BnBzaIkXXNLSjhhBRWaOGFGGao4YYcdujhhyCGKOKIJJZo4okopqjiiiy26OKLMMaYHQA00qhbQjXSeMCNCOUIwI5PKeTjj0EK6SOQNi00JI8HLVkkjjkimaSRNUoZIJVVPtmjj0wa5KRzWBKpZZNRdknQkFYyhOaYXnLJZkFfTglllm+eWSaYc9Ip55Y5mmlnn3UOdKSfgt6JJ5lVyqjooow26uijkEYq6aSUVmrppZhmqummkQHIKYsONkiQp596SGqpKPKH1X1W5oZqh6e+/1qiqoqxGqusF96Ka4i0FlWdrrtOCGywHfZ62aj5EUvhsMpqaGxRojHb7HzSTmvhs6Ehq6C1ElbL7YTYDvBrst9C9qBK3pZLYLjjbquuuQNodFK67+rHrq3k1ksetDHh566+FYZ6oLYRAtxXbfIarPDC45XXL8MQRzwbfwlLbPHFhT1bMcYcd3wYcIqd6/HIJOsUrrislqwySm4eiqiOhAo0aKAHrOlym4aCFyYAMdfc8p4vi1kcRiAT5Z7NQONcY89x6pyn0EnDmbPTQUN95dNWq/kz1VUzPfXVfMJ8s9RTaw3o2H8uTbPPekadttVgK/2jxxoVjdRyK+ett0N1g4CM996AB95SRsoJbvjhDBFu39+IN35430an7Ph6L71W2uTy9e0f5qkl11BWjnFuL0aSi46a5wyBvpnpAjLOumSoL6T65a/Xbm3siYdu++7E4h6X7rwH/6rvChkAvPDIb0r8QbOXlPzzyp/1uVLHQ2+9pMsb1HzP13cPI0yWO69eQAAh+QQBCgAHACwBAEQAVwI3AIL///+T//9J/wBkyP8yyABkZGQ8PDwAAAAI/wAPCBxIsKDBgwgTKlzIsKHDhxAjSpxIsSLEAhgzatzIkaPFjyBDihwZ0oDJkyhTqlRJsKPLlxlJypxJs6bNmzhz6owIs6fHnUCDCm1oYIDRo0iTKjUaIICBlj6jFhhKtarVq1izjpQaVavXrxaLLh27tOnTgVx9gl3Ltq3btw/T9oRLt61YsniZOoUq12Xdv4ADC67Z9+Xgw0Lv5s17VmBhv4gjS56M+HFHyphlKl5MtvEByz8zix5NeijojaVTR9zMeann0xpVy55NmyLsmLVzE2Td+qhZvrB1Cx+e+zZG4rl59x7wG61x5NCjYzY+Vbpq5b2bO35uvbt3utS/k/9eSb78SeCnxatfz769+/fw48ufT7++fbsm7+vfz78/TbEp+SfggAQWeABvLBmo4IIMdofdUQk2KKF/qAFgoYUMoXbAhRguVCGHAGQYG4gNfXhhiSNyKGKKJ3rIYosKmdghQg+OFeB2MZHo4osh7njchir6OBWIPcbII4o5BmnkkUIOqWRCMs4IJZNCAgnjksfpiGWWTyIUZZFTJtnlQbFNSFuNnJ1n5ppstgkWmotp5+acdNZ5E5x4yWnnnnz2KRGeZOnp56CEFnrgcp0Z+p55K22plqJ0ooSoUp5BKh6gRhlkllyW7pnSpJV26iCimjrFqah9qhToXqheSmpBm6b/1aqhn/rG6qyjLkfjqbgqmtKtvUaH6QC7yhqsqPkdK+yrBhnAq7LQRpvasKU6a6y00f2IrafMDhQrV9tmq2G4bVILq6nXkqsbTOqayWiEZKbbLm1SzWvvvbM9i+++/A5mWb8AB9wWdwIXbHBQ1FV38MIMy5RwwxCLFOWKP2oZJpdjGjQxxU5eeXHHUnq58ZZWnkgkiBydnPHIJFssMpUfC+RyvGJ6TDPGIb+MM5g678wxyDzfDHTQGsMc88xCl0x0QR4hzTSPS6OndNTOaSvwwxFnrbVEt23t9dcO/Qv22GT3PFfZaOP77tr5dZX2aFZXlOzbdQ2rlJ6G0Z0ZRqE+eaRX33qvZXdSglYdd+CVFQB4Q38jDtfgSBXOtOOU8V1R45TbNalrmdOr+EdGLd65VZAnJfrok1luUeiov7k5pa2npvrlA5weO1ClY367aLNLpPvupL9uq+3AA9Z7RL8XP1TuzAGrfGTHQ5T887iz/S71qSscFvGyBQQAIfkEAQoABwAsAQBEAFcCNwCC////k///Sf8AZMj/MsgAZGRkPDw8AAAACP8ADwgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyPGigY8gQ4r8qLCAyZMoU5rsyLKly5cwY8qcSbOmzZs4cz40MKCnz59AexoMEMCAyqModSpdyrSp06dQo0qdCpFn0Ks+hxZFypWq169gw4odS7YsRqtYr2o1yvWo2bdw48qdS7fuzrRYEbJtm9Ku37+AAwsezBIt3p96+aokzLix48eQ4Ro+LPTgXsUrI2vezLmz546TKa/FnPSz6dOoU3cOfXg06cyqY8ueTbssa7yuX9fezbu3b5sjg4cs+brA7+PIkytfzry58+fQo0snfHK69evYnSPNzr2799Okv4v/H0++bvHq5dP7BsC+/YHFCdvLdxtfPoD38BHav59fv338pdUnH4Do+fdffwbtR2CBBymI4ED7zZdShPcJ5GBfAg74IEER0megex42eCCGGbK3oHElmrghhPuFmOCIJIqoYYwygkjjizMGWKONOuLIY48FXXgjh+6pF9h5fBmp5JJMdoRkW01GKeWUDT25HZVYZkmllS5q6eWXywknpgEHCIckmKkNZxGQaHZ3m2gCEXVZeG2aZthEcrJZJ3ZvUtZTnuft+dmdEgEKm6Dc9eknUScmiehqiBW6FYOPXqeonwOQSZBilXJG6ESXdZrdpX5qilCXojL2qUShpmodqZSZ/8rQoa6qGimoetYaJqZpyanrbKs+ZCiKvz4Ha69FFRtbsA4Nq6yxvGLl67OoMduQs9QyN+a2I2V7mpoV5ertuOSWa+656Kar7rrstuvuu/DGK++89NbLm5CUBtniihbCKC6+tPqo4pBE5phvwQYHjPDABPebMLEfMnzwwvw1fECEJyrUIb8X+zuxwz8qTHHGKVb8774WAywyyBKvzLLJH79McsQtQywwzDGrbLO+Jtrr889ABy300EQXbfTRSCd9lkY7K00dZg1xS5LTNfFkUQBlNk31kZjJWhCm024tk9UVYW2U2I2R5jVBYCeLdkxkU2S21m+b13VCba9dN0dxV1Z09t5cK6Y326UC7lLfFP1tuF9qL4Tp4ItbhDiukTN+t0KPV873AFdnrbndguO9qNufn8V52Z6XLlfjCOWtuumdK/66Wawf5PrsfjON+1t0LiQ15HQFBAA7",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reset environment\n",
    "env.reset()\n",
    "\n",
    "frames = []  # for storing the frames captured during the simulation\n",
    "\n",
    "# Simulate the environment\n",
    "for _ in range(50):\n",
    "    action = env.action_space.sample()  # choose a random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    frames.append(env.render())\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "# Close the environment to free resources\n",
    "env.close()\n",
    "\n",
    "generate_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the objective:\n",
    "\n",
    "The agent's object is straighforward: maximize average speed while minimizing collisions, with an additional goal of staying in the right lane as much as possible for extra reward. These objectives align well with the rewards settings described (collision_reward, high_speed_reward, and implicitly mentioning a right_lane_reward). This clarity will guide the design of your reinforcement learning model and the reward structure you implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavy Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.7724457 ,  0.6666667 ,  0.3125    ,  0.        ],\n",
       "        [ 1.        ,  0.10608794, -0.6666667 , -0.04544504,  0.        ],\n",
       "        [ 1.        ,  0.23279905, -0.33333334, -0.01973059,  0.        ],\n",
       "        [ 1.        ,  0.36426824, -0.33333334, -0.02661807,  0.        ],\n",
       "        [ 1.        ,  0.49082628,  0.        , -0.03075124,  0.        ]],\n",
       "       dtype=float32),\n",
       " {'speed': 25,\n",
       "  'crashed': False,\n",
       "  'action': 3,\n",
       "  'rewards': {'collision_reward': 0.0,\n",
       "   'right_lane_reward': 1.0,\n",
       "   'high_speed_reward': 0.5,\n",
       "   'on_road_reward': 1.0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance with Stable Baselines3\n",
    "\n",
    "Stable Baselines3 is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Reinforcement Learning agents can be trained using libraries such as eleurent/rl-agents, openai/baselines or Stable Baselines3. In the next cell, a SB3 DQN is trained on highway-fast-v0 with its default kinematics observation and an MLP model. The hyperparameters come from the [documentation](https://highway-env.farama.org/quickstart/#training-an-agent) and, for a first training, the default values are used. Further tuning is performed next in this notebook.\n",
    "\n",
    "Before the fine-tuning process, the chosen hyperparameters for the DQN model were set with a mix of default values and some tailored adjustments expected to work well for a range of problems:\n",
    "\n",
    "- `net_arch=[256, 256]`: This specifies a neural network architecture with two hidden layers, each consisting of 256 neurons. This relatively large network architecture is chosen with the expectation that it can capture the complexity of the state space and learn effective policies.\n",
    "\n",
    "- `learning_rate=5e-4`: The learning rate determines the step size at each iteration while moving toward a minimum of the loss function. A rate of 0.0005 is a common starting point that is not too large to overshoot minima nor too small to slow the convergence.\n",
    "\n",
    "- `buffer_size=15000`: This is the size of the replay buffer. A larger buffer size allows the agent to learn from a broader set of experiences, potentially smoothing out the learning process and avoiding overfitting to recent experiences.\n",
    "\n",
    "- `learning_starts=200`: This parameter determines how many steps of the model are run before the learning process begins. This warm-up period allows the replay buffer to fill with experiences, ensuring that learning does not begin with a poor understanding of the environment.\n",
    "\n",
    "- `batch_size=32`: This sets the number of experiences to sample from the buffer to update the network at each learning step. A batch size of 32 is a standard choice that balances the variance and convergence rate.\n",
    "\n",
    "- `gamma=0.8`: The discount factor, gamma, balances immediate and future rewards. A value of 0.8 suggests that future rewards are taken into consideration significantly, but with a preference for more immediate rewards.\n",
    "\n",
    "- `train_freq=1`: This sets the frequency of training the network. Here, the network is trained at every step.\n",
    "\n",
    "- `gradient_steps=1`: This is the number of gradient steps taken for each training step. Since this is set to 1, it means that a single optimization step is taken for each training step.\n",
    "\n",
    "- `target_update_interval=50`: This parameter sets how often the target network is updated. The target network stabilizes training by providing a fixed snapshot of the policy for a period of time. An interval of 50 is a balance between stability and responsiveness to changes in the policy.\n",
    "\n",
    "- `verbose=1`: This sets the verbosity level to output detailed logs during training.\n",
    "\n",
    "- `tensorboard_log=\"highway_dqn_baseline/tensorboard_logs\"`: This argument points to the directory where TensorBoard logs should be saved, enabling visualization of the training process.\n",
    "\n",
    "These hyperparameters were chosen to provide a balance between exploration and exploitation, learning efficiency, and computational resources. They are often selected based on empirical results from similar environments or as a starting point for further refinement through fine-tuning or hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to highway_dqn_baseline/tensorboard_logs/DQN_2\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 11.8     |\n",
      "|    exploration_rate | 0.97     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 69       |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 63       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | 8.1      |\n",
      "|    exploration_rate | 0.958    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 69       |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 88       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | 9.62     |\n",
      "|    exploration_rate | 0.927    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 154      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.4     |\n",
      "|    ep_rew_mean      | 7.72     |\n",
      "|    exploration_rate | 0.921    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 167      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 8.15     |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 219      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0541   |\n",
      "|    n_updates        | 18       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.4     |\n",
      "|    ep_rew_mean      | 7.79     |\n",
      "|    exploration_rate | 0.881    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 250      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0351   |\n",
      "|    n_updates        | 49       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10       |\n",
      "|    ep_rew_mean      | 7.55     |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 281      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 80       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.3     |\n",
      "|    ep_rew_mean      | 7.82     |\n",
      "|    exploration_rate | 0.843    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 330      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 129      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.4     |\n",
      "|    ep_rew_mean      | 7.88     |\n",
      "|    exploration_rate | 0.822    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 64       |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 374      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.474    |\n",
      "|    n_updates        | 173      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.3     |\n",
      "|    ep_rew_mean      | 7.87     |\n",
      "|    exploration_rate | 0.803    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 64       |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 414      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.208    |\n",
      "|    n_updates        | 213      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | 7.7      |\n",
      "|    exploration_rate | 0.789    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 445      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 244      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.4     |\n",
      "|    ep_rew_mean      | 7.9      |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 498      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 297      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | 7.78     |\n",
      "|    exploration_rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 530      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 329      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | 7.82     |\n",
      "|    exploration_rate | 0.727    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 574      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 373      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | 7.7      |\n",
      "|    exploration_rate | 0.712    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 606      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.449    |\n",
      "|    n_updates        | 405      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | 7.65     |\n",
      "|    exploration_rate | 0.693    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 646      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 445      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10       |\n",
      "|    ep_rew_mean      | 7.61     |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 683      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.502    |\n",
      "|    n_updates        | 482      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.81     |\n",
      "|    ep_rew_mean      | 7.41     |\n",
      "|    exploration_rate | 0.665    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 706      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 505      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.59     |\n",
      "|    ep_rew_mean      | 7.24     |\n",
      "|    exploration_rate | 0.654    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 729      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 528      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.55     |\n",
      "|    ep_rew_mean      | 7.24     |\n",
      "|    exploration_rate | 0.637    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 764      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 563      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.63     |\n",
      "|    ep_rew_mean      | 7.29     |\n",
      "|    exploration_rate | 0.616    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 809      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.491    |\n",
      "|    n_updates        | 608      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.51     |\n",
      "|    ep_rew_mean      | 7.21     |\n",
      "|    exploration_rate | 0.602    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 837      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.282    |\n",
      "|    n_updates        | 636      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.51     |\n",
      "|    ep_rew_mean      | 7.21     |\n",
      "|    exploration_rate | 0.584    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 875      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 674      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.78     |\n",
      "|    ep_rew_mean      | 7.44     |\n",
      "|    exploration_rate | 0.554    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 939      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.216    |\n",
      "|    n_updates        | 738      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.86     |\n",
      "|    ep_rew_mean      | 7.49     |\n",
      "|    exploration_rate | 0.532    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 986      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.248    |\n",
      "|    n_updates        | 785      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.59     |\n",
      "|    ep_rew_mean      | 7.3      |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 1022     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.59     |\n",
      "|    n_updates        | 821      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | 7.78     |\n",
      "|    exploration_rate | 0.473    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 1110     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.75     |\n",
      "|    n_updates        | 909      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | 7.77     |\n",
      "|    exploration_rate | 0.44     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 1178     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.209    |\n",
      "|    n_updates        | 977      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.3     |\n",
      "|    ep_rew_mean      | 7.84     |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 1199     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.409    |\n",
      "|    n_updates        | 998      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 7.96     |\n",
      "|    exploration_rate | 0.399    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 1265     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.666    |\n",
      "|    n_updates        | 1064     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.17     |\n",
      "|    exploration_rate | 0.37     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 1326     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.428    |\n",
      "|    n_updates        | 1125     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.2      |\n",
      "|    exploration_rate | 0.352    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 1365     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.306    |\n",
      "|    n_updates        | 1164     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | 8.37     |\n",
      "|    exploration_rate | 0.316    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 1439     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.278    |\n",
      "|    n_updates        | 1238     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.5     |\n",
      "|    ep_rew_mean      | 8.66     |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 1520     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 1319     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | 8.87     |\n",
      "|    exploration_rate | 0.246    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 1587     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.113    |\n",
      "|    n_updates        | 1386     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12       |\n",
      "|    ep_rew_mean      | 9.1      |\n",
      "|    exploration_rate | 0.218    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 1647     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.542    |\n",
      "|    n_updates        | 1446     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | 8.97     |\n",
      "|    exploration_rate | 0.202    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 1680     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 1479     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | 9.02     |\n",
      "|    exploration_rate | 0.184    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 1718     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.121    |\n",
      "|    n_updates        | 1517     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.4     |\n",
      "|    ep_rew_mean      | 9.44     |\n",
      "|    exploration_rate | 0.137    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 1817     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.365    |\n",
      "|    n_updates        | 1616     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | 9.76     |\n",
      "|    exploration_rate | 0.105    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 1885     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0332   |\n",
      "|    n_updates        | 1684     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | 9.87     |\n",
      "|    exploration_rate | 0.0837   |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 1929     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 1728     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 10       |\n",
      "|    exploration_rate | 0.0586   |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 1982     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0229   |\n",
      "|    n_updates        | 1781     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | 10.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 2039     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.399    |\n",
      "|    n_updates        | 1838     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 2107     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 1906     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14       |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 2167     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 1966     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 2254     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2053     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 2316     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 2115     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 11.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 2360     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.191    |\n",
      "|    n_updates        | 2159     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 11.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 2427     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 2226     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 11.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 2484     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 2283     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 11.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 2520     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 2319     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 2567     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.303    |\n",
      "|    n_updates        | 2366     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 2605     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.256    |\n",
      "|    n_updates        | 2404     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 2668     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.282    |\n",
      "|    n_updates        | 2467     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 2739     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2538     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 11.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 2817     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.267    |\n",
      "|    n_updates        | 2616     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 2870     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 2669     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 2937     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 2736     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 12       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 3031     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 2830     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 3120     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 2919     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 3181     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 2980     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 3220     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.341    |\n",
      "|    n_updates        | 3019     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 12.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 3276     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0814   |\n",
      "|    n_updates        | 3075     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 12.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 3331     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 3130     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 3405     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 3204     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 3459     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 3258     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 3510     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0205   |\n",
      "|    n_updates        | 3309     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 3601     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.294    |\n",
      "|    n_updates        | 3400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 12.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 3650     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0275   |\n",
      "|    n_updates        | 3449     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 12.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 3747     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 3546     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 3817     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0201   |\n",
      "|    n_updates        | 3616     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 3850     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 3649     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 3908     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 3707     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 12.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 3974     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0215   |\n",
      "|    n_updates        | 3773     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | 12.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 4083     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0977   |\n",
      "|    n_updates        | 3882     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 4173     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 3972     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 4222     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.235    |\n",
      "|    n_updates        | 4021     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | 13.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 4289     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0539   |\n",
      "|    n_updates        | 4088     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.7     |\n",
      "|    ep_rew_mean      | 13.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 4342     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 4141     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 13.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 4434     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 4233     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 4536     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.264    |\n",
      "|    n_updates        | 4335     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 4646     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.336    |\n",
      "|    n_updates        | 4445     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 4718     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 4517     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 13.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 4776     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 4575     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 4856     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0659   |\n",
      "|    n_updates        | 4655     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 4949     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 4748     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 5012     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0792   |\n",
      "|    n_updates        | 4811     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 5090     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 4889     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 5136     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.413    |\n",
      "|    n_updates        | 4935     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 5211     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 5010     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 5282     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 5081     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 5379     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.094    |\n",
      "|    n_updates        | 5178     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 5456     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.546    |\n",
      "|    n_updates        | 5255     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 5536     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.241    |\n",
      "|    n_updates        | 5335     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 5616     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.082    |\n",
      "|    n_updates        | 5415     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 5679     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0916   |\n",
      "|    n_updates        | 5478     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 5769     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 5568     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 5827     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0295   |\n",
      "|    n_updates        | 5626     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 5893     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0937   |\n",
      "|    n_updates        | 5692     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 5992     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0497   |\n",
      "|    n_updates        | 5791     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 6059     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 5858     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 6127     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 5926     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 6221     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.241    |\n",
      "|    n_updates        | 6020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 6291     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.222    |\n",
      "|    n_updates        | 6090     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 6346     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.236    |\n",
      "|    n_updates        | 6145     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 6428     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0908   |\n",
      "|    n_updates        | 6227     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 6497     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0953   |\n",
      "|    n_updates        | 6296     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 6576     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.146    |\n",
      "|    n_updates        | 6375     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 6641     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.243    |\n",
      "|    n_updates        | 6440     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 6725     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 6524     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 6817     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 6616     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 6897     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0809   |\n",
      "|    n_updates        | 6696     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 6967     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.148    |\n",
      "|    n_updates        | 6766     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 7045     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 6844     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 7121     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.00797  |\n",
      "|    n_updates        | 6920     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 7161     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0837   |\n",
      "|    n_updates        | 6960     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.3     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 7212     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.119    |\n",
      "|    n_updates        | 7011     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 7280     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 7079     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 7335     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.079    |\n",
      "|    n_updates        | 7134     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 7406     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.24     |\n",
      "|    n_updates        | 7205     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 7490     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0864   |\n",
      "|    n_updates        | 7289     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 7523     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0353   |\n",
      "|    n_updates        | 7322     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 7580     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 7379     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 7669     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 7468     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 7747     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 7546     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 7825     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.238    |\n",
      "|    n_updates        | 7624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 7893     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0182   |\n",
      "|    n_updates        | 7692     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 7962     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 7761     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 8049     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 7848     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total_timesteps  | 8129     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0769   |\n",
      "|    n_updates        | 7928     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 8231     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.11     |\n",
      "|    n_updates        | 8030     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 8288     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 8087     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 8372     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0979   |\n",
      "|    n_updates        | 8171     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 8458     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 8257     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 8534     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 8333     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total_timesteps  | 8607     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.261    |\n",
      "|    n_updates        | 8406     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 8684     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.244    |\n",
      "|    n_updates        | 8483     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 8740     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 8539     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 8803     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.119    |\n",
      "|    n_updates        | 8602     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 8872     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.229    |\n",
      "|    n_updates        | 8671     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 8983     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0321   |\n",
      "|    n_updates        | 8782     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 9092     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.235    |\n",
      "|    n_updates        | 8891     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 9162     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.129    |\n",
      "|    n_updates        | 8961     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 9234     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0598   |\n",
      "|    n_updates        | 9033     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total_timesteps  | 9314     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0968   |\n",
      "|    n_updates        | 9113     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 9413     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.203    |\n",
      "|    n_updates        | 9212     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 9471     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.283    |\n",
      "|    n_updates        | 9270     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 158      |\n",
      "|    total_timesteps  | 9591     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0631   |\n",
      "|    n_updates        | 9390     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 9682     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0962   |\n",
      "|    n_updates        | 9481     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 16.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 9778     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 9577     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 163      |\n",
      "|    total_timesteps  | 9883     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 9682     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total_timesteps  | 9947     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 9746     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total_timesteps  | 10021    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0946   |\n",
      "|    n_updates        | 9820     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 10115    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 9914     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 10191    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 9990     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total_timesteps  | 10291    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.113    |\n",
      "|    n_updates        | 10090    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 16.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 171      |\n",
      "|    total_timesteps  | 10385    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.104    |\n",
      "|    n_updates        | 10184    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 16.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 10476    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0929   |\n",
      "|    n_updates        | 10275    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 174      |\n",
      "|    total_timesteps  | 10554    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0441   |\n",
      "|    n_updates        | 10353    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 176      |\n",
      "|    total_timesteps  | 10654    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.237    |\n",
      "|    n_updates        | 10453    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 177      |\n",
      "|    total_timesteps  | 10732    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0325   |\n",
      "|    n_updates        | 10531    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 178      |\n",
      "|    total_timesteps  | 10811    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 10610    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total_timesteps  | 10911    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0778   |\n",
      "|    n_updates        | 10710    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total_timesteps  | 11031    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 10830    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 184      |\n",
      "|    total_timesteps  | 11129    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 10928    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 185      |\n",
      "|    total_timesteps  | 11206    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 11005    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 186      |\n",
      "|    total_timesteps  | 11246    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 11045    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 188      |\n",
      "|    total_timesteps  | 11366    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 11165    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 189      |\n",
      "|    total_timesteps  | 11463    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 11262    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 17.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 191      |\n",
      "|    total_timesteps  | 11561    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0679   |\n",
      "|    n_updates        | 11360    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 193      |\n",
      "|    total_timesteps  | 11680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.11     |\n",
      "|    n_updates        | 11479    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 195      |\n",
      "|    total_timesteps  | 11800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 11599    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 197      |\n",
      "|    total_timesteps  | 11919    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0798   |\n",
      "|    n_updates        | 11718    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 12025    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0329   |\n",
      "|    n_updates        | 11824    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 12135    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0175   |\n",
      "|    n_updates        | 11934    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 202      |\n",
      "|    total_timesteps  | 12234    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.301    |\n",
      "|    n_updates        | 12033    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 204      |\n",
      "|    total_timesteps  | 12337    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0181   |\n",
      "|    n_updates        | 12136    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total_timesteps  | 12440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.262    |\n",
      "|    n_updates        | 12239    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 207      |\n",
      "|    total_timesteps  | 12552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 12351    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total_timesteps  | 12655    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.104    |\n",
      "|    n_updates        | 12454    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 12732    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0929   |\n",
      "|    n_updates        | 12531    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 12852    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.152    |\n",
      "|    n_updates        | 12651    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 214      |\n",
      "|    total_timesteps  | 12948    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 12747    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.1     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 13061    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0928   |\n",
      "|    n_updates        | 12860    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 217      |\n",
      "|    total_timesteps  | 13158    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 12957    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 13270    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 13069    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.8     |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 221      |\n",
      "|    total_timesteps  | 13390    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 13189    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 13465    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 13264    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 224      |\n",
      "|    total_timesteps  | 13571    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0558   |\n",
      "|    n_updates        | 13370    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total_timesteps  | 13658    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0845   |\n",
      "|    n_updates        | 13457    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 227      |\n",
      "|    total_timesteps  | 13744    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 13543    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.9     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total_timesteps  | 13841    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.086    |\n",
      "|    n_updates        | 13640    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total_timesteps  | 13940    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 13739    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26       |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 14060    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0641   |\n",
      "|    n_updates        | 13859    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26       |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 234      |\n",
      "|    total_timesteps  | 14164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 13963    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.8     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 235      |\n",
      "|    total_timesteps  | 14259    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.123    |\n",
      "|    n_updates        | 14058    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 237      |\n",
      "|    total_timesteps  | 14333    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 14132    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total_timesteps  | 14406    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0349   |\n",
      "|    n_updates        | 14205    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total_timesteps  | 14518    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0314   |\n",
      "|    n_updates        | 14317    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 241      |\n",
      "|    total_timesteps  | 14600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0192   |\n",
      "|    n_updates        | 14399    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 242      |\n",
      "|    total_timesteps  | 14683    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0958   |\n",
      "|    n_updates        | 14482    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 14751    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0848   |\n",
      "|    n_updates        | 14550    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total_timesteps  | 14833    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0891   |\n",
      "|    n_updates        | 14632    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 246      |\n",
      "|    total_timesteps  | 14910    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0825   |\n",
      "|    n_updates        | 14709    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 248      |\n",
      "|    total_timesteps  | 15002    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0172   |\n",
      "|    n_updates        | 14801    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 250      |\n",
      "|    total_timesteps  | 15122    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 14921    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total_timesteps  | 15227    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 15026    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 253      |\n",
      "|    total_timesteps  | 15323    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 15122    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 255      |\n",
      "|    total_timesteps  | 15408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0202   |\n",
      "|    n_updates        | 15207    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 257      |\n",
      "|    total_timesteps  | 15528    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.023    |\n",
      "|    n_updates        | 15327    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 258      |\n",
      "|    total_timesteps  | 15608    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0521   |\n",
      "|    n_updates        | 15407    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 260      |\n",
      "|    total_timesteps  | 15715    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0582   |\n",
      "|    n_updates        | 15514    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 15811    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.119    |\n",
      "|    n_updates        | 15610    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 263      |\n",
      "|    total_timesteps  | 15922    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 15721    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 265      |\n",
      "|    total_timesteps  | 16020    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0538   |\n",
      "|    n_updates        | 15819    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 266      |\n",
      "|    total_timesteps  | 16118    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0175   |\n",
      "|    n_updates        | 15917    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 268      |\n",
      "|    total_timesteps  | 16238    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 16037    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 270      |\n",
      "|    total_timesteps  | 16324    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 16123    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 272      |\n",
      "|    total_timesteps  | 16436    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 16235    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 16519    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 16318    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 275      |\n",
      "|    total_timesteps  | 16620    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0775   |\n",
      "|    n_updates        | 16419    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 277      |\n",
      "|    total_timesteps  | 16730    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0539   |\n",
      "|    n_updates        | 16529    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 278      |\n",
      "|    total_timesteps  | 16822    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 16621    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 280      |\n",
      "|    total_timesteps  | 16942    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.019    |\n",
      "|    n_updates        | 16741    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 282      |\n",
      "|    total_timesteps  | 17053    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0718   |\n",
      "|    n_updates        | 16852    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 284      |\n",
      "|    total_timesteps  | 17151    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0267   |\n",
      "|    n_updates        | 16950    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.1     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 286      |\n",
      "|    total_timesteps  | 17260    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 17059    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 287      |\n",
      "|    total_timesteps  | 17361    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 17160    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 289      |\n",
      "|    total_timesteps  | 17444    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0179   |\n",
      "|    n_updates        | 17243    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.2     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 17527    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0463   |\n",
      "|    n_updates        | 17326    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 291      |\n",
      "|    total_timesteps  | 17598    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.11     |\n",
      "|    n_updates        | 17397    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total_timesteps  | 17718    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0526   |\n",
      "|    n_updates        | 17517    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.1     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 295      |\n",
      "|    total_timesteps  | 17838    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 17637    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.2     |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 297      |\n",
      "|    total_timesteps  | 17929    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 17728    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.2     |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 299      |\n",
      "|    total_timesteps  | 18049    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 17848    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 301      |\n",
      "|    total_timesteps  | 18152    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 17951    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 302      |\n",
      "|    total_timesteps  | 18250    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0184   |\n",
      "|    n_updates        | 18049    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.6     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 18370    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0496   |\n",
      "|    n_updates        | 18169    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.6     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 306      |\n",
      "|    total_timesteps  | 18483    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0348   |\n",
      "|    n_updates        | 18282    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 308      |\n",
      "|    total_timesteps  | 18594    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 18393    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26       |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 310      |\n",
      "|    total_timesteps  | 18714    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 18513    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.9     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 312      |\n",
      "|    total_timesteps  | 18828    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0999   |\n",
      "|    n_updates        | 18627    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.6     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 313      |\n",
      "|    total_timesteps  | 18880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0352   |\n",
      "|    n_updates        | 18679    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 314      |\n",
      "|    total_timesteps  | 18972    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0277   |\n",
      "|    n_updates        | 18771    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.6     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 316      |\n",
      "|    total_timesteps  | 19081    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 18880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 318      |\n",
      "|    total_timesteps  | 19189    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.00514  |\n",
      "|    n_updates        | 18988    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 319      |\n",
      "|    total_timesteps  | 19284    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0892   |\n",
      "|    n_updates        | 19083    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 321      |\n",
      "|    total_timesteps  | 19393    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0166   |\n",
      "|    n_updates        | 19192    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 323      |\n",
      "|    total_timesteps  | 19489    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0252   |\n",
      "|    n_updates        | 19288    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 324      |\n",
      "|    total_timesteps  | 19584    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 19383    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1004     |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 326      |\n",
      "|    total_timesteps  | 19704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 19503    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.6     |\n",
      "|    ep_rew_mean      | 20.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1008     |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 328      |\n",
      "|    total_timesteps  | 19824    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0231   |\n",
      "|    n_updates        | 19623    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.8     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1012     |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 330      |\n",
      "|    total_timesteps  | 19944    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 19743    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN('MlpPolicy', env,\n",
    "              policy_kwargs=dict(net_arch=[256, 256]),\n",
    "              learning_rate=5e-4,\n",
    "              buffer_size=15000,\n",
    "              learning_starts=200,\n",
    "              batch_size=32,\n",
    "              gamma=0.8,\n",
    "              train_freq=1,\n",
    "              gradient_steps=1,\n",
    "              target_update_interval=50,\n",
    "              verbose=1,\n",
    "              tensorboard_log=\"logs/highway_dqn_baseline\")\n",
    "model.learn(int(2e4))\n",
    "model.save(\"models/highway_dqn_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 13726), started 13:37:59 ago. (Use '!kill 13726' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4a8ee53495ec920d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4a8ee53495ec920d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir ./logs/highway_dqn_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorBoard graphs display the performance of two separate training runs of a baseline model using a Deep Q-Network (DQN) algorithm. The runs are labeled as DQN_1 and DQN_2, and the plots show three different metrics: episode length mean, episode reward mean, and frames per second (fps), which can be indicative of training efficiency.\n",
    "\n",
    "From the first graph, the mean episode length for both runs increases over time, suggesting that the agent is learning to sustain longer episodes through its interactions with the environment. Both runs appear to converge towards similar performance, with DQN_1 showing a slightly higher mean episode length. The second graph shows the mean episode reward increasing over time, indicating that the agent is learning to maximize rewards. Similar to episode length, both runs show converging trends, with DQN_1 marginally outperforming DQN_2 by the end of the training.\n",
    "\n",
    "The third graph displays the fps, which remain relatively consistent throughout training for both runs, with minor initial fluctuations. The fps measure how quickly the model is processing frames, and it seems to stabilize quickly, implying a steady computational performance.\n",
    "\n",
    "Considering the final report, since the difference in performance between DQN_1 and DQN_2 is not statistically significant, and the two runs are part of a baseline before fine-tuning, it may be appropriate to report aggregated results or choose the one with the slightly better performance, which in this case would be DQN_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model Prediction - Creating frames from each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 23.500006081055453, Steps = 30\n",
      "Episode 2: Total Reward = 23.486887835703005, Steps = 30\n",
      "Episode 3: Total Reward = 23.65231596620102, Steps = 30\n",
      "Episode 4: Total Reward = 22.653207248099772, Steps = 30\n",
      "Episode 5: Total Reward = 16.902661979455726, Steps = 22\n",
      "Episode 6: Total Reward = 24.785649286899023, Steps = 30\n",
      "Episode 7: Total Reward = 23.219610652632348, Steps = 30\n",
      "Episode 8: Total Reward = 25.901320683983528, Steps = 30\n",
      "Episode 9: Total Reward = 9.486887716361112, Steps = 13\n",
      "Episode 10: Total Reward = 24.118932739240602, Steps = 30\n",
      "Episode 11: Total Reward = 6.098602219269405, Steps = 8\n",
      "Episode 12: Total Reward = 22.88688141957536, Steps = 30\n",
      "Episode 13: Total Reward = 25.62022116903626, Steps = 30\n",
      "Episode 14: Total Reward = 23.882577672449397, Steps = 30\n",
      "Episode 15: Total Reward = 23.98564109022169, Steps = 30\n",
      "Episode 16: Total Reward = 22.61843625110848, Steps = 30\n",
      "Episode 17: Total Reward = 23.483767843659518, Steps = 30\n",
      "Episode 18: Total Reward = 25.35293221979073, Steps = 30\n",
      "Episode 19: Total Reward = 25.11634807188066, Steps = 30\n",
      "Episode 20: Total Reward = 22.820221169036344, Steps = 30\n",
      "Episode 21: Total Reward = 7.017239717314708, Steps = 9\n",
      "Episode 22: Total Reward = 27.086887716632614, Steps = 30\n",
      "Episode 23: Total Reward = 25.109804534529097, Steps = 30\n",
      "Episode 24: Total Reward = 22.653554486654265, Steps = 30\n",
      "Episode 25: Total Reward = 23.486583347934932, Steps = 30\n",
      "Episode 26: Total Reward = 25.08519186372777, Steps = 30\n",
      "Episode 27: Total Reward = 22.953554502369677, Steps = 30\n",
      "Episode 28: Total Reward = 7.119752048328283, Steps = 9\n",
      "Episode 29: Total Reward = 23.883953349135275, Steps = 30\n",
      "Episode 30: Total Reward = 24.119587135839364, Steps = 30\n",
      "Episode 31: Total Reward = 23.452266072573934, Steps = 30\n",
      "Episode 32: Total Reward = 15.415306874832845, Steps = 19\n",
      "Episode 33: Total Reward = 26.550231634152144, Steps = 30\n",
      "Episode 34: Total Reward = 22.251657407164593, Steps = 30\n",
      "Episode 35: Total Reward = 24.875726120898186, Steps = 30\n",
      "Episode 36: Total Reward = 23.58569500870977, Steps = 30\n",
      "Episode 37: Total Reward = 24.78549676864251, Steps = 30\n",
      "Episode 38: Total Reward = 23.11895563867714, Steps = 30\n",
      "Episode 39: Total Reward = 23.735995300990336, Steps = 30\n",
      "Episode 40: Total Reward = 22.553554502369582, Steps = 30\n",
      "Episode 41: Total Reward = 9.179156548625556, Steps = 11\n",
      "Episode 42: Total Reward = 23.88626555312406, Steps = 30\n",
      "Episode 43: Total Reward = 24.95355371545498, Steps = 30\n",
      "Episode 44: Total Reward = 25.07763241506044, Steps = 30\n",
      "Episode 45: Total Reward = 22.186264344247963, Steps = 30\n",
      "Episode 46: Total Reward = 22.619595826255495, Steps = 30\n",
      "Episode 47: Total Reward = 6.752925351791462, Steps = 9\n",
      "Episode 48: Total Reward = 23.78558476291715, Steps = 30\n",
      "Episode 49: Total Reward = 20.286887835703006, Steps = 30\n",
      "Episode 50: Total Reward = 22.819598886457406, Steps = 30\n",
      "Episode 51: Total Reward = 25.753554502369667, Steps = 30\n",
      "Episode 52: Total Reward = 24.51893273923649, Steps = 30\n",
      "Episode 53: Total Reward = 25.385266315260363, Steps = 30\n",
      "Episode 54: Total Reward = 15.886880953816611, Steps = 19\n",
      "Episode 55: Total Reward = 25.719867534277135, Steps = 30\n",
      "Episode 56: Total Reward = 13.418931377889313, Steps = 19\n",
      "Episode 57: Total Reward = 22.91893273917831, Steps = 30\n",
      "Episode 58: Total Reward = 23.1517681925449, Steps = 30\n",
      "Episode 59: Total Reward = 22.751740481234364, Steps = 30\n",
      "Episode 60: Total Reward = 21.85235246472588, Steps = 30\n",
      "Episode 61: Total Reward = 25.719262661639174, Steps = 30\n",
      "Episode 62: Total Reward = 21.719758751592902, Steps = 30\n",
      "Episode 63: Total Reward = 21.58688586041627, Steps = 30\n",
      "Episode 64: Total Reward = 21.986425418260563, Steps = 30\n",
      "Episode 65: Total Reward = 22.586422968535054, Steps = 30\n",
      "Episode 66: Total Reward = 24.21831647561026, Steps = 30\n",
      "Episode 67: Total Reward = 25.55180628966438, Steps = 30\n",
      "Episode 68: Total Reward = 14.766101335270315, Steps = 17\n",
      "Episode 69: Total Reward = 23.453552413140407, Steps = 30\n",
      "Episode 70: Total Reward = 24.019598886457405, Steps = 30\n",
      "Episode 71: Total Reward = 25.11817627118578, Steps = 30\n",
      "Episode 72: Total Reward = 22.652266072539184, Steps = 30\n",
      "Episode 73: Total Reward = 22.78625069743766, Steps = 30\n",
      "Episode 74: Total Reward = 25.351242206812692, Steps = 30\n",
      "Episode 75: Total Reward = 21.320221169036344, Steps = 30\n",
      "Episode 76: Total Reward = 10.486425596368147, Steps = 14\n",
      "Episode 77: Total Reward = 22.686887835703008, Steps = 30\n",
      "Episode 78: Total Reward = 12.613039142990253, Steps = 15\n",
      "Episode 79: Total Reward = 21.502660898878005, Steps = 30\n",
      "Episode 80: Total Reward = 11.886887819987134, Steps = 16\n",
      "Episode 81: Total Reward = 12.717945151280405, Steps = 16\n",
      "Episode 82: Total Reward = 14.020169016995638, Steps = 20\n",
      "Episode 83: Total Reward = 20.24876458350549, Steps = 26\n",
      "Episode 84: Total Reward = 22.519555039459267, Steps = 30\n",
      "Episode 85: Total Reward = 12.177532070489127, Steps = 14\n",
      "Episode 86: Total Reward = 23.486583347934925, Steps = 30\n",
      "Episode 87: Total Reward = 23.251077665238657, Steps = 30\n",
      "Episode 88: Total Reward = 16.03205728169731, Steps = 20\n",
      "Episode 89: Total Reward = 5.420168923598683, Steps = 7\n",
      "Episode 90: Total Reward = 14.098146140977757, Steps = 19\n",
      "Episode 91: Total Reward = 22.152931524441094, Steps = 30\n",
      "Episode 92: Total Reward = 25.429599876992718, Steps = 30\n",
      "Episode 93: Total Reward = 14.931066401327621, Steps = 18\n",
      "Episode 94: Total Reward = 25.729435488813568, Steps = 30\n",
      "Episode 95: Total Reward = 22.63368049564197, Steps = 30\n",
      "Episode 96: Total Reward = 22.3198282060971, Steps = 30\n",
      "Episode 97: Total Reward = 24.185642178760986, Steps = 30\n",
      "Episode 98: Total Reward = 23.48688783570301, Steps = 30\n",
      "Episode 99: Total Reward = 23.617695679180773, Steps = 30\n",
      "Episode 100: Total Reward = 24.820221169036333, Steps = 30\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN.load(\"models/highway_dqn_baseline\")\n",
    "\n",
    "\n",
    "def evaluate_model(env, model, num_episodes=100):\n",
    "    frames = []  # Store frames for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        done = truncated = False\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Steps = {steps}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return frames\n",
    "\n",
    "highway_dqn_baseline_frames = evaluate_model(env, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model - Test Visual Inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video highway_baseline_performance.mp4.\n",
      "Moviepy - Writing video highway_baseline_performance.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready highway_baseline_performance.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"highway_baseline_performance.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Specify the frame rate (frames per second)\n",
    "fps = 20\n",
    "\n",
    "# Create a video clip from the frames\n",
    "clip = ImageSequenceClip(highway_dqn_baseline_frames, fps=fps)\n",
    "clip.write_videofile('videos/highway_baseline_performance.mp4', codec='libx264')\n",
    "\n",
    "Video(\"videos/highway_baseline_performance.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model - Hyperparameter fine tuning\n",
    "\n",
    "The fine-tuning process for the DQN model is conducted using Bayesian optimization via Optuna, which methodically searched for the optimal combination of hyperparameters within defined ranges. This approach systematically adjusts and evaluates the hyperparameters to maximize the expected outcome—here, the mean reward from the policy evaluation. The parameters explored include learning rate, batch size, gamma (discount factor), and network architecture.\n",
    "\n",
    "The fine-tuning focus on expanding the search space for gamma and altering the network architecture options. The learning rate was adjusted to lie between 1e-5 and 1e-3, and batch size variants were considered at 32, 64, and 128. The discount factor gamma's range was broadened from 0.8-0.99 in the baseline to 0.5-0.99 in the fine-tuning, allowing more exploration of the agent's long-term reward expectations. Network architecture choices were among single-layer [128], dual-layer [256, 256], and triple-layer [128, 128, 128]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-20 20:21:37,764] A new study created in memory with name: no-name-4bc95848-0f9e-424f-8247-8d39f74ae754\n",
      "[I 2024-04-20 20:27:33,047] Trial 0 finished with value: 20.104343913197518 and parameters: {'learning_rate': 0.0005170963465495028, 'batch_size': 64, 'gamma': 0.7482520915339761, 'net_arch': [128, 128, 128]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 20:33:27,169] Trial 1 finished with value: 19.475283718556167 and parameters: {'learning_rate': 0.00018556917294933184, 'batch_size': 32, 'gamma': 0.9163504464688864, 'net_arch': [128, 128, 128]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 20:39:21,235] Trial 2 finished with value: 11.769020607098938 and parameters: {'learning_rate': 1.4026570703885657e-05, 'batch_size': 32, 'gamma': 0.9171465040561244, 'net_arch': [256, 256]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 20:45:01,211] Trial 3 finished with value: 8.334700427874923 and parameters: {'learning_rate': 5.5734266784752756e-05, 'batch_size': 32, 'gamma': 0.6211077863720205, 'net_arch': [128]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 20:50:52,908] Trial 4 finished with value: 16.738425655141473 and parameters: {'learning_rate': 0.0005657273011624016, 'batch_size': 32, 'gamma': 0.7526535083841008, 'net_arch': [128, 128, 128]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 20:56:30,694] Trial 5 finished with value: 10.885256605041214 and parameters: {'learning_rate': 0.0003770549917138347, 'batch_size': 32, 'gamma': 0.7132458973449161, 'net_arch': [128]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 21:02:12,818] Trial 6 finished with value: 10.058362133800983 and parameters: {'learning_rate': 3.89314936984574e-05, 'batch_size': 128, 'gamma': 0.9008912128462754, 'net_arch': [128]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 21:08:02,076] Trial 7 finished with value: 14.226767831593753 and parameters: {'learning_rate': 0.00017455558544313954, 'batch_size': 64, 'gamma': 0.8817208586337737, 'net_arch': [128, 128, 128]}. Best is trial 0 with value: 20.104343913197518.\n",
      "[I 2024-04-20 21:14:01,055] Trial 8 finished with value: 21.801490084007384 and parameters: {'learning_rate': 0.0007551062542275508, 'batch_size': 64, 'gamma': 0.6123696099516621, 'net_arch': [256, 256]}. Best is trial 8 with value: 21.801490084007384.\n",
      "[I 2024-04-20 21:19:58,123] Trial 9 finished with value: 17.179446616619824 and parameters: {'learning_rate': 0.00043620563307071467, 'batch_size': 64, 'gamma': 0.6222554180465528, 'net_arch': [256, 256]}. Best is trial 8 with value: 21.801490084007384.\n",
      "[I 2024-04-20 21:26:03,111] Trial 10 finished with value: 21.9598880429618 and parameters: {'learning_rate': 0.0009781407348947153, 'batch_size': 128, 'gamma': 0.5110216087919963, 'net_arch': [256, 256]}. Best is trial 10 with value: 21.9598880429618.\n",
      "[I 2024-04-20 21:32:10,181] Trial 11 finished with value: 22.9408059386909 and parameters: {'learning_rate': 0.0009203206275040492, 'batch_size': 128, 'gamma': 0.5020368282453737, 'net_arch': [256, 256]}. Best is trial 11 with value: 22.9408059386909.\n",
      "[I 2024-04-20 21:38:16,039] Trial 12 finished with value: 19.60404224127531 and parameters: {'learning_rate': 0.0009554095172376008, 'batch_size': 128, 'gamma': 0.5174795382242334, 'net_arch': [256, 256]}. Best is trial 11 with value: 22.9408059386909.\n",
      "[I 2024-04-20 21:44:24,408] Trial 13 finished with value: 20.564380304589868 and parameters: {'learning_rate': 0.0002579874729103095, 'batch_size': 128, 'gamma': 0.5053137297277718, 'net_arch': [256, 256]}. Best is trial 11 with value: 22.9408059386909.\n",
      "[I 2024-04-20 21:50:27,595] Trial 14 finished with value: 14.006683790236712 and parameters: {'learning_rate': 9.498690037172405e-05, 'batch_size': 128, 'gamma': 0.5626739265129435, 'net_arch': [256, 256]}. Best is trial 11 with value: 22.9408059386909.\n",
      "[I 2024-04-20 21:56:35,398] Trial 15 finished with value: 23.340910299792885 and parameters: {'learning_rate': 0.0009537108481166637, 'batch_size': 128, 'gamma': 0.661526895125826, 'net_arch': [256, 256]}. Best is trial 15 with value: 23.340910299792885.\n",
      "[I 2024-04-20 22:02:39,993] Trial 16 finished with value: 12.873527001962065 and parameters: {'learning_rate': 1.5370688300145226e-05, 'batch_size': 128, 'gamma': 0.6852707035079287, 'net_arch': [256, 256]}. Best is trial 15 with value: 23.340910299792885.\n",
      "[I 2024-04-20 22:08:43,051] Trial 17 finished with value: 17.995509631484747 and parameters: {'learning_rate': 0.00028256936623663497, 'batch_size': 128, 'gamma': 0.7805248086587414, 'net_arch': [256, 256]}. Best is trial 15 with value: 23.340910299792885.\n",
      "[I 2024-04-20 22:14:46,791] Trial 18 finished with value: 18.68106144552119 and parameters: {'learning_rate': 0.00011263252096866826, 'batch_size': 128, 'gamma': 0.8337573727880704, 'net_arch': [256, 256]}. Best is trial 15 with value: 23.340910299792885.\n",
      "[I 2024-04-20 22:20:29,932] Trial 19 finished with value: 16.022095787525178 and parameters: {'learning_rate': 0.000650043549324489, 'batch_size': 128, 'gamma': 0.656577372518144, 'net_arch': [128]}. Best is trial 15 with value: 23.340910299792885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 0.0009537108481166637, 'batch_size': 128, 'gamma': 0.661526895125826, 'net_arch': [256, 256]}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "def optimize_agent(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    gamma = trial.suggest_uniform('gamma', 0.5, 0.99)\n",
    "    net_arch = trial.suggest_categorical('net_arch', [[128], [256, 256], [128, 128, 128]])\n",
    "\n",
    "    # Create the model with these hyperparameters\n",
    "    model = DQN('MlpPolicy', env,\n",
    "                policy_kwargs=dict(net_arch=net_arch),\n",
    "                learning_rate=learning_rate,\n",
    "                buffer_size=15000,\n",
    "                learning_starts=200,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                train_freq=1,\n",
    "                gradient_steps=1,\n",
    "                target_update_interval=50,\n",
    "                verbose=0,\n",
    "                tensorboard_log=\"./logs/highway_dqn_optuna\")\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(int(2e4))\n",
    "    \n",
    "    # Evaluate the model, here using mean reward\n",
    "    mean_reward, _ = evaluate_policy(model.policy, env, n_eval_episodes=50)\n",
    "    \n",
    "    # Clear memory\n",
    "    model.save(f\"models/highway_dqn_optuna/optuna_dqn_{trial.number}\")\n",
    "    model.env.close()\n",
    "    del model\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=20)\n",
    "\n",
    "print('Best hyperparameters: ', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning efforts yielded a best set of hyperparameters with a learning rate of approximately 0.000953, batch size of 128, a gamma of 0.6615, and a network architecture of two 256-unit layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model - Re-train after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to highway_dqn_fine_tuned_baseline/tensorboard_logs/DQN_3\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 62       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.4     |\n",
      "|    ep_rew_mean      | 8.46     |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 91       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.31     |\n",
      "|    exploration_rate | 0.936    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 135      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | 8.66     |\n",
      "|    exploration_rate | 0.912    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 186      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | 8.29     |\n",
      "|    exploration_rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 222      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.015    |\n",
      "|    n_updates        | 21       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | 8.77     |\n",
      "|    exploration_rate | 0.866    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 64       |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 282      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0729   |\n",
      "|    n_updates        | 81       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | 8.46     |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 316      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0803   |\n",
      "|    n_updates        | 115      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | 8.94     |\n",
      "|    exploration_rate | 0.821    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 377      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0972   |\n",
      "|    n_updates        | 176      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.5     |\n",
      "|    ep_rew_mean      | 8.7      |\n",
      "|    exploration_rate | 0.803    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 414      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 213      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | 8.86     |\n",
      "|    exploration_rate | 0.779    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 466      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0875   |\n",
      "|    n_updates        | 265      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.5     |\n",
      "|    ep_rew_mean      | 8.76     |\n",
      "|    exploration_rate | 0.76     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 505      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 304      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.6      |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 538      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 337      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.28     |\n",
      "|    exploration_rate | 0.733    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 562      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.076    |\n",
      "|    n_updates        | 361      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.27     |\n",
      "|    exploration_rate | 0.714    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 602      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 401      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 8.08     |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 629      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.146    |\n",
      "|    n_updates        | 428      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.37     |\n",
      "|    exploration_rate | 0.671    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 692      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.071    |\n",
      "|    n_updates        | 491      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.32     |\n",
      "|    exploration_rate | 0.652    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 732      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 531      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 8.14     |\n",
      "|    exploration_rate | 0.64     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 758      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0787   |\n",
      "|    n_updates        | 557      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 8.27     |\n",
      "|    exploration_rate | 0.614    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 812      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 611      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 8.1      |\n",
      "|    exploration_rate | 0.601    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 839      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 638      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 8.08     |\n",
      "|    exploration_rate | 0.582    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 879      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 678      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.35     |\n",
      "|    exploration_rate | 0.548    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 951      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.141    |\n",
      "|    n_updates        | 750      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 8.4      |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 1001     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.4      |\n",
      "|    exploration_rate | 0.506    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 1041     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 840      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | 8.56     |\n",
      "|    exploration_rate | 0.473    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 1109     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0531   |\n",
      "|    n_updates        | 908      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 8.41     |\n",
      "|    exploration_rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 1154     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.146    |\n",
      "|    n_updates        | 953      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.66     |\n",
      "|    exploration_rate | 0.426    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 1209     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 1008     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.75     |\n",
      "|    exploration_rate | 0.402    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 1259     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 1058     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | 8.62     |\n",
      "|    exploration_rate | 0.386    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 1292     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.121    |\n",
      "|    n_updates        | 1091     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.76     |\n",
      "|    exploration_rate | 0.364    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 1339     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0849   |\n",
      "|    n_updates        | 1138     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | 8.68     |\n",
      "|    exploration_rate | 0.341    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 1387     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 1186     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.83     |\n",
      "|    exploration_rate | 0.316    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 1439     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 1238     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.78     |\n",
      "|    exploration_rate | 0.29     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 1495     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 1294     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | 8.92     |\n",
      "|    exploration_rate | 0.265    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 1547     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 1346     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | 8.94     |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 1601     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.117    |\n",
      "|    n_updates        | 1400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.5     |\n",
      "|    ep_rew_mean      | 9.04     |\n",
      "|    exploration_rate | 0.216    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 1651     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.136    |\n",
      "|    n_updates        | 1450     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | 9.25     |\n",
      "|    exploration_rate | 0.186    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 1713     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0996   |\n",
      "|    n_updates        | 1512     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | 9.41     |\n",
      "|    exploration_rate | 0.167    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 1753     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0976   |\n",
      "|    n_updates        | 1552     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12       |\n",
      "|    ep_rew_mean      | 9.47     |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 1801     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0667   |\n",
      "|    n_updates        | 1600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | 9.59     |\n",
      "|    exploration_rate | 0.124    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 1844     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0979   |\n",
      "|    n_updates        | 1643     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.4     |\n",
      "|    ep_rew_mean      | 9.76     |\n",
      "|    exploration_rate | 0.0828   |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 1931     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 1730     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | 10.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 2024     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0929   |\n",
      "|    n_updates        | 1823     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 2074     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0713   |\n",
      "|    n_updates        | 1873     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 2116     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 1915     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | 10.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 2171     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0542   |\n",
      "|    n_updates        | 1970     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.5     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 2231     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0739   |\n",
      "|    n_updates        | 2030     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 2321     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0871   |\n",
      "|    n_updates        | 2120     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14       |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 2404     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0484   |\n",
      "|    n_updates        | 2203     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 2473     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 2272     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 11.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 2548     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0579   |\n",
      "|    n_updates        | 2347     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 12       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 2646     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0634   |\n",
      "|    n_updates        | 2445     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 2759     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0436   |\n",
      "|    n_updates        | 2558     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 12.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 2845     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0789   |\n",
      "|    n_updates        | 2644     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.2     |\n",
      "|    ep_rew_mean      | 13.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 2908     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0675   |\n",
      "|    n_updates        | 2707     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.4     |\n",
      "|    ep_rew_mean      | 13.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 2975     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0866   |\n",
      "|    n_updates        | 2774     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | 13.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 3042     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0524   |\n",
      "|    n_updates        | 2841     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | 13.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 3101     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0632   |\n",
      "|    n_updates        | 2900     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.7     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 3163     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 2962     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17       |\n",
      "|    ep_rew_mean      | 13.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 3246     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0983   |\n",
      "|    n_updates        | 3045     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.1     |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 3310     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.089    |\n",
      "|    n_updates        | 3109     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 3374     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0372   |\n",
      "|    n_updates        | 3173     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.1     |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 3421     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0425   |\n",
      "|    n_updates        | 3220     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 3493     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0438   |\n",
      "|    n_updates        | 3292     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.3     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 3533     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 3332     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 3560     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0479   |\n",
      "|    n_updates        | 3359     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 3622     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0701   |\n",
      "|    n_updates        | 3421     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.4     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 3664     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0862   |\n",
      "|    n_updates        | 3463     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | 13.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 3702     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0866   |\n",
      "|    n_updates        | 3501     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | 13.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 3791     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0533   |\n",
      "|    n_updates        | 3590     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 3861     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0669   |\n",
      "|    n_updates        | 3660     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.1     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 3937     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0629   |\n",
      "|    n_updates        | 3736     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 3971     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0559   |\n",
      "|    n_updates        | 3770     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.4     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 4048     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0557   |\n",
      "|    n_updates        | 3847     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.4     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 4109     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0741   |\n",
      "|    n_updates        | 3908     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.4     |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 4189     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0536   |\n",
      "|    n_updates        | 3988     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.2     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 4266     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 4065     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 4386     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0679   |\n",
      "|    n_updates        | 4185     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 13.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 4430     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0733   |\n",
      "|    n_updates        | 4229     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 13.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 4466     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0653   |\n",
      "|    n_updates        | 4265     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 13.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 4536     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 4335     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 12.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 4581     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0447   |\n",
      "|    n_updates        | 4380     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 4676     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0573   |\n",
      "|    n_updates        | 4475     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | 13.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 4760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0512   |\n",
      "|    n_updates        | 4559     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 4790     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0536   |\n",
      "|    n_updates        | 4589     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 13.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 4864     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0461   |\n",
      "|    n_updates        | 4663     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 13.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 4923     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0472   |\n",
      "|    n_updates        | 4722     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 13.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 5004     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 4803     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 13.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 5078     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0535   |\n",
      "|    n_updates        | 4877     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.1     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 5141     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0625   |\n",
      "|    n_updates        | 4940     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 5191     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0588   |\n",
      "|    n_updates        | 4990     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 5297     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0712   |\n",
      "|    n_updates        | 5096     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 5345     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.066    |\n",
      "|    n_updates        | 5144     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 5458     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0955   |\n",
      "|    n_updates        | 5257     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 5553     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0827   |\n",
      "|    n_updates        | 5352     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 5638     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0349   |\n",
      "|    n_updates        | 5437     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 5709     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0605   |\n",
      "|    n_updates        | 5508     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 5820     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0311   |\n",
      "|    n_updates        | 5619     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 5890     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0667   |\n",
      "|    n_updates        | 5689     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 5959     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0404   |\n",
      "|    n_updates        | 5758     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 6041     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0686   |\n",
      "|    n_updates        | 5840     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 6128     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0411   |\n",
      "|    n_updates        | 5927     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 6184     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0491   |\n",
      "|    n_updates        | 5983     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 6244     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0465   |\n",
      "|    n_updates        | 6043     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.3     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 6294     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0407   |\n",
      "|    n_updates        | 6093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 6344     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0278   |\n",
      "|    n_updates        | 6143     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 6425     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0491   |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 6483     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0492   |\n",
      "|    n_updates        | 6282     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 6573     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0909   |\n",
      "|    n_updates        | 6372     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 6630     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0759   |\n",
      "|    n_updates        | 6429     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 6679     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0548   |\n",
      "|    n_updates        | 6478     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 6730     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0697   |\n",
      "|    n_updates        | 6529     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 6814     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.067    |\n",
      "|    n_updates        | 6613     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.3     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 6912     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0356   |\n",
      "|    n_updates        | 6711     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 6962     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0453   |\n",
      "|    n_updates        | 6761     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 7016     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0264   |\n",
      "|    n_updates        | 6815     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 7098     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0367   |\n",
      "|    n_updates        | 6897     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 7156     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0643   |\n",
      "|    n_updates        | 6955     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 7247     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0314   |\n",
      "|    n_updates        | 7046     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 7325     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0787   |\n",
      "|    n_updates        | 7124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 7396     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0461   |\n",
      "|    n_updates        | 7195     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 7465     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0205   |\n",
      "|    n_updates        | 7264     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 7556     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0624   |\n",
      "|    n_updates        | 7355     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 7609     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0597   |\n",
      "|    n_updates        | 7408     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 7700     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0382   |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 7802     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0747   |\n",
      "|    n_updates        | 7601     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 7869     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.033    |\n",
      "|    n_updates        | 7668     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 7971     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0705   |\n",
      "|    n_updates        | 7770     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 8064     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0437   |\n",
      "|    n_updates        | 7863     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 8143     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0457   |\n",
      "|    n_updates        | 7942     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total_timesteps  | 8195     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0358   |\n",
      "|    n_updates        | 7994     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 8297     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0707   |\n",
      "|    n_updates        | 8096     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 8391     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0542   |\n",
      "|    n_updates        | 8190     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 8457     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0522   |\n",
      "|    n_updates        | 8256     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total_timesteps  | 8513     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0491   |\n",
      "|    n_updates        | 8312     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | 16.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 8576     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0432   |\n",
      "|    n_updates        | 8375     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 8636     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0438   |\n",
      "|    n_updates        | 8435     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | 16.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 8710     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.056    |\n",
      "|    n_updates        | 8509     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | 16.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total_timesteps  | 8808     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0584   |\n",
      "|    n_updates        | 8607     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | 16.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total_timesteps  | 8891     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0795   |\n",
      "|    n_updates        | 8690     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 16.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 8941     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0457   |\n",
      "|    n_updates        | 8740     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 9040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0451   |\n",
      "|    n_updates        | 8839     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 17       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total_timesteps  | 9140     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.061    |\n",
      "|    n_updates        | 8939     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 9240     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0507   |\n",
      "|    n_updates        | 9039     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 9323     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0473   |\n",
      "|    n_updates        | 9122     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 9393     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0293   |\n",
      "|    n_updates        | 9192     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 9489     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0372   |\n",
      "|    n_updates        | 9288     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total_timesteps  | 9542     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0715   |\n",
      "|    n_updates        | 9341     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 9610     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0395   |\n",
      "|    n_updates        | 9409     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 162      |\n",
      "|    total_timesteps  | 9689     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0567   |\n",
      "|    n_updates        | 9488     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 163      |\n",
      "|    total_timesteps  | 9759     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0829   |\n",
      "|    n_updates        | 9558     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total_timesteps  | 9849     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0417   |\n",
      "|    n_updates        | 9648     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 166      |\n",
      "|    total_timesteps  | 9927     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0247   |\n",
      "|    n_updates        | 9726     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 10003    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0213   |\n",
      "|    n_updates        | 9802     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 10055    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.041    |\n",
      "|    n_updates        | 9854     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total_timesteps  | 10142    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0289   |\n",
      "|    n_updates        | 9941     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 16.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 171      |\n",
      "|    total_timesteps  | 10222    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0439   |\n",
      "|    n_updates        | 10021    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 10329    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0327   |\n",
      "|    n_updates        | 10128    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | 16.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 174      |\n",
      "|    total_timesteps  | 10384    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0371   |\n",
      "|    n_updates        | 10183    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 176      |\n",
      "|    total_timesteps  | 10480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0385   |\n",
      "|    n_updates        | 10279    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 17.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 177      |\n",
      "|    total_timesteps  | 10560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0485   |\n",
      "|    n_updates        | 10359    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 17.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 178      |\n",
      "|    total_timesteps  | 10641    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0294   |\n",
      "|    n_updates        | 10440    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total_timesteps  | 10761    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 10560    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total_timesteps  | 10836    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0384   |\n",
      "|    n_updates        | 10635    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 183      |\n",
      "|    total_timesteps  | 10902    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0502   |\n",
      "|    n_updates        | 10701    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 184      |\n",
      "|    total_timesteps  | 10984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0425   |\n",
      "|    n_updates        | 10783    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 186      |\n",
      "|    total_timesteps  | 11086    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0246   |\n",
      "|    n_updates        | 10885    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total_timesteps  | 11167    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0178   |\n",
      "|    n_updates        | 10966    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 189      |\n",
      "|    total_timesteps  | 11287    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0662   |\n",
      "|    n_updates        | 11086    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 191      |\n",
      "|    total_timesteps  | 11367    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.04     |\n",
      "|    n_updates        | 11166    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 192      |\n",
      "|    total_timesteps  | 11458    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0591   |\n",
      "|    n_updates        | 11257    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total_timesteps  | 11557    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0342   |\n",
      "|    n_updates        | 11356    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 195      |\n",
      "|    total_timesteps  | 11638    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0306   |\n",
      "|    n_updates        | 11437    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 197      |\n",
      "|    total_timesteps  | 11728    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0283   |\n",
      "|    n_updates        | 11527    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 198      |\n",
      "|    total_timesteps  | 11827    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.037    |\n",
      "|    n_updates        | 11626    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 200      |\n",
      "|    total_timesteps  | 11895    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0472   |\n",
      "|    n_updates        | 11694    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 11994    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0305   |\n",
      "|    n_updates        | 11793    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 203      |\n",
      "|    total_timesteps  | 12072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0293   |\n",
      "|    n_updates        | 11871    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 203      |\n",
      "|    total_timesteps  | 12121    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0445   |\n",
      "|    n_updates        | 11920    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 205      |\n",
      "|    total_timesteps  | 12241    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0364   |\n",
      "|    n_updates        | 12040    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 207      |\n",
      "|    total_timesteps  | 12324    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0663   |\n",
      "|    n_updates        | 12123    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 208      |\n",
      "|    total_timesteps  | 12385    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0442   |\n",
      "|    n_updates        | 12184    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 12494    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0304   |\n",
      "|    n_updates        | 12293    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 211      |\n",
      "|    total_timesteps  | 12572    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0633   |\n",
      "|    n_updates        | 12371    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 213      |\n",
      "|    total_timesteps  | 12668    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0323   |\n",
      "|    n_updates        | 12467    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 214      |\n",
      "|    total_timesteps  | 12761    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.062    |\n",
      "|    n_updates        | 12560    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 12863    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0409   |\n",
      "|    n_updates        | 12662    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 217      |\n",
      "|    total_timesteps  | 12928    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0267   |\n",
      "|    n_updates        | 12727    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 218      |\n",
      "|    total_timesteps  | 13001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0509   |\n",
      "|    n_updates        | 12800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 13052    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0299   |\n",
      "|    n_updates        | 12851    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 220      |\n",
      "|    total_timesteps  | 13125    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0313   |\n",
      "|    n_updates        | 12924    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 13221    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0457   |\n",
      "|    n_updates        | 13020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 223      |\n",
      "|    total_timesteps  | 13285    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0213   |\n",
      "|    n_updates        | 13084    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 224      |\n",
      "|    total_timesteps  | 13348    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0328   |\n",
      "|    n_updates        | 13147    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total_timesteps  | 13418    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 13217    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total_timesteps  | 13489    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0458   |\n",
      "|    n_updates        | 13288    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 227      |\n",
      "|    total_timesteps  | 13542    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0411   |\n",
      "|    n_updates        | 13341    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 17.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total_timesteps  | 13595    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0494   |\n",
      "|    n_updates        | 13394    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total_timesteps  | 13689    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0216   |\n",
      "|    n_updates        | 13488    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 231      |\n",
      "|    total_timesteps  | 13763    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0373   |\n",
      "|    n_updates        | 13562    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 13827    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0415   |\n",
      "|    n_updates        | 13626    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 17       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 233      |\n",
      "|    total_timesteps  | 13904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0346   |\n",
      "|    n_updates        | 13703    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 235      |\n",
      "|    total_timesteps  | 13980    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0423   |\n",
      "|    n_updates        | 13779    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 237      |\n",
      "|    total_timesteps  | 14078    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0612   |\n",
      "|    n_updates        | 13877    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total_timesteps  | 14177    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0437   |\n",
      "|    n_updates        | 13976    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total_timesteps  | 14297    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0223   |\n",
      "|    n_updates        | 14096    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 242      |\n",
      "|    total_timesteps  | 14398    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0382   |\n",
      "|    n_updates        | 14197    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 14498    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0336   |\n",
      "|    n_updates        | 14297    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total_timesteps  | 14576    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0266   |\n",
      "|    n_updates        | 14375    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 17.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 247      |\n",
      "|    total_timesteps  | 14659    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0332   |\n",
      "|    n_updates        | 14458    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 247      |\n",
      "|    total_timesteps  | 14705    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0424   |\n",
      "|    n_updates        | 14504    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 249      |\n",
      "|    total_timesteps  | 14804    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0314   |\n",
      "|    n_updates        | 14603    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 17.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 251      |\n",
      "|    total_timesteps  | 14895    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0462   |\n",
      "|    n_updates        | 14694    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 17.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total_timesteps  | 14970    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0316   |\n",
      "|    n_updates        | 14769    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 18.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 15078    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0318   |\n",
      "|    n_updates        | 14877    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 15102    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0342   |\n",
      "|    n_updates        | 14901    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | 17.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 256      |\n",
      "|    total_timesteps  | 15198    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0263   |\n",
      "|    n_updates        | 14997    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 17.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 257      |\n",
      "|    total_timesteps  | 15278    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0651   |\n",
      "|    n_updates        | 15077    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 259      |\n",
      "|    total_timesteps  | 15355    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0463   |\n",
      "|    n_updates        | 15154    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 260      |\n",
      "|    total_timesteps  | 15434    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0446   |\n",
      "|    n_updates        | 15233    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 15518    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0267   |\n",
      "|    n_updates        | 15317    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 263      |\n",
      "|    total_timesteps  | 15594    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 15393    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 265      |\n",
      "|    total_timesteps  | 15711    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0443   |\n",
      "|    n_updates        | 15510    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 266      |\n",
      "|    total_timesteps  | 15784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0288   |\n",
      "|    n_updates        | 15583    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 267      |\n",
      "|    total_timesteps  | 15875    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.063    |\n",
      "|    n_updates        | 15674    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 269      |\n",
      "|    total_timesteps  | 15945    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.051    |\n",
      "|    n_updates        | 15744    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 270      |\n",
      "|    total_timesteps  | 16036    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0339   |\n",
      "|    n_updates        | 15835    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 271      |\n",
      "|    total_timesteps  | 16110    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 15909    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 16196    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0228   |\n",
      "|    n_updates        | 15995    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 274      |\n",
      "|    total_timesteps  | 16293    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0356   |\n",
      "|    n_updates        | 16092    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 276      |\n",
      "|    total_timesteps  | 16373    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0135   |\n",
      "|    n_updates        | 16172    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 277      |\n",
      "|    total_timesteps  | 16475    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0287   |\n",
      "|    n_updates        | 16274    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 279      |\n",
      "|    total_timesteps  | 16567    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0275   |\n",
      "|    n_updates        | 16366    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 281      |\n",
      "|    total_timesteps  | 16666    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0291   |\n",
      "|    n_updates        | 16465    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 282      |\n",
      "|    total_timesteps  | 16749    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0438   |\n",
      "|    n_updates        | 16548    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 283      |\n",
      "|    total_timesteps  | 16839    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0185   |\n",
      "|    n_updates        | 16638    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 285      |\n",
      "|    total_timesteps  | 16940    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0192   |\n",
      "|    n_updates        | 16739    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 286      |\n",
      "|    total_timesteps  | 17015    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0332   |\n",
      "|    n_updates        | 16814    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 287      |\n",
      "|    total_timesteps  | 17068    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0564   |\n",
      "|    n_updates        | 16867    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 289      |\n",
      "|    total_timesteps  | 17145    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0183   |\n",
      "|    n_updates        | 16944    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 17239    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.015    |\n",
      "|    n_updates        | 17038    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 292      |\n",
      "|    total_timesteps  | 17330    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0201   |\n",
      "|    n_updates        | 17129    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 18.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total_timesteps  | 17403    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0516   |\n",
      "|    n_updates        | 17202    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 295      |\n",
      "|    total_timesteps  | 17511    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0156   |\n",
      "|    n_updates        | 17310    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 296      |\n",
      "|    total_timesteps  | 17595    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 17394    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 297      |\n",
      "|    total_timesteps  | 17674    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.037    |\n",
      "|    n_updates        | 17473    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 299      |\n",
      "|    total_timesteps  | 17768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0352   |\n",
      "|    n_updates        | 17567    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 301      |\n",
      "|    total_timesteps  | 17881    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.021    |\n",
      "|    n_updates        | 17680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 19.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 302      |\n",
      "|    total_timesteps  | 17953    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0335   |\n",
      "|    n_updates        | 17752    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 18066    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0294   |\n",
      "|    n_updates        | 17865    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 18157    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0268   |\n",
      "|    n_updates        | 17956    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 19.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1004     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 307      |\n",
      "|    total_timesteps  | 18252    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0266   |\n",
      "|    n_updates        | 18051    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1008     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 309      |\n",
      "|    total_timesteps  | 18367    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0402   |\n",
      "|    n_updates        | 18166    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1012     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 311      |\n",
      "|    total_timesteps  | 18469    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0207   |\n",
      "|    n_updates        | 18268    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1016     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 312      |\n",
      "|    total_timesteps  | 18571    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0343   |\n",
      "|    n_updates        | 18370    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1020     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 314      |\n",
      "|    total_timesteps  | 18671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0172   |\n",
      "|    n_updates        | 18470    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1024     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 315      |\n",
      "|    total_timesteps  | 18751    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.015    |\n",
      "|    n_updates        | 18550    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1028     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 317      |\n",
      "|    total_timesteps  | 18837    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0388   |\n",
      "|    n_updates        | 18636    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1032     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 318      |\n",
      "|    total_timesteps  | 18921    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0452   |\n",
      "|    n_updates        | 18720    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1036     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 319      |\n",
      "|    total_timesteps  | 18993    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 18792    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1040     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 321      |\n",
      "|    total_timesteps  | 19100    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0309   |\n",
      "|    n_updates        | 18899    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1044     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 323      |\n",
      "|    total_timesteps  | 19175    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0194   |\n",
      "|    n_updates        | 18974    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 20.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1048     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 324      |\n",
      "|    total_timesteps  | 19292    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0347   |\n",
      "|    n_updates        | 19091    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1052     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 326      |\n",
      "|    total_timesteps  | 19387    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 19186    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1056     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 328      |\n",
      "|    total_timesteps  | 19507    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 19306    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1060     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 329      |\n",
      "|    total_timesteps  | 19586    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0313   |\n",
      "|    n_updates        | 19385    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1064     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 331      |\n",
      "|    total_timesteps  | 19679    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0394   |\n",
      "|    n_updates        | 19478    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1068     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 332      |\n",
      "|    total_timesteps  | 19760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0362   |\n",
      "|    n_updates        | 19559    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1072     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 334      |\n",
      "|    total_timesteps  | 19880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0261   |\n",
      "|    n_updates        | 19679    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1076     |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 336      |\n",
      "|    total_timesteps  | 19976    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.000953 |\n",
      "|    loss             | 0.0182   |\n",
      "|    n_updates        | 19775    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN('MlpPolicy', env,\n",
    "              policy_kwargs=dict(net_arch=[256, 256]),\n",
    "              learning_rate=0.000953,\n",
    "              buffer_size=15000,\n",
    "              learning_starts=200,\n",
    "              batch_size=128,\n",
    "              gamma=0.6615,\n",
    "              train_freq=1,\n",
    "              gradient_steps=1,\n",
    "              target_update_interval=50,\n",
    "              verbose=1,\n",
    "              tensorboard_log=\"./logs/highway_dqn_fine_tuned_baseline\")\n",
    "model.learn(int(2e4))\n",
    "model.save(\"./models/highway_dqn_fine_tuned_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation - After tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-906c18d60131b383\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-906c18d60131b383\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir ./logs/highway_dqn_fine_tuned_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The achieved results from this fine-tuning process exhibit significant improvements, as indicated by the best trial achieving a mean reward of 23.34, which surpasses the baseline results prior to fine-tuning. The increase in the mean episode length and mean episode reward, along with consistent frames per second, reflects an enhanced learning ability and potentially a more stable and efficient policy.\n",
    "\n",
    "When comparing these fine-tuned results to the previous baseline model, the adjustments in hyperparameters appear to have led to a more proficient agent. The exploration rate's decline suggests that as training progressed, the agent became more confident in its strategy and less reliant on exploration. This balance between exploration and exploitation is crucial for the agent's performance, as evidenced by the improved mean reward and episode length. It is important to note that the increase in batch size and the adjustment in network architecture may have contributed significantly to these improvements by providing more robust and frequent updates during learning, as well as a network capacity more suited to the complexity of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned Baseline model - Test Visual Inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 27.384857012362403, Steps = 30\n",
      "Episode 2: Total Reward = 8.267515452624355, Steps = 10\n",
      "Episode 3: Total Reward = 26.09874733095471, Steps = 30\n",
      "Episode 4: Total Reward = 28.374905191930093, Steps = 30\n",
      "Episode 5: Total Reward = 20.729516090447255, Steps = 22\n",
      "Episode 6: Total Reward = 28.67694621977947, Steps = 30\n",
      "Episode 7: Total Reward = 12.446866391137508, Steps = 14\n",
      "Episode 8: Total Reward = 12.617457649010083, Steps = 14\n",
      "Episode 9: Total Reward = 28.709306895206076, Steps = 30\n",
      "Episode 10: Total Reward = 19.33238913183402, Steps = 22\n",
      "Episode 11: Total Reward = 4.778845409167895, Steps = 6\n",
      "Episode 12: Total Reward = 23.151793926602107, Steps = 30\n",
      "Episode 13: Total Reward = 29.2093451644562, Steps = 30\n",
      "Episode 14: Total Reward = 12.280226505133037, Steps = 14\n",
      "Episode 15: Total Reward = 28.641248929884334, Steps = 30\n",
      "Episode 16: Total Reward = 29.329254127762656, Steps = 30\n",
      "Episode 17: Total Reward = 18.384023910882114, Steps = 21\n",
      "Episode 18: Total Reward = 28.07449489941365, Steps = 30\n",
      "Episode 19: Total Reward = 26.416316958893926, Steps = 30\n",
      "Episode 20: Total Reward = 25.13179560840623, Steps = 28\n",
      "Episode 21: Total Reward = 28.742221834004635, Steps = 30\n",
      "Episode 22: Total Reward = 24.045718614739826, Steps = 30\n",
      "Episode 23: Total Reward = 28.44169751575511, Steps = 30\n",
      "Episode 24: Total Reward = 15.515805304168566, Steps = 17\n",
      "Episode 25: Total Reward = 25.617546756188656, Steps = 30\n",
      "Episode 26: Total Reward = 24.74897830075855, Steps = 30\n",
      "Episode 27: Total Reward = 5.712165367235626, Steps = 7\n",
      "Episode 28: Total Reward = 27.60920853139644, Steps = 30\n",
      "Episode 29: Total Reward = 28.675536288387228, Steps = 30\n",
      "Episode 30: Total Reward = 28.51039254063247, Steps = 30\n",
      "Episode 31: Total Reward = 17.160847629199736, Steps = 19\n",
      "Episode 32: Total Reward = 25.850746725959866, Steps = 30\n",
      "Episode 33: Total Reward = 11.545973384250903, Steps = 13\n",
      "Episode 34: Total Reward = 7.7117200456750705, Steps = 9\n",
      "Episode 35: Total Reward = 29.17536860998309, Steps = 30\n",
      "Episode 36: Total Reward = 28.57517199490573, Steps = 30\n",
      "Episode 37: Total Reward = 28.575354871719295, Steps = 30\n",
      "Episode 38: Total Reward = 1.8801294969714422, Steps = 3\n",
      "Episode 39: Total Reward = 23.186415539275888, Steps = 30\n",
      "Episode 40: Total Reward = 8.44505341006576, Steps = 10\n",
      "Episode 41: Total Reward = 29.009807268355374, Steps = 30\n",
      "Episode 42: Total Reward = 8.611711492949349, Steps = 10\n",
      "Episode 43: Total Reward = 28.776473939791455, Steps = 30\n",
      "Episode 44: Total Reward = 27.51578576827923, Steps = 30\n",
      "Episode 45: Total Reward = 27.583901754513864, Steps = 30\n",
      "Episode 46: Total Reward = 27.878290766310396, Steps = 30\n",
      "Episode 47: Total Reward = 28.450197165538913, Steps = 30\n",
      "Episode 48: Total Reward = 25.995989862961306, Steps = 30\n",
      "Episode 49: Total Reward = 8.862129456250612, Steps = 10\n",
      "Episode 50: Total Reward = 28.276307346834212, Steps = 30\n",
      "Episode 51: Total Reward = 10.012182170125524, Steps = 11\n",
      "Episode 52: Total Reward = 26.416940623167374, Steps = 30\n",
      "Episode 53: Total Reward = 16.298955932850053, Steps = 18\n",
      "Episode 54: Total Reward = 10.150826976799364, Steps = 12\n",
      "Episode 55: Total Reward = 25.618804788102196, Steps = 30\n",
      "Episode 56: Total Reward = 9.929726046679747, Steps = 11\n",
      "Episode 57: Total Reward = 24.277950762125666, Steps = 30\n",
      "Episode 58: Total Reward = 15.277443223358063, Steps = 17\n",
      "Episode 59: Total Reward = 20.719276953477227, Steps = 24\n",
      "Episode 60: Total Reward = 17.22971195594015, Steps = 19\n",
      "Episode 61: Total Reward = 2.9307166415764856, Steps = 4\n",
      "Episode 62: Total Reward = 20.01737507062415, Steps = 23\n",
      "Episode 63: Total Reward = 26.616743162623973, Steps = 30\n",
      "Episode 64: Total Reward = 27.497034286522343, Steps = 30\n",
      "Episode 65: Total Reward = 29.3093353284483, Steps = 30\n",
      "Episode 66: Total Reward = 29.375043479849978, Steps = 30\n",
      "Episode 67: Total Reward = 28.17570903785992, Steps = 30\n",
      "Episode 68: Total Reward = 15.364798460557225, Steps = 18\n",
      "Episode 69: Total Reward = 29.19432958872473, Steps = 30\n",
      "Episode 70: Total Reward = 24.394976459261947, Steps = 26\n",
      "Episode 71: Total Reward = 29.243612817793444, Steps = 30\n",
      "Episode 72: Total Reward = 28.475242107304393, Steps = 30\n",
      "Episode 73: Total Reward = 26.816588408207547, Steps = 30\n",
      "Episode 74: Total Reward = 25.777750312559423, Steps = 30\n",
      "Episode 75: Total Reward = 10.795444283392733, Steps = 12\n",
      "Episode 76: Total Reward = 2.046796163638108, Steps = 3\n",
      "Episode 77: Total Reward = 21.64448847951806, Steps = 26\n",
      "Episode 78: Total Reward = 4.712193690313512, Steps = 6\n",
      "Episode 79: Total Reward = 28.341583056785716, Steps = 30\n",
      "Episode 80: Total Reward = 24.618329269462656, Steps = 30\n",
      "Episode 81: Total Reward = 6.611720090378734, Steps = 8\n",
      "Episode 82: Total Reward = 12.377891733642713, Steps = 14\n",
      "Episode 83: Total Reward = 12.66305938876827, Steps = 14\n",
      "Episode 84: Total Reward = 26.51802827444531, Steps = 30\n",
      "Episode 85: Total Reward = 27.946890662714374, Steps = 30\n",
      "Episode 86: Total Reward = 29.643154836830085, Steps = 30\n",
      "Episode 87: Total Reward = 2.046796163638108, Steps = 3\n",
      "Episode 88: Total Reward = 9.711266529642517, Steps = 11\n",
      "Episode 89: Total Reward = 27.38594360410032, Steps = 30\n",
      "Episode 90: Total Reward = 29.241233508622233, Steps = 30\n",
      "Episode 91: Total Reward = 13.59861615555424, Steps = 15\n",
      "Episode 92: Total Reward = 28.50854264244048, Steps = 30\n",
      "Episode 93: Total Reward = 1.2130006968016176, Steps = 2\n",
      "Episode 94: Total Reward = 28.807625948839338, Steps = 30\n",
      "Episode 95: Total Reward = 15.584615641254072, Steps = 18\n",
      "Episode 96: Total Reward = 26.278200430255556, Steps = 30\n",
      "Episode 97: Total Reward = 26.876357190177785, Steps = 30\n",
      "Episode 98: Total Reward = 20.543281901507257, Steps = 23\n",
      "Episode 99: Total Reward = 19.377876099743062, Steps = 21\n",
      "Episode 100: Total Reward = 5.212640325734789, Steps = 6\n",
      "Moviepy - Building video highway_fine_tuned_baseline_performance.mp4.\n",
      "Moviepy - Writing video highway_fine_tuned_baseline_performance.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready highway_fine_tuned_baseline_performance.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"highway_fine_tuned_baseline_performance.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "model = DQN.load(\"./models/highway_dqn_fine_tuned_baseline\")\n",
    "highway_dqn_fine_tuned_baseline_frames = evaluate_model(env, model)\n",
    "\n",
    "# Specify the frame rate (frames per second)\n",
    "fps = 20\n",
    "\n",
    "# Create a video clip from the frames\n",
    "clip = ImageSequenceClip(highway_dqn_baseline_frames, fps=fps)\n",
    "clip.write_videofile('./videos/highway_fine_tuned_baseline_performance.mp4', codec='libx264')\n",
    "\n",
    "Video(\"./videos/highway_fine_tuned_baseline_performance.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model - Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "model = DQN.load(\"./models/highway_dqn_fine_tuned_baseline\")\n",
    "highway_dqn_fine_tuned_baseline_frames = evaluate_model(env, model)\n",
    "\n",
    "# Specify the frame rate (frames per second)\n",
    "fps = 20\n",
    "\n",
    "# Create a video clip from the frames\n",
    "clip = ImageSequenceClip(highway_dqn_baseline_frames, fps=fps)\n",
    "clip.write_videofile('./videos/highway_fine_tuned_baseline_performance.mp4', codec='libx264')\n",
    "\n",
    "Video(\"./videos/highway_fine_tuned_baseline_performance.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the DQN Model\n",
    "\n",
    "Now that a baseline has been set, let's implement a custom version of the DQN algorithm from scratch using TensorFlow/Keras. A neural network model for approximating Q-values is built, as well as the necessary components of the DQN algorithm like the experience replay buffer and the target network update mechanism.\n",
    "\n",
    "The implementation is outlined as following:\n",
    "\n",
    "- Define the Q-Network using Keras.\n",
    "- Create the experience replay buffer.\n",
    "- Implement the epsilon-greedy policy for action selection.\n",
    "- Define the training procedure, which includes sampling from the buffer and updating the Q-Network.\n",
    "- Periodically update the target network weights.\n",
    "\n",
    "Below is a code snippet with a custom DQN using TensorFlow/Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.keras.utils.disable_interactive_logging()\n",
    "\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dense1 = layers.Dense(256, activation='relu', input_shape=(input_shape,))\n",
    "        self.dense2 = layers.Dense(256, activation='relu')\n",
    "        self.dense3 = layers.Dense(action_space, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "# Define the Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state = np.reshape(state, -1)  # Flatten the state\n",
    "        next_state = np.reshape(next_state, -1)  # Flatten the next_state\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "# Define the DQNAgent\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, action_space, buffer_size, batch_size, gamma, learning_rate):\n",
    "        print(f\"Initializing DQN with action space: {action_space}\")\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(input_shape, action_space)\n",
    "        self.target_model = DQN(input_shape, action_space)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randint(0, self.action_space - 1)\n",
    "        state = np.reshape(state, (1, -1))  # Reshape for consistent input\n",
    "        q_values = self.model(state) \n",
    "        return np.argmax(q_values[0])  # Consider indexing to handle the batch\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "        # Reshape states and next_states to remove any unwanted dimensions\n",
    "        states = np.reshape(states, (self.batch_size, -1))\n",
    "        next_states = np.reshape(next_states, (self.batch_size, -1))\n",
    "\n",
    "        current_q = self.model.predict(states)\n",
    "        next_q = self.target_model.predict(next_states)\n",
    "        target_q = np.copy(current_q)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            action = actions[i]\n",
    "            if dones[i]:\n",
    "                target_q[i, action] = rewards[i]\n",
    "            else:\n",
    "                target_q[i, action] = rewards[i] + self.gamma * np.max(next_q[i])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states)\n",
    "            loss = tf.keras.losses.MeanSquaredError()(target_q, q_values)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss.numpy()\n",
    "    \n",
    "    def train(self, env, episodes):\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir = './logs/custom_dqn/' + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state, info = env.reset()  # Capture both parts of the tuple\n",
    "            state = np.reshape(state, [1, -1])  # Flatten the state correctly\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            losses = []  # Track losses for each episode\n",
    "\n",
    "            while not done:\n",
    "                action = self.act(state.flatten())\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                done = done or truncated\n",
    "                next_state = np.reshape(next_state, [1, -1])\n",
    "                self.memory.add(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                loss = self.replay()\n",
    "                if loss is not None:\n",
    "                    losses.append(loss)\n",
    "\n",
    "                # Log averaged loss if there were any losses recorded\n",
    "                average_loss = np.mean(losses) if losses else 0\n",
    "                \n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar('Average Loss', average_loss, step=episode)\n",
    "                    tf.summary.scalar('Total Reward', total_reward, step=episode)\n",
    "                    tf.summary.scalar('Epsilon', self.epsilon, step=episode)\n",
    "                summary_writer.flush()\n",
    "                \n",
    "                if done:\n",
    "                    self.update_epsilon()\n",
    "                    print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {self.epsilon}\")\n",
    "                \n",
    "            # Update target network\n",
    "            if episode % 10 == 0:\n",
    "                self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the custom DQN Model\n",
    "\n",
    "Before we proceed, initialize your model by specifying the input shape and the number of actions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DQN with action space: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leone/msc/reinforcement-learning/rl_graded_assessment_2/rl-project/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize DQN Agent\n",
    "env.reset()\n",
    "# Initialize and run the agent\n",
    "action_space = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "input_shape = np.prod(state_shape)  # Flatten the state dimensions for input to the network\n",
    "\n",
    "agent = DQNAgent(input_shape, action_space, buffer_size=15000, batch_size=128, gamma=0.6615, learning_rate=0.000953)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the custom DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 3.278865637264499, Epsilon: 0.9322301194154049\n",
      "Episode: 1, Total Reward: 5.6149523440357765, Epsilon: 0.9275689688183278\n",
      "Episode: 2, Total Reward: 6.832563203692795, Epsilon: 0.9229311239742362\n",
      "Episode: 3, Total Reward: 2.046796163638108, Epsilon: 0.918316468354365\n",
      "Episode: 4, Total Reward: 2.9455725130846884, Epsilon: 0.9137248860125932\n",
      "Episode: 5, Total Reward: 1.8467961636381087, Epsilon: 0.9091562615825302\n",
      "Episode: 6, Total Reward: 2.1530974449830915, Epsilon: 0.9046104802746175\n",
      "Episode: 7, Total Reward: 9.617252375744068, Epsilon: 0.9000874278732445\n",
      "Episode: 8, Total Reward: 2.913158342536706, Epsilon: 0.8955869907338783\n",
      "Episode: 9, Total Reward: 5.412033284376648, Epsilon: 0.8911090557802088\n",
      "Episode: 10, Total Reward: 8.278378401841094, Epsilon: 0.8866535105013078\n",
      "Episode: 11, Total Reward: 4.046451578715396, Epsilon: 0.8822202429488013\n",
      "Episode: 12, Total Reward: 13.629110400315602, Epsilon: 0.8778091417340573\n",
      "Episode: 13, Total Reward: 2.1973833082431526, Epsilon: 0.8734200960253871\n",
      "Episode: 14, Total Reward: 2.6151527517185955, Epsilon: 0.8690529955452602\n",
      "Episode: 15, Total Reward: 23.40177075708481, Epsilon: 0.8647077305675338\n",
      "Episode: 16, Total Reward: 9.433058072125824, Epsilon: 0.8603841919146962\n",
      "Episode: 17, Total Reward: 4.031061226499198, Epsilon: 0.8560822709551227\n",
      "Episode: 18, Total Reward: 2.879825009203372, Epsilon: 0.851801859600347\n",
      "Episode: 19, Total Reward: 10.945157490524036, Epsilon: 0.8475428503023453\n",
      "Episode: 20, Total Reward: 1.7666666666666668, Epsilon: 0.8433051360508336\n",
      "Episode: 21, Total Reward: 2.084982129562962, Epsilon: 0.8390886103705794\n",
      "Episode: 22, Total Reward: 2.7640499749098195, Epsilon: 0.8348931673187264\n",
      "Episode: 23, Total Reward: 2.9126961034751258, Epsilon: 0.8307187014821328\n",
      "Episode: 24, Total Reward: 5.479506773096324, Epsilon: 0.8265651079747222\n",
      "Episode: 25, Total Reward: 7.233683997512353, Epsilon: 0.8224322824348486\n",
      "Episode: 26, Total Reward: 2.846020961436447, Epsilon: 0.8183201210226743\n",
      "Episode: 27, Total Reward: 15.429877827116377, Epsilon: 0.8142285204175609\n",
      "Episode: 28, Total Reward: 11.732240114237143, Epsilon: 0.810157377815473\n",
      "Episode: 29, Total Reward: 5.745828855299539, Epsilon: 0.8061065909263957\n",
      "Episode: 30, Total Reward: 9.032101081041032, Epsilon: 0.8020760579717637\n",
      "Episode: 31, Total Reward: 4.012198970597832, Epsilon: 0.798065677681905\n",
      "Episode: 32, Total Reward: 1.7654653579803719, Epsilon: 0.7940753492934954\n",
      "Episode: 33, Total Reward: 8.145361695807921, Epsilon: 0.7901049725470279\n",
      "Episode: 34, Total Reward: 10.986582652352206, Epsilon: 0.7861544476842928\n",
      "Episode: 35, Total Reward: 14.31480887578051, Epsilon: 0.7822236754458713\n",
      "Episode: 36, Total Reward: 3.5812396645101985, Epsilon: 0.778312557068642\n",
      "Episode: 37, Total Reward: 4.645290052820694, Epsilon: 0.7744209942832988\n",
      "Episode: 38, Total Reward: 3.913462830304775, Epsilon: 0.7705488893118823\n",
      "Episode: 39, Total Reward: 6.112540133195731, Epsilon: 0.7666961448653229\n",
      "Episode: 40, Total Reward: 9.046185661619388, Epsilon: 0.7628626641409962\n",
      "Episode: 41, Total Reward: 20.708624581669756, Epsilon: 0.7590483508202912\n",
      "Episode: 42, Total Reward: 9.34848560355072, Epsilon: 0.7552531090661897\n",
      "Episode: 43, Total Reward: 4.446020961436447, Epsilon: 0.7514768435208588\n",
      "Episode: 44, Total Reward: 10.779053358030069, Epsilon: 0.7477194593032545\n",
      "Episode: 45, Total Reward: 12.64452999399523, Epsilon: 0.7439808620067382\n",
      "Episode: 46, Total Reward: 2.279362770141792, Epsilon: 0.7402609576967045\n",
      "Episode: 47, Total Reward: 12.677737654120348, Epsilon: 0.736559652908221\n",
      "Episode: 48, Total Reward: 5.631021220912754, Epsilon: 0.7328768546436799\n",
      "Episode: 49, Total Reward: 7.378684214621858, Epsilon: 0.7292124703704616\n",
      "Episode: 50, Total Reward: 12.513503706750395, Epsilon: 0.7255664080186093\n",
      "Episode: 51, Total Reward: 12.112216433371264, Epsilon: 0.7219385759785162\n",
      "Episode: 52, Total Reward: 8.577902059156814, Epsilon: 0.7183288830986236\n",
      "Episode: 53, Total Reward: 8.510967812850996, Epsilon: 0.7147372386831305\n",
      "Episode: 54, Total Reward: 3.8979629207312296, Epsilon: 0.7111635524897149\n",
      "Episode: 55, Total Reward: 13.667735704039188, Epsilon: 0.7076077347272662\n",
      "Episode: 56, Total Reward: 7.819140585544187, Epsilon: 0.7040696960536299\n",
      "Episode: 57, Total Reward: 1.848486085051929, Epsilon: 0.7005493475733617\n",
      "Episode: 58, Total Reward: 6.851257916956048, Epsilon: 0.697046600835495\n",
      "Episode: 59, Total Reward: 4.364055255193558, Epsilon: 0.6935613678313175\n",
      "Episode: 60, Total Reward: 10.385228020405878, Epsilon: 0.6900935609921609\n",
      "Episode: 61, Total Reward: 10.94628654142389, Epsilon: 0.6866430931872001\n",
      "Episode: 62, Total Reward: 4.0827514690120275, Epsilon: 0.6832098777212641\n",
      "Episode: 63, Total Reward: 13.481821001716519, Epsilon: 0.6797938283326578\n",
      "Episode: 64, Total Reward: 5.166044718341952, Epsilon: 0.6763948591909945\n",
      "Episode: 65, Total Reward: 2.6795072143925247, Epsilon: 0.6730128848950395\n",
      "Episode: 66, Total Reward: 1.9157750342935527, Epsilon: 0.6696478204705644\n",
      "Episode: 67, Total Reward: 1.1824417009602195, Epsilon: 0.6662995813682115\n",
      "Episode: 68, Total Reward: 2.7996955122319305, Epsilon: 0.6629680834613705\n",
      "Episode: 69, Total Reward: 23.363319099957657, Epsilon: 0.6596532430440636\n",
      "Episode: 70, Total Reward: 5.36400918240917, Epsilon: 0.6563549768288433\n",
      "Episode: 71, Total Reward: 7.986887835462178, Epsilon: 0.653073201944699\n",
      "Episode: 72, Total Reward: 3.2660439427918613, Epsilon: 0.6498078359349755\n",
      "Episode: 73, Total Reward: 22.450287329600314, Epsilon: 0.6465587967553006\n",
      "Episode: 74, Total Reward: 7.352017284753572, Epsilon: 0.6433260027715241\n",
      "Episode: 75, Total Reward: 13.349867130423123, Epsilon: 0.6401093727576664\n",
      "Episode: 76, Total Reward: 7.945499609272518, Epsilon: 0.6369088258938781\n",
      "Episode: 77, Total Reward: 10.73035146061323, Epsilon: 0.6337242817644086\n",
      "Episode: 78, Total Reward: 6.813000696801617, Epsilon: 0.6305556603555866\n",
      "Episode: 79, Total Reward: 21.620220368238986, Epsilon: 0.6274028820538087\n",
      "Episode: 80, Total Reward: 9.015047342866087, Epsilon: 0.6242658676435396\n",
      "Episode: 81, Total Reward: 2.6969208913237335, Epsilon: 0.6211445383053219\n",
      "Episode: 82, Total Reward: 9.780998339826857, Epsilon: 0.6180388156137953\n",
      "Episode: 83, Total Reward: 7.885351123386708, Epsilon: 0.6149486215357263\n",
      "Episode: 84, Total Reward: 8.031430504586176, Epsilon: 0.6118738784280476\n",
      "Episode: 85, Total Reward: 3.112647419750347, Epsilon: 0.6088145090359074\n",
      "Episode: 86, Total Reward: 5.779784912048729, Epsilon: 0.6057704364907278\n",
      "Episode: 87, Total Reward: 1.9151527517185956, Epsilon: 0.6027415843082742\n",
      "Episode: 88, Total Reward: 3.7795067735555845, Epsilon: 0.5997278763867329\n",
      "Episode: 89, Total Reward: 3.746451578715396, Epsilon: 0.5967292370047992\n",
      "Episode: 90, Total Reward: 21.677994422436424, Epsilon: 0.5937455908197752\n",
      "Episode: 91, Total Reward: 3.2829801350049936, Epsilon: 0.5907768628656763\n",
      "Episode: 92, Total Reward: 13.36150285675749, Epsilon: 0.5878229785513479\n",
      "Episode: 93, Total Reward: 5.846173439762539, Epsilon: 0.5848838636585911\n",
      "Episode: 94, Total Reward: 6.384262410709682, Epsilon: 0.5819594443402982\n",
      "Episode: 95, Total Reward: 6.645863672170419, Epsilon: 0.5790496471185967\n",
      "Episode: 96, Total Reward: 23.417294102042575, Epsilon: 0.5761543988830038\n",
      "Episode: 97, Total Reward: 15.650114834864697, Epsilon: 0.5732736268885887\n",
      "Episode: 98, Total Reward: 2.6333333333333333, Epsilon: 0.5704072587541458\n",
      "Episode: 99, Total Reward: 7.578490732777525, Epsilon: 0.567555222460375\n",
      "Episode: 100, Total Reward: 3.9824476765899655, Epsilon: 0.5647174463480732\n",
      "Episode: 101, Total Reward: 4.046451578715396, Epsilon: 0.5618938591163328\n",
      "Episode: 102, Total Reward: 6.632892968901848, Epsilon: 0.5590843898207511\n",
      "Episode: 103, Total Reward: 15.364906117097163, Epsilon: 0.5562889678716474\n",
      "Episode: 104, Total Reward: 3.8788564290452654, Epsilon: 0.5535075230322891\n",
      "Episode: 105, Total Reward: 12.876832603425193, Epsilon: 0.5507399854171277\n",
      "Episode: 106, Total Reward: 24.877113588633712, Epsilon: 0.547986285490042\n",
      "Episode: 107, Total Reward: 23.149775515871884, Epsilon: 0.5452463540625918\n",
      "Episode: 108, Total Reward: 12.843697308862431, Epsilon: 0.5425201222922789\n",
      "Episode: 109, Total Reward: 10.442525332583052, Epsilon: 0.5398075216808175\n",
      "Episode: 110, Total Reward: 5.517512226255392, Epsilon: 0.5371084840724134\n",
      "Episode: 111, Total Reward: 24.20839783174126, Epsilon: 0.5344229416520513\n",
      "Episode: 112, Total Reward: 9.998757914906863, Epsilon: 0.531750826943791\n",
      "Episode: 113, Total Reward: 13.345905724598282, Epsilon: 0.5290920728090721\n",
      "Episode: 114, Total Reward: 7.478850237081105, Epsilon: 0.5264466124450268\n",
      "Episode: 115, Total Reward: 5.745501621531213, Epsilon: 0.5238143793828016\n",
      "Episode: 116, Total Reward: 8.597830096170098, Epsilon: 0.5211953074858876\n",
      "Episode: 117, Total Reward: 7.261988065049088, Epsilon: 0.5185893309484582\n",
      "Episode: 118, Total Reward: 15.94556500789244, Epsilon: 0.5159963842937159\n",
      "Episode: 119, Total Reward: 6.079332513344608, Epsilon: 0.5134164023722473\n",
      "Episode: 120, Total Reward: 2.978905846418022, Epsilon: 0.510849320360386\n",
      "Episode: 121, Total Reward: 12.711697172822765, Epsilon: 0.5082950737585841\n",
      "Episode: 122, Total Reward: 1.2796673634682842, Epsilon: 0.5057535983897912\n",
      "Episode: 123, Total Reward: 2.013462830304775, Epsilon: 0.5032248303978422\n",
      "Episode: 124, Total Reward: 6.518327923134823, Epsilon: 0.500708706245853\n",
      "Episode: 125, Total Reward: 5.879332045057952, Epsilon: 0.4982051627146237\n",
      "Episode: 126, Total Reward: 4.230801684011194, Epsilon: 0.49571413690105054\n",
      "Episode: 127, Total Reward: 5.429631500823545, Epsilon: 0.4932355662165453\n",
      "Episode: 128, Total Reward: 5.8121539616925055, Epsilon: 0.4907693883854626\n",
      "Episode: 129, Total Reward: 7.5183998799539085, Epsilon: 0.4883155414435353\n",
      "Episode: 130, Total Reward: 2.9455725130846884, Epsilon: 0.4858739637363176\n",
      "Episode: 131, Total Reward: 6.8998917972179505, Epsilon: 0.483444593917636\n",
      "Episode: 132, Total Reward: 4.583242440902298, Epsilon: 0.4810273709480478\n",
      "Episode: 133, Total Reward: 6.045868951994921, Epsilon: 0.47862223409330756\n",
      "Episode: 134, Total Reward: 12.744730782300321, Epsilon: 0.47622912292284103\n",
      "Episode: 135, Total Reward: 6.6786900826678774, Epsilon: 0.4738479773082268\n",
      "Episode: 136, Total Reward: 5.019777114259341, Epsilon: 0.47147873742168567\n",
      "Episode: 137, Total Reward: 10.179156548416445, Epsilon: 0.46912134373457726\n",
      "Episode: 138, Total Reward: 1.665465357980372, Epsilon: 0.46677573701590436\n",
      "Episode: 139, Total Reward: 17.196721041106564, Epsilon: 0.46444185833082485\n",
      "Episode: 140, Total Reward: 2.7478199378352626, Epsilon: 0.46211964903917074\n",
      "Episode: 141, Total Reward: 24.786265111823152, Epsilon: 0.4598090507939749\n",
      "Episode: 142, Total Reward: 21.083426321880882, Epsilon: 0.457510005540005\n",
      "Episode: 143, Total Reward: 9.51096718520515, Epsilon: 0.45522245551230495\n",
      "Episode: 144, Total Reward: 2.5653785916491794, Epsilon: 0.4529463432347434\n",
      "Episode: 145, Total Reward: 15.742687365228823, Epsilon: 0.4506816115185697\n",
      "Episode: 146, Total Reward: 14.75072298335977, Epsilon: 0.4484282034609769\n",
      "Episode: 147, Total Reward: 10.966623501854652, Epsilon: 0.446186062443672\n",
      "Episode: 148, Total Reward: 14.04473981564892, Epsilon: 0.4439551321314536\n",
      "Episode: 149, Total Reward: 5.365464629784676, Epsilon: 0.4417353564707963\n",
      "Episode: 150, Total Reward: 3.7116805492396576, Epsilon: 0.43952667968844233\n",
      "Episode: 151, Total Reward: 5.3157870874109046, Epsilon: 0.43732904629000013\n",
      "Episode: 152, Total Reward: 3.6457113081948775, Epsilon: 0.4351424010585501\n",
      "Episode: 153, Total Reward: 9.231096658663173, Epsilon: 0.43296668905325736\n",
      "Episode: 154, Total Reward: 7.978377016393839, Epsilon: 0.43080185560799106\n",
      "Episode: 155, Total Reward: 7.916140753561909, Epsilon: 0.4286478463299511\n",
      "Episode: 156, Total Reward: 4.332090037838703, Epsilon: 0.42650460709830135\n",
      "Episode: 157, Total Reward: 2.6491083676268863, Epsilon: 0.42437208406280985\n",
      "Episode: 158, Total Reward: 13.612484000969852, Epsilon: 0.4222502236424958\n",
      "Episode: 159, Total Reward: 14.177525807725715, Epsilon: 0.42013897252428334\n",
      "Episode: 160, Total Reward: 9.97884891548402, Epsilon: 0.4180382776616619\n",
      "Episode: 161, Total Reward: 2.8801294969714415, Epsilon: 0.4159480862733536\n",
      "Episode: 162, Total Reward: 6.345203246074302, Epsilon: 0.41386834584198684\n",
      "Episode: 163, Total Reward: 25.432765602277104, Epsilon: 0.4117990041127769\n",
      "Episode: 164, Total Reward: 3.4824016038055765, Epsilon: 0.40974000909221303\n",
      "Episode: 165, Total Reward: 12.212027746737123, Epsilon: 0.40769130904675194\n",
      "Episode: 166, Total Reward: 22.981552051607604, Epsilon: 0.40565285250151817\n",
      "Episode: 167, Total Reward: 4.511494400782889, Epsilon: 0.4036245882390106\n",
      "Episode: 168, Total Reward: 14.91096838420266, Epsilon: 0.4016064652978155\n",
      "Episode: 169, Total Reward: 8.946643032338345, Epsilon: 0.3995984329713264\n",
      "Episode: 170, Total Reward: 24.68399697019076, Epsilon: 0.3976004408064698\n",
      "Episode: 171, Total Reward: 5.753110225951134, Epsilon: 0.39561243860243744\n",
      "Episode: 172, Total Reward: 5.598849163979333, Epsilon: 0.3936343764094253\n",
      "Episode: 173, Total Reward: 13.713261403218233, Epsilon: 0.39166620452737816\n",
      "Episode: 174, Total Reward: 3.811407755946598, Epsilon: 0.3897078735047413\n",
      "Episode: 175, Total Reward: 7.66244707363382, Epsilon: 0.3877593341372176\n",
      "Episode: 176, Total Reward: 17.729622655340656, Epsilon: 0.3858205374665315\n",
      "Episode: 177, Total Reward: 3.46537776719166, Epsilon: 0.38389143477919885\n",
      "Episode: 178, Total Reward: 5.72946964108876, Epsilon: 0.3819719776053028\n",
      "Episode: 179, Total Reward: 13.868092408174988, Epsilon: 0.3800621177172763\n",
      "Episode: 180, Total Reward: 4.645358475965596, Epsilon: 0.37816180712868996\n",
      "Episode: 181, Total Reward: 21.418913084233193, Epsilon: 0.37627099809304654\n",
      "Episode: 182, Total Reward: 4.298755930461491, Epsilon: 0.3743896431025813\n",
      "Episode: 183, Total Reward: 14.277623691032948, Epsilon: 0.37251769488706843\n",
      "Episode: 184, Total Reward: 3.6450456826944917, Epsilon: 0.3706551064126331\n",
      "Episode: 185, Total Reward: 18.74372109545907, Epsilon: 0.36880183088056995\n",
      "Episode: 186, Total Reward: 1.1182336398733663, Epsilon: 0.3669578217261671\n",
      "Episode: 187, Total Reward: 4.21549612665924, Epsilon: 0.36512303261753626\n",
      "Episode: 188, Total Reward: 7.318581602688201, Epsilon: 0.3632974174544486\n",
      "Episode: 189, Total Reward: 8.694170819614046, Epsilon: 0.3614809303671764\n",
      "Episode: 190, Total Reward: 9.744232177030776, Epsilon: 0.3596735257153405\n",
      "Episode: 191, Total Reward: 9.46507491384052, Epsilon: 0.3578751580867638\n",
      "Episode: 192, Total Reward: 23.761584868677847, Epsilon: 0.35608578229633\n",
      "Episode: 193, Total Reward: 25.2097734329246, Epsilon: 0.3543053533848483\n",
      "Episode: 194, Total Reward: 9.67930688718695, Epsilon: 0.35253382661792404\n",
      "Episode: 195, Total Reward: 3.8120653400649838, Epsilon: 0.3507711574848344\n",
      "Episode: 196, Total Reward: 6.0654714604674655, Epsilon: 0.34901730169741024\n",
      "Episode: 197, Total Reward: 7.365110394799728, Epsilon: 0.3472722151889232\n",
      "Episode: 198, Total Reward: 9.81153038120041, Epsilon: 0.3455358541129786\n",
      "Episode: 199, Total Reward: 16.484743214098625, Epsilon: 0.3438081748424137\n",
      "Episode: 200, Total Reward: 5.97930684533812, Epsilon: 0.3420891339682016\n",
      "Episode: 201, Total Reward: 4.033333333333333, Epsilon: 0.3403786882983606\n",
      "Episode: 202, Total Reward: 4.77883272924831, Epsilon: 0.3386767948568688\n",
      "Episode: 203, Total Reward: 17.898490911935838, Epsilon: 0.33698341088258443\n",
      "Episode: 204, Total Reward: 22.683444692184025, Epsilon: 0.3352984938281715\n",
      "Episode: 205, Total Reward: 8.395732349632349, Epsilon: 0.33362200135903064\n",
      "Episode: 206, Total Reward: 10.779156639984643, Epsilon: 0.33195389135223546\n",
      "Episode: 207, Total Reward: 21.181960720538676, Epsilon: 0.3302941218954743\n",
      "Episode: 208, Total Reward: 7.848347294766919, Epsilon: 0.32864265128599696\n",
      "Episode: 209, Total Reward: 16.644406973011193, Epsilon: 0.326999438029567\n",
      "Episode: 210, Total Reward: 11.446445497630405, Epsilon: 0.3253644408394192\n",
      "Episode: 211, Total Reward: 18.645030297735904, Epsilon: 0.3237376186352221\n",
      "Episode: 212, Total Reward: 4.779307687748202, Epsilon: 0.322118930542046\n",
      "Episode: 213, Total Reward: 23.809017587329226, Epsilon: 0.32050833588933575\n",
      "Episode: 214, Total Reward: 3.0131583425367054, Epsilon: 0.31890579420988907\n",
      "Episode: 215, Total Reward: 22.79847165427955, Epsilon: 0.3173112652388396\n",
      "Episode: 216, Total Reward: 2.4993780516752837, Epsilon: 0.3157247089126454\n",
      "Episode: 217, Total Reward: 5.046446298431657, Epsilon: 0.3141460853680822\n",
      "Episode: 218, Total Reward: 11.979526228428337, Epsilon: 0.3125753549412418\n",
      "Episode: 219, Total Reward: 2.5987979638514176, Epsilon: 0.31101247816653554\n",
      "Episode: 220, Total Reward: 11.810478842737771, Epsilon: 0.30945741577570285\n",
      "Episode: 221, Total Reward: 27.49271528538321, Epsilon: 0.3079101286968243\n",
      "Episode: 222, Total Reward: 9.553097445564676, Epsilon: 0.3063705780533402\n",
      "Episode: 223, Total Reward: 10.74550142209351, Epsilon: 0.30483872516307353\n",
      "Episode: 224, Total Reward: 3.3676891557421866, Epsilon: 0.3033145315372582\n",
      "Episode: 225, Total Reward: 7.914572997843533, Epsilon: 0.3017979588795719\n",
      "Episode: 226, Total Reward: 8.078490506623247, Epsilon: 0.30028896908517405\n",
      "Episode: 227, Total Reward: 25.09564794847137, Epsilon: 0.2987875242397482\n",
      "Episode: 228, Total Reward: 6.812182174693273, Epsilon: 0.29729358661854943\n",
      "Episode: 229, Total Reward: 11.979778830964214, Epsilon: 0.29580711868545667\n",
      "Episode: 230, Total Reward: 4.812170120870821, Epsilon: 0.2943280830920294\n",
      "Episode: 231, Total Reward: 3.497065513399303, Epsilon: 0.29285644267656924\n",
      "Episode: 232, Total Reward: 16.610504370630032, Epsilon: 0.2913921604631864\n",
      "Episode: 233, Total Reward: 24.45084087122069, Epsilon: 0.28993519966087045\n",
      "Episode: 234, Total Reward: 5.964883971675879, Epsilon: 0.2884855236625661\n",
      "Episode: 235, Total Reward: 6.046894146613125, Epsilon: 0.28704309604425327\n",
      "Episode: 236, Total Reward: 12.429783896964045, Epsilon: 0.285607880564032\n",
      "Episode: 237, Total Reward: 4.546451578715396, Epsilon: 0.28417984116121187\n",
      "Episode: 238, Total Reward: 25.128016569502122, Epsilon: 0.2827589419554058\n",
      "Episode: 239, Total Reward: 12.876793927455616, Epsilon: 0.28134514724562876\n",
      "Episode: 240, Total Reward: 12.298208774002592, Epsilon: 0.2799384215094006\n",
      "Episode: 241, Total Reward: 5.896874996396284, Epsilon: 0.27853872940185365\n",
      "Episode: 242, Total Reward: 5.579778936418984, Epsilon: 0.27714603575484437\n",
      "Episode: 243, Total Reward: 2.631551367227586, Epsilon: 0.2757603055760701\n",
      "Episode: 244, Total Reward: 2.018315462896295, Epsilon: 0.2743815040481898\n",
      "Episode: 245, Total Reward: 10.999752153514873, Epsilon: 0.2730095965279488\n",
      "Episode: 246, Total Reward: 20.81087788474977, Epsilon: 0.27164454854530906\n",
      "Episode: 247, Total Reward: 18.21302309588505, Epsilon: 0.2702863258025825\n",
      "Episode: 248, Total Reward: 18.217036553984762, Epsilon: 0.2689348941735696\n",
      "Episode: 249, Total Reward: 23.844257595308118, Epsilon: 0.26759021970270175\n",
      "Episode: 250, Total Reward: 27.607039395389968, Epsilon: 0.2662522686041882\n",
      "Episode: 251, Total Reward: 8.896380446538167, Epsilon: 0.2649210072611673\n",
      "Episode: 252, Total Reward: 13.310879661339683, Epsilon: 0.26359640222486147\n",
      "Episode: 253, Total Reward: 20.33080697972853, Epsilon: 0.26227842021373715\n",
      "Episode: 254, Total Reward: 12.677904928761667, Epsilon: 0.2609670281126685\n",
      "Episode: 255, Total Reward: 23.229491049765215, Epsilon: 0.25966219297210513\n",
      "Episode: 256, Total Reward: 13.017383680982407, Epsilon: 0.2583638820072446\n",
      "Episode: 257, Total Reward: 8.778017145031734, Epsilon: 0.2570720625972084\n",
      "Episode: 258, Total Reward: 22.0772010098638, Epsilon: 0.25578670228422234\n",
      "Episode: 259, Total Reward: 19.610751528445093, Epsilon: 0.25450776877280124\n",
      "Episode: 260, Total Reward: 7.278726143728467, Epsilon: 0.2532352299289372\n",
      "Episode: 261, Total Reward: 7.079784899990366, Epsilon: 0.2519690537792925\n",
      "Episode: 262, Total Reward: 13.183197452671772, Epsilon: 0.2507092085103961\n",
      "Episode: 263, Total Reward: 13.610773045422222, Epsilon: 0.2494556624678441\n",
      "Episode: 264, Total Reward: 16.64413450449101, Epsilon: 0.24820838415550486\n",
      "Episode: 265, Total Reward: 10.944084918841481, Epsilon: 0.24696734223472733\n",
      "Episode: 266, Total Reward: 5.612650030689837, Epsilon: 0.2457325055235537\n",
      "Episode: 267, Total Reward: 6.846039920045269, Epsilon: 0.24450384299593592\n",
      "Episode: 268, Total Reward: 12.598145751691995, Epsilon: 0.24328132378095624\n",
      "Episode: 269, Total Reward: 27.54249030614129, Epsilon: 0.24206491716205145\n",
      "Episode: 270, Total Reward: 18.409184992289926, Epsilon: 0.2408545925762412\n",
      "Episode: 271, Total Reward: 19.34345861918134, Epsilon: 0.23965031961336\n",
      "Episode: 272, Total Reward: 6.779306900833245, Epsilon: 0.2384520680152932\n",
      "Episode: 273, Total Reward: 5.481628901345275, Epsilon: 0.23725980767521673\n",
      "Episode: 274, Total Reward: 3.5784656606936793, Epsilon: 0.23607350863684065\n",
      "Episode: 275, Total Reward: 7.1187027001555965, Epsilon: 0.23489314109365644\n",
      "Episode: 276, Total Reward: 12.829897956202041, Epsilon: 0.23371867538818816\n",
      "Episode: 277, Total Reward: 3.483880380783735, Epsilon: 0.23255008201124722\n",
      "Episode: 278, Total Reward: 28.475973293884604, Epsilon: 0.231387331601191\n",
      "Episode: 279, Total Reward: 5.897337235458764, Epsilon: 0.23023039494318503\n",
      "Episode: 280, Total Reward: 12.012649937502154, Epsilon: 0.2290792429684691\n",
      "Episode: 281, Total Reward: 14.51236227444091, Epsilon: 0.22793384675362674\n",
      "Episode: 282, Total Reward: 10.143814532256654, Epsilon: 0.22679417751985861\n",
      "Episode: 283, Total Reward: 4.482441700960219, Epsilon: 0.22566020663225933\n",
      "Episode: 284, Total Reward: 7.285708482025992, Epsilon: 0.22453190559909803\n",
      "Episode: 285, Total Reward: 9.933247773578719, Epsilon: 0.22340924607110255\n",
      "Episode: 286, Total Reward: 9.829576067795402, Epsilon: 0.22229219984074702\n",
      "Episode: 287, Total Reward: 15.109862798929496, Epsilon: 0.2211807388415433\n",
      "Episode: 288, Total Reward: 15.677580554051527, Epsilon: 0.22007483514733558\n",
      "Episode: 289, Total Reward: 11.031414733932152, Epsilon: 0.2189744609715989\n",
      "Episode: 290, Total Reward: 4.432044433857515, Epsilon: 0.2178795886667409\n",
      "Episode: 291, Total Reward: 11.5104174658608, Epsilon: 0.2167901907234072\n",
      "Episode: 292, Total Reward: 16.97883483868755, Epsilon: 0.21570623976979014\n",
      "Episode: 293, Total Reward: 4.745975472799184, Epsilon: 0.21462770857094118\n",
      "Episode: 294, Total Reward: 8.153547620514992, Epsilon: 0.21355457002808648\n",
      "Episode: 295, Total Reward: 6.429576342302315, Epsilon: 0.21248679717794605\n",
      "Episode: 296, Total Reward: 25.940536916443357, Epsilon: 0.21142436319205632\n",
      "Episode: 297, Total Reward: 20.968233558764044, Epsilon: 0.21036724137609603\n",
      "Episode: 298, Total Reward: 6.9450391361267005, Epsilon: 0.20931540516921554\n",
      "Episode: 299, Total Reward: 12.54348063161347, Epsilon: 0.20826882814336947\n",
      "Episode: 300, Total Reward: 23.934996404182055, Epsilon: 0.20722748400265262\n",
      "Episode: 301, Total Reward: 9.411548566495297, Epsilon: 0.20619134658263935\n",
      "Episode: 302, Total Reward: 7.884539612537691, Epsilon: 0.20516038984972615\n",
      "Episode: 303, Total Reward: 10.5448659016457, Epsilon: 0.2041345879004775\n",
      "Episode: 304, Total Reward: 3.7796673634682842, Epsilon: 0.2031139149609751\n",
      "Episode: 305, Total Reward: 11.298894050292427, Epsilon: 0.20209834538617025\n",
      "Episode: 306, Total Reward: 14.543924311809235, Epsilon: 0.2010878536592394\n",
      "Episode: 307, Total Reward: 21.976813685433214, Epsilon: 0.2000824143909432\n",
      "Episode: 308, Total Reward: 13.912640050916917, Epsilon: 0.19908200231898848\n",
      "Episode: 309, Total Reward: 16.878218030729936, Epsilon: 0.19808659230739353\n",
      "Episode: 310, Total Reward: 7.69625254126683, Epsilon: 0.19709615934585656\n",
      "Episode: 311, Total Reward: 9.64786336117636, Epsilon: 0.19611067854912728\n",
      "Episode: 312, Total Reward: 24.906980231491215, Epsilon: 0.19513012515638165\n",
      "Episode: 313, Total Reward: 25.910769986105155, Epsilon: 0.19415447453059972\n",
      "Episode: 314, Total Reward: 9.411438234489482, Epsilon: 0.19318370215794672\n",
      "Episode: 315, Total Reward: 11.596549576352588, Epsilon: 0.192217783647157\n",
      "Episode: 316, Total Reward: 17.644387651466225, Epsilon: 0.1912566947289212\n",
      "Episode: 317, Total Reward: 2.946020961436447, Epsilon: 0.1903004112552766\n",
      "Episode: 318, Total Reward: 23.44511947951107, Epsilon: 0.18934890919900021\n",
      "Episode: 319, Total Reward: 25.751637486744432, Epsilon: 0.18840216465300522\n",
      "Episode: 320, Total Reward: 27.441801748971432, Epsilon: 0.18746015382974018\n",
      "Episode: 321, Total Reward: 27.23312055242951, Epsilon: 0.1865228530605915\n",
      "Episode: 322, Total Reward: 12.144724418662113, Epsilon: 0.18559023879528855\n",
      "Episode: 323, Total Reward: 22.581465817155426, Epsilon: 0.1846622876013121\n",
      "Episode: 324, Total Reward: 15.047696129070737, Epsilon: 0.18373897616330553\n",
      "Episode: 325, Total Reward: 11.16524527147062, Epsilon: 0.182820281282489\n",
      "Episode: 326, Total Reward: 17.876859701834878, Epsilon: 0.18190617987607657\n",
      "Episode: 327, Total Reward: 27.32841833605359, Epsilon: 0.18099664897669618\n",
      "Episode: 328, Total Reward: 6.612641352551635, Epsilon: 0.1800916657318127\n",
      "Episode: 329, Total Reward: 15.809866341538473, Epsilon: 0.17919120740315364\n",
      "Episode: 330, Total Reward: 5.543886466952016, Epsilon: 0.17829525136613786\n",
      "Episode: 331, Total Reward: 28.243614027531258, Epsilon: 0.17740377510930716\n",
      "Episode: 332, Total Reward: 12.07789030203415, Epsilon: 0.17651675623376062\n",
      "Episode: 333, Total Reward: 16.863541557607633, Epsilon: 0.1756341724525918\n",
      "Episode: 334, Total Reward: 15.686415706612605, Epsilon: 0.17475600159032884\n",
      "Episode: 335, Total Reward: 23.011827416258363, Epsilon: 0.17388222158237718\n",
      "Episode: 336, Total Reward: 3.5478619837706002, Epsilon: 0.1730128104744653\n",
      "Episode: 337, Total Reward: 8.513112965098324, Epsilon: 0.17214774642209296\n",
      "Episode: 338, Total Reward: 8.244900807777569, Epsilon: 0.1712870076899825\n",
      "Episode: 339, Total Reward: 10.778884533646384, Epsilon: 0.17043057265153258\n",
      "Episode: 340, Total Reward: 14.752307676335933, Epsilon: 0.16957841978827493\n",
      "Episode: 341, Total Reward: 17.611885984685568, Epsilon: 0.16873052768933355\n",
      "Episode: 342, Total Reward: 27.507594741822952, Epsilon: 0.1678868750508869\n",
      "Episode: 343, Total Reward: 6.778349698518451, Epsilon: 0.16704744067563246\n",
      "Episode: 344, Total Reward: 6.832087887711595, Epsilon: 0.1662122034722543\n",
      "Episode: 345, Total Reward: 16.761701783902424, Epsilon: 0.16538114245489302\n",
      "Episode: 346, Total Reward: 10.47821205038542, Epsilon: 0.16455423674261854\n",
      "Episode: 347, Total Reward: 15.945486990233725, Epsilon: 0.16373146555890544\n",
      "Episode: 348, Total Reward: 28.542974962164152, Epsilon: 0.16291280823111093\n",
      "Episode: 349, Total Reward: 6.4796672568929665, Epsilon: 0.16209824418995536\n",
      "Episode: 350, Total Reward: 16.74546809738089, Epsilon: 0.16128775296900558\n",
      "Episode: 351, Total Reward: 19.148495206019778, Epsilon: 0.16048131420416054\n",
      "Episode: 352, Total Reward: 5.711916622332718, Epsilon: 0.15967890763313974\n",
      "Episode: 353, Total Reward: 10.179778830995378, Epsilon: 0.15888051309497406\n",
      "Episode: 354, Total Reward: 11.477735721403, Epsilon: 0.1580861105294992\n",
      "Episode: 355, Total Reward: 2.013000696801617, Epsilon: 0.1572956799768517\n",
      "Episode: 356, Total Reward: 8.710748821424385, Epsilon: 0.15650920157696743\n",
      "Episode: 357, Total Reward: 9.412168070638781, Epsilon: 0.1557266555690826\n",
      "Episode: 358, Total Reward: 10.644542706843113, Epsilon: 0.1549480222912372\n",
      "Episode: 359, Total Reward: 9.87835981510532, Epsilon: 0.15417328217978102\n",
      "Episode: 360, Total Reward: 3.6787320067316505, Epsilon: 0.1534024157688821\n",
      "Episode: 361, Total Reward: 5.268932797871055, Epsilon: 0.1526354036900377\n",
      "Episode: 362, Total Reward: 6.76585377548353, Epsilon: 0.1518722266715875\n",
      "Episode: 363, Total Reward: 4.53020553411137, Epsilon: 0.15111286553822956\n",
      "Episode: 364, Total Reward: 12.97811175471233, Epsilon: 0.15035730121053842\n",
      "Episode: 365, Total Reward: 7.612490969275168, Epsilon: 0.14960551470448571\n",
      "Episode: 366, Total Reward: 2.946491675870039, Epsilon: 0.14885748713096328\n",
      "Episode: 367, Total Reward: 15.394499948828596, Epsilon: 0.14811319969530845\n",
      "Episode: 368, Total Reward: 16.08458954503743, Epsilon: 0.1473726336968319\n",
      "Episode: 369, Total Reward: 8.64597349656211, Epsilon: 0.14663577052834775\n",
      "Episode: 370, Total Reward: 6.013112269752317, Epsilon: 0.14590259167570602\n",
      "Episode: 371, Total Reward: 27.285443241652363, Epsilon: 0.1451730787173275\n",
      "Episode: 372, Total Reward: 11.05050273278067, Epsilon: 0.14444721332374086\n",
      "Episode: 373, Total Reward: 12.811749179155813, Epsilon: 0.14372497725712216\n",
      "Episode: 374, Total Reward: 19.781618921571216, Epsilon: 0.14300635237083656\n",
      "Episode: 375, Total Reward: 16.64458052271954, Epsilon: 0.14229132060898236\n",
      "Episode: 376, Total Reward: 24.2467818279589, Epsilon: 0.14157986400593744\n",
      "Episode: 377, Total Reward: 13.097976153523913, Epsilon: 0.14087196468590776\n",
      "Episode: 378, Total Reward: 22.21586388272497, Epsilon: 0.14016760486247823\n",
      "Episode: 379, Total Reward: 13.550951577304746, Epsilon: 0.13946676683816583\n",
      "Episode: 380, Total Reward: 14.061756862934073, Epsilon: 0.138769433003975\n",
      "Episode: 381, Total Reward: 17.809396463990677, Epsilon: 0.13807558583895513\n",
      "Episode: 382, Total Reward: 15.779778830967768, Epsilon: 0.13738520790976036\n",
      "Episode: 383, Total Reward: 12.7454013219463, Epsilon: 0.13669828187021155\n",
      "Episode: 384, Total Reward: 9.082395522752226, Epsilon: 0.13601479046086049\n",
      "Episode: 385, Total Reward: 6.309874830769578, Epsilon: 0.1353347165085562\n",
      "Episode: 386, Total Reward: 14.316298930716904, Epsilon: 0.1346580429260134\n",
      "Episode: 387, Total Reward: 26.350990005898087, Epsilon: 0.13398475271138335\n",
      "Episode: 388, Total Reward: 12.544834068963196, Epsilon: 0.13331482894782642\n",
      "Episode: 389, Total Reward: 26.2431277109261, Epsilon: 0.13264825480308728\n",
      "Episode: 390, Total Reward: 20.383584168654412, Epsilon: 0.13198501352907185\n",
      "Episode: 391, Total Reward: 9.311235001497975, Epsilon: 0.1313250884614265\n",
      "Episode: 392, Total Reward: 25.32879735601599, Epsilon: 0.13066846301911936\n",
      "Episode: 393, Total Reward: 5.447864298088065, Epsilon: 0.13001512070402377\n",
      "Episode: 394, Total Reward: 24.95015799418641, Epsilon: 0.12936504510050365\n",
      "Episode: 395, Total Reward: 6.2154758268092225, Epsilon: 0.12871821987500112\n",
      "Episode: 396, Total Reward: 12.3437546258525, Epsilon: 0.12807462877562611\n",
      "Episode: 397, Total Reward: 23.060980675218733, Epsilon: 0.12743425563174798\n",
      "Episode: 398, Total Reward: 24.444767739821113, Epsilon: 0.12679708435358925\n",
      "Episode: 399, Total Reward: 10.615303224663284, Epsilon: 0.1261630989318213\n",
      "Episode: 400, Total Reward: 17.71030224498591, Epsilon: 0.1255322834371622\n",
      "Episode: 401, Total Reward: 1.2130006968016176, Epsilon: 0.12490462201997637\n",
      "Episode: 402, Total Reward: 12.81109756705203, Epsilon: 0.1242800989098765\n",
      "Episode: 403, Total Reward: 10.145149178560898, Epsilon: 0.12365869841532712\n",
      "Episode: 404, Total Reward: 8.919273360267166, Epsilon: 0.12304040492325048\n",
      "Episode: 405, Total Reward: 7.130126723408856, Epsilon: 0.12242520289863423\n",
      "Episode: 406, Total Reward: 27.816523266927625, Epsilon: 0.12181307688414106\n",
      "Episode: 407, Total Reward: 17.0012478547332, Epsilon: 0.12120401149972035\n",
      "Episode: 408, Total Reward: 17.69498655533423, Epsilon: 0.12059799144222175\n",
      "Episode: 409, Total Reward: 17.97678179106627, Epsilon: 0.11999500148501063\n",
      "Episode: 410, Total Reward: 4.182091835753768, Epsilon: 0.11939502647758558\n",
      "Episode: 411, Total Reward: 6.150372489219567, Epsilon: 0.11879805134519765\n",
      "Episode: 412, Total Reward: 10.179362642144437, Epsilon: 0.11820406108847166\n",
      "Episode: 413, Total Reward: 27.783079567596403, Epsilon: 0.1176130407830293\n",
      "Episode: 414, Total Reward: 8.844924427805987, Epsilon: 0.11702497557911415\n",
      "Episode: 415, Total Reward: 17.6426427231891, Epsilon: 0.11643985070121858\n",
      "Episode: 416, Total Reward: 1.9134628303047752, Epsilon: 0.11585765144771248\n",
      "Episode: 417, Total Reward: 24.85213799619328, Epsilon: 0.11527836319047392\n",
      "Episode: 418, Total Reward: 10.344150583159392, Epsilon: 0.11470197137452155\n",
      "Episode: 419, Total Reward: 27.607099859571086, Epsilon: 0.11412846151764894\n",
      "Episode: 420, Total Reward: 23.411232552619754, Epsilon: 0.1135578192100607\n",
      "Episode: 421, Total Reward: 27.30944386532448, Epsilon: 0.11299003011401039\n",
      "Episode: 422, Total Reward: 5.312174400509056, Epsilon: 0.11242507996344034\n",
      "Episode: 423, Total Reward: 6.978848966232843, Epsilon: 0.11186295456362313\n",
      "Episode: 424, Total Reward: 25.40871305629051, Epsilon: 0.11130363979080501\n",
      "Episode: 425, Total Reward: 25.877230129218887, Epsilon: 0.11074712159185099\n",
      "Episode: 426, Total Reward: 4.968972895025699, Epsilon: 0.11019338598389174\n",
      "Episode: 427, Total Reward: 27.18497966194954, Epsilon: 0.10964241905397228\n",
      "Episode: 428, Total Reward: 26.895355927881543, Epsilon: 0.10909420695870241\n",
      "Episode: 429, Total Reward: 1.9151527517185956, Epsilon: 0.1085487359239089\n",
      "Episode: 430, Total Reward: 20.2098031169064, Epsilon: 0.10800599224428936\n",
      "Episode: 431, Total Reward: 2.812378333155173, Epsilon: 0.10746596228306791\n",
      "Episode: 432, Total Reward: 9.545973384667894, Epsilon: 0.10692863247165257\n",
      "Episode: 433, Total Reward: 26.977839849208515, Epsilon: 0.1063939893092943\n",
      "Episode: 434, Total Reward: 25.016594153106116, Epsilon: 0.10586201936274783\n",
      "Episode: 435, Total Reward: 17.26217787115704, Epsilon: 0.10533270926593409\n",
      "Episode: 436, Total Reward: 2.8793542947697803, Epsilon: 0.10480604571960442\n",
      "Episode: 437, Total Reward: 7.282792353080876, Epsilon: 0.1042820154910064\n",
      "Episode: 438, Total Reward: 8.577904682733408, Epsilon: 0.10376060541355137\n",
      "Episode: 439, Total Reward: 24.97817213438879, Epsilon: 0.1032418023864836\n",
      "Episode: 440, Total Reward: 17.160427339474047, Epsilon: 0.10272559337455119\n",
      "Episode: 441, Total Reward: 14.776306676211503, Epsilon: 0.10221196540767843\n",
      "Episode: 442, Total Reward: 22.98249466614877, Epsilon: 0.10170090558064004\n",
      "Episode: 443, Total Reward: 5.611911342048979, Epsilon: 0.10119240105273684\n",
      "Episode: 444, Total Reward: 13.376244099720113, Epsilon: 0.10068643904747315\n",
      "Episode: 445, Total Reward: 16.74487438143725, Epsilon: 0.10018300685223579\n",
      "Episode: 446, Total Reward: 2.812840547725858, Epsilon: 0.0996820918179746\n",
      "Episode: 447, Total Reward: 5.679306938037714, Epsilon: 0.09918368135888474\n",
      "Episode: 448, Total Reward: 14.397112842911966, Epsilon: 0.09868776295209031\n",
      "Episode: 449, Total Reward: 22.367205182669544, Epsilon: 0.09819432413732986\n",
      "Episode: 450, Total Reward: 4.111692482014309, Epsilon: 0.09770335251664321\n",
      "Episode: 451, Total Reward: 12.061178898331393, Epsilon: 0.09721483575406\n",
      "Episode: 452, Total Reward: 13.628769331273672, Epsilon: 0.09672876157528969\n",
      "Episode: 453, Total Reward: 22.97679339608904, Epsilon: 0.09624511776741324\n",
      "Episode: 454, Total Reward: 10.498715140718659, Epsilon: 0.09576389217857617\n",
      "Episode: 455, Total Reward: 6.8453610004619145, Epsilon: 0.09528507271768329\n",
      "Episode: 456, Total Reward: 17.08289766710441, Epsilon: 0.09480864735409487\n",
      "Episode: 457, Total Reward: 10.298595703743747, Epsilon: 0.0943346041173244\n",
      "Episode: 458, Total Reward: 22.87470918439824, Epsilon: 0.09386293109673778\n",
      "Episode: 459, Total Reward: 15.713964276012392, Epsilon: 0.09339361644125409\n",
      "Episode: 460, Total Reward: 20.328646457380643, Epsilon: 0.09292664835904782\n",
      "Episode: 461, Total Reward: 12.563531688588123, Epsilon: 0.09246201511725258\n",
      "Episode: 462, Total Reward: 12.176910789822436, Epsilon: 0.09199970504166631\n",
      "Episode: 463, Total Reward: 21.176947382244368, Epsilon: 0.09153970651645797\n",
      "Episode: 464, Total Reward: 23.909321402800597, Epsilon: 0.09108200798387568\n",
      "Episode: 465, Total Reward: 16.031880689656106, Epsilon: 0.0906265979439563\n",
      "Episode: 466, Total Reward: 14.911285285659806, Epsilon: 0.09017346495423652\n",
      "Episode: 467, Total Reward: 7.101638066963794, Epsilon: 0.08972259762946533\n",
      "Episode: 468, Total Reward: 13.182453576239618, Epsilon: 0.089273984641318\n",
      "Episode: 469, Total Reward: 15.544412325718142, Epsilon: 0.0888276147181114\n",
      "Episode: 470, Total Reward: 27.82977037834484, Epsilon: 0.08838347664452084\n",
      "Episode: 471, Total Reward: 20.60993545953093, Epsilon: 0.08794155926129824\n",
      "Episode: 472, Total Reward: 28.008554912508696, Epsilon: 0.08750185146499175\n",
      "Episode: 473, Total Reward: 21.997610045389237, Epsilon: 0.08706434220766679\n",
      "Episode: 474, Total Reward: 25.61911508653025, Epsilon: 0.08662902049662846\n",
      "Episode: 475, Total Reward: 17.477727174464295, Epsilon: 0.08619587539414532\n",
      "Episode: 476, Total Reward: 7.745157173289914, Epsilon: 0.08576489601717459\n",
      "Episode: 477, Total Reward: 25.019122456922908, Epsilon: 0.08533607153708872\n",
      "Episode: 478, Total Reward: 6.899914463588292, Epsilon: 0.08490939117940327\n",
      "Episode: 479, Total Reward: 23.442809708752126, Epsilon: 0.08448484422350626\n",
      "Episode: 480, Total Reward: 2.013000696801617, Epsilon: 0.08406242000238873\n",
      "Episode: 481, Total Reward: 26.767760977806553, Epsilon: 0.08364210790237678\n",
      "Episode: 482, Total Reward: 6.145974413516467, Epsilon: 0.0832238973628649\n",
      "Episode: 483, Total Reward: 12.6788443456845, Epsilon: 0.08280777787605056\n",
      "Episode: 484, Total Reward: 23.01437858288351, Epsilon: 0.08239373898667031\n",
      "Episode: 485, Total Reward: 20.33108213253485, Epsilon: 0.08198177029173696\n",
      "Episode: 486, Total Reward: 9.096772066489004, Epsilon: 0.08157186144027828\n",
      "Episode: 487, Total Reward: 1.665465357980372, Epsilon: 0.0811640021330769\n",
      "Episode: 488, Total Reward: 4.978845409167895, Epsilon: 0.08075818212241151\n",
      "Episode: 489, Total Reward: 24.018698758266357, Epsilon: 0.08035439121179945\n",
      "Episode: 490, Total Reward: 24.951491006070558, Epsilon: 0.07995261925574046\n",
      "Episode: 491, Total Reward: 23.943466233091517, Epsilon: 0.07955285615946175\n",
      "Episode: 492, Total Reward: 19.96657122514795, Epsilon: 0.07915509187866444\n",
      "Episode: 493, Total Reward: 10.584542785031756, Epsilon: 0.07875931641927113\n",
      "Episode: 494, Total Reward: 14.245501403940402, Epsilon: 0.07836551983717477\n",
      "Episode: 495, Total Reward: 17.61216795990105, Epsilon: 0.07797369223798889\n",
      "Episode: 496, Total Reward: 17.31495211005229, Epsilon: 0.07758382377679894\n",
      "Episode: 497, Total Reward: 14.445045874791047, Epsilon: 0.07719590465791494\n",
      "Episode: 498, Total Reward: 25.34445741344577, Epsilon: 0.07680992513462537\n",
      "Episode: 499, Total Reward: 20.278051116409554, Epsilon: 0.07642587550895225\n",
      "Episode: 500, Total Reward: 25.63046393261208, Epsilon: 0.07604374613140748\n",
      "Episode: 501, Total Reward: 18.38085936720202, Epsilon: 0.07566352740075044\n",
      "Episode: 502, Total Reward: 12.978694228340002, Epsilon: 0.07528520976374668\n",
      "Episode: 503, Total Reward: 16.113878939046973, Epsilon: 0.07490878371492794\n",
      "Episode: 504, Total Reward: 7.980517667534146, Epsilon: 0.0745342397963533\n",
      "Episode: 505, Total Reward: 13.443015063622545, Epsilon: 0.07416156859737154\n",
      "Episode: 506, Total Reward: 4.912166062581644, Epsilon: 0.07379076075438468\n",
      "Episode: 507, Total Reward: 10.31201777310242, Epsilon: 0.07342180695061275\n",
      "Episode: 508, Total Reward: 15.348409348692773, Epsilon: 0.07305469791585968\n",
      "Episode: 509, Total Reward: 11.130741029466698, Epsilon: 0.07268942442628039\n",
      "Episode: 510, Total Reward: 10.262751212961877, Epsilon: 0.07232597730414898\n",
      "Episode: 511, Total Reward: 17.831407872436678, Epsilon: 0.07196434741762824\n",
      "Episode: 512, Total Reward: 16.477127307603556, Epsilon: 0.0716045256805401\n",
      "Episode: 513, Total Reward: 16.713507304976417, Epsilon: 0.0712465030521374\n",
      "Episode: 514, Total Reward: 1.2796673634682842, Epsilon: 0.0708902705368767\n",
      "Episode: 515, Total Reward: 17.51657489600975, Epsilon: 0.07053581918419231\n",
      "Episode: 516, Total Reward: 10.945823320506717, Epsilon: 0.07018314008827135\n",
      "Episode: 517, Total Reward: 25.016576318741595, Epsilon: 0.06983222438783\n",
      "Episode: 518, Total Reward: 9.586363485300788, Epsilon: 0.06948306326589085\n",
      "Episode: 519, Total Reward: 27.098944162920155, Epsilon: 0.0691356479495614\n",
      "Episode: 520, Total Reward: 14.678364693402477, Epsilon: 0.06878996970981359\n",
      "Episode: 521, Total Reward: 28.6077668609283, Epsilon: 0.06844601986126451\n",
      "Episode: 522, Total Reward: 16.448352144981563, Epsilon: 0.06810378976195819\n",
      "Episode: 523, Total Reward: 14.035037680672614, Epsilon: 0.0677632708131484\n",
      "Episode: 524, Total Reward: 1.7666666666666668, Epsilon: 0.06742445445908266\n",
      "Episode: 525, Total Reward: 25.051372594274778, Epsilon: 0.06708733218678724\n",
      "Episode: 526, Total Reward: 26.71600618526409, Epsilon: 0.0667518955258533\n",
      "Episode: 527, Total Reward: 20.42812891688477, Epsilon: 0.06641813604822402\n",
      "Episode: 528, Total Reward: 19.345031304523022, Epsilon: 0.0660860453679829\n",
      "Episode: 529, Total Reward: 24.913131136774204, Epsilon: 0.06575561514114299\n",
      "Episode: 530, Total Reward: 17.597933678395844, Epsilon: 0.06542683706543727\n",
      "Episode: 531, Total Reward: 27.473442957687798, Epsilon: 0.06509970288011008\n",
      "Episode: 532, Total Reward: 27.109981170776788, Epsilon: 0.06477420436570952\n",
      "Episode: 533, Total Reward: 28.176839707020328, Epsilon: 0.06445033334388098\n",
      "Episode: 534, Total Reward: 23.912754567610886, Epsilon: 0.06412808167716157\n",
      "Episode: 535, Total Reward: 14.61552521665541, Epsilon: 0.06380744126877576\n",
      "Episode: 536, Total Reward: 7.3683104237239245, Epsilon: 0.06348840406243188\n",
      "Episode: 537, Total Reward: 9.216123948871052, Epsilon: 0.06317096204211972\n",
      "Episode: 538, Total Reward: 10.377741269022875, Epsilon: 0.06285510723190912\n",
      "Episode: 539, Total Reward: 2.2793542947697802, Epsilon: 0.06254083169574957\n",
      "Episode: 540, Total Reward: 24.917388380518734, Epsilon: 0.062228127537270826\n",
      "Episode: 541, Total Reward: 27.780372148031418, Epsilon: 0.06191698689958447\n",
      "Episode: 542, Total Reward: 13.677740318617403, Epsilon: 0.061607401965086545\n",
      "Episode: 543, Total Reward: 24.51549997839009, Epsilon: 0.06129936495526111\n",
      "Episode: 544, Total Reward: 28.107277576827133, Epsilon: 0.0609928681304848\n",
      "Episode: 545, Total Reward: 26.432694443260306, Epsilon: 0.060687903789832374\n",
      "Episode: 546, Total Reward: 26.050557423659278, Epsilon: 0.06038446427088321\n",
      "Episode: 547, Total Reward: 12.278820326778353, Epsilon: 0.06008254194952879\n",
      "Episode: 548, Total Reward: 23.44535070824042, Epsilon: 0.05978212923978115\n",
      "Episode: 549, Total Reward: 16.298585807359753, Epsilon: 0.05948321859358224\n",
      "Episode: 550, Total Reward: 17.677266145433038, Epsilon: 0.05918580250061433\n",
      "Episode: 551, Total Reward: 27.043154392844148, Epsilon: 0.058889873488111255\n",
      "Episode: 552, Total Reward: 25.881349347291167, Epsilon: 0.058595424120670696\n",
      "Episode: 553, Total Reward: 26.974444584780734, Epsilon: 0.05830244700006734\n",
      "Episode: 554, Total Reward: 24.044484299357137, Epsilon: 0.058010934765067\n",
      "Episode: 555, Total Reward: 10.246317972112449, Epsilon: 0.05772088009124167\n",
      "Episode: 556, Total Reward: 13.2802419313372, Epsilon: 0.05743227569078546\n",
      "Episode: 557, Total Reward: 27.14171156774304, Epsilon: 0.05714511431233153\n",
      "Episode: 558, Total Reward: 12.118570816670141, Epsilon: 0.05685938874076987\n",
      "Episode: 559, Total Reward: 8.812838513840173, Epsilon: 0.056575091797066025\n",
      "Episode: 560, Total Reward: 12.983252017354602, Epsilon: 0.056292216338080694\n",
      "Episode: 561, Total Reward: 18.318654721534177, Epsilon: 0.05601075525639029\n",
      "Episode: 562, Total Reward: 29.14330121297231, Epsilon: 0.05573070148010834\n",
      "Episode: 563, Total Reward: 11.352167898375182, Epsilon: 0.0554520479727078\n",
      "Episode: 564, Total Reward: 11.418567323366448, Epsilon: 0.05517478773284426\n",
      "Episode: 565, Total Reward: 12.411535270780291, Epsilon: 0.05489891379418004\n",
      "Episode: 566, Total Reward: 17.662406312964205, Epsilon: 0.05462441922520914\n",
      "Episode: 567, Total Reward: 27.863922379087462, Epsilon: 0.0543512971290831\n",
      "Episode: 568, Total Reward: 3.5831174273246336, Epsilon: 0.05407954064343768\n",
      "Episode: 569, Total Reward: 19.276223193763304, Epsilon: 0.05380914294022049\n",
      "Episode: 570, Total Reward: 9.68270787428503, Epsilon: 0.05354009722551939\n",
      "Episode: 571, Total Reward: 28.908863113533883, Epsilon: 0.05327239673939179\n",
      "Episode: 572, Total Reward: 27.843320246393343, Epsilon: 0.053006034755694834\n",
      "Episode: 573, Total Reward: 27.749409027819674, Epsilon: 0.052741004581916356\n",
      "Episode: 574, Total Reward: 22.3154867237944, Epsilon: 0.052477299559006776\n",
      "Episode: 575, Total Reward: 15.531566310635, Epsilon: 0.052214913061211746\n",
      "Episode: 576, Total Reward: 13.344201605788426, Epsilon: 0.05195383849590569\n",
      "Episode: 577, Total Reward: 23.851835020699056, Epsilon: 0.05169406930342616\n",
      "Episode: 578, Total Reward: 9.562899617877022, Epsilon: 0.05143559895690903\n",
      "Episode: 579, Total Reward: 22.80999843437039, Epsilon: 0.051178420962124486\n",
      "Episode: 580, Total Reward: 6.598543612034349, Epsilon: 0.05092252885731386\n",
      "Episode: 581, Total Reward: 10.5121536569645, Epsilon: 0.05066791621302729\n",
      "Episode: 582, Total Reward: 18.560544725407514, Epsilon: 0.05041457663196215\n",
      "Episode: 583, Total Reward: 19.17648386919943, Epsilon: 0.050162503748802344\n",
      "Episode: 584, Total Reward: 14.395906512162933, Epsilon: 0.049911691230058335\n",
      "Episode: 585, Total Reward: 26.984966868970336, Epsilon: 0.04966213277390804\n",
      "Episode: 586, Total Reward: 12.514319479892244, Epsilon: 0.0494138221100385\n",
      "Episode: 587, Total Reward: 14.266546399785415, Epsilon: 0.04916675299948831\n",
      "Episode: 588, Total Reward: 26.450530120173518, Epsilon: 0.04892091923449087\n",
      "Episode: 589, Total Reward: 9.59753816340146, Epsilon: 0.04867631463831842\n",
      "Episode: 590, Total Reward: 12.861964449567362, Epsilon: 0.048432933065126825\n",
      "Episode: 591, Total Reward: 27.57567114251072, Epsilon: 0.048190768399801194\n",
      "Episode: 592, Total Reward: 21.876931821735436, Epsilon: 0.04794981455780219\n",
      "Episode: 593, Total Reward: 10.269277277342626, Epsilon: 0.04771006548501318\n",
      "Episode: 594, Total Reward: 28.24299067473024, Epsilon: 0.047471515157588115\n",
      "Episode: 595, Total Reward: 27.776808582116455, Epsilon: 0.047234157581800176\n",
      "Episode: 596, Total Reward: 28.57559134679541, Epsilon: 0.046997986793891174\n",
      "Episode: 597, Total Reward: 8.51169583847516, Epsilon: 0.04676299685992172\n",
      "Episode: 598, Total Reward: 3.912161364576896, Epsilon: 0.04652918187562211\n",
      "Episode: 599, Total Reward: 15.21267996224602, Epsilon: 0.046296535966244\n",
      "Episode: 600, Total Reward: 17.577254346175373, Epsilon: 0.046065053286412784\n",
      "Episode: 601, Total Reward: 8.098268281055264, Epsilon: 0.04583472801998072\n",
      "Episode: 602, Total Reward: 4.878690797296056, Epsilon: 0.045605554379880814\n",
      "Episode: 603, Total Reward: 12.0003506639382, Epsilon: 0.04537752660798141\n",
      "Episode: 604, Total Reward: 15.609831444137379, Epsilon: 0.0451506389749415\n",
      "Episode: 605, Total Reward: 18.517642084918254, Epsilon: 0.044924885780066794\n",
      "Episode: 606, Total Reward: 27.7427845859659, Epsilon: 0.04470026135116646\n",
      "Episode: 607, Total Reward: 26.684378072967768, Epsilon: 0.04447676004441063\n",
      "Episode: 608, Total Reward: 12.776967020654045, Epsilon: 0.04425437624418858\n",
      "Episode: 609, Total Reward: 29.47647410144116, Epsilon: 0.04403310436296763\n",
      "Episode: 610, Total Reward: 20.097813153156928, Epsilon: 0.043812938841152796\n",
      "Episode: 611, Total Reward: 2.6333333333333333, Epsilon: 0.04359387414694703\n",
      "Episode: 612, Total Reward: 20.71122959018316, Epsilon: 0.043375904776212296\n",
      "Episode: 613, Total Reward: 7.711555465058954, Epsilon: 0.043159025252331236\n",
      "Episode: 614, Total Reward: 25.652125169736543, Epsilon: 0.04294323012606958\n",
      "Episode: 615, Total Reward: 26.61011605640988, Epsilon: 0.04272851397543923\n",
      "Episode: 616, Total Reward: 8.268978070094942, Epsilon: 0.04251487140556204\n",
      "Episode: 617, Total Reward: 28.1748670734589, Epsilon: 0.04230229704853423\n",
      "Episode: 618, Total Reward: 29.14155541263064, Epsilon: 0.04209078556329156\n",
      "Episode: 619, Total Reward: 21.210279635490494, Epsilon: 0.0418803316354751\n",
      "Episode: 620, Total Reward: 23.97638824990017, Epsilon: 0.041670929977297724\n",
      "Episode: 621, Total Reward: 1.946334030134951, Epsilon: 0.04146257532741124\n",
      "Episode: 622, Total Reward: 2.5982180338942524, Epsilon: 0.04125526245077418\n",
      "Episode: 623, Total Reward: 20.111695863303535, Epsilon: 0.04104898613852031\n",
      "Episode: 624, Total Reward: 17.944585616746803, Epsilon: 0.04084374120782771\n",
      "Episode: 625, Total Reward: 9.764056055990707, Epsilon: 0.04063952250178857\n",
      "Episode: 626, Total Reward: 12.27880811024958, Epsilon: 0.04043632488927963\n",
      "Episode: 627, Total Reward: 5.94523152200858, Epsilon: 0.04023414326483323\n",
      "Episode: 628, Total Reward: 17.509293592722557, Epsilon: 0.040032972548509065\n",
      "Episode: 629, Total Reward: 27.743028074728407, Epsilon: 0.03983280768576652\n",
      "Episode: 630, Total Reward: 10.353553699740134, Epsilon: 0.03963364364733769\n",
      "Episode: 631, Total Reward: 29.127375103834567, Epsilon: 0.039435475429100995\n",
      "Episode: 632, Total Reward: 11.952017294761571, Epsilon: 0.03923829805195549\n",
      "Episode: 633, Total Reward: 20.881797230540762, Epsilon: 0.03904210656169572\n",
      "Episode: 634, Total Reward: 28.182795054651653, Epsilon: 0.03884689602888724\n",
      "Episode: 635, Total Reward: 22.34965643626917, Epsilon: 0.0386526615487428\n",
      "Episode: 636, Total Reward: 5.745828855299539, Epsilon: 0.03845939824099909\n",
      "Episode: 637, Total Reward: 25.78490945914541, Epsilon: 0.03826710124979409\n",
      "Episode: 638, Total Reward: 28.19815703841507, Epsilon: 0.038075765743545126\n",
      "Episode: 639, Total Reward: 24.05139495937671, Epsilon: 0.0378853869148274\n",
      "Episode: 640, Total Reward: 28.2112238626147, Epsilon: 0.03769595998025326\n",
      "Episode: 641, Total Reward: 25.434077314653766, Epsilon: 0.03750748018035199\n",
      "Episode: 642, Total Reward: 26.60911153608532, Epsilon: 0.037319942779450235\n",
      "Episode: 643, Total Reward: 24.515837231379948, Epsilon: 0.037133343065552986\n",
      "Episode: 644, Total Reward: 27.87521319348969, Epsilon: 0.03694767635022522\n",
      "Episode: 645, Total Reward: 14.695770616145419, Epsilon: 0.036762937968474095\n",
      "Episode: 646, Total Reward: 14.944572622125202, Epsilon: 0.03657912327863173\n",
      "Episode: 647, Total Reward: 2.8969208913237336, Epsilon: 0.036396227662238566\n",
      "Episode: 648, Total Reward: 12.699070488370962, Epsilon: 0.03621424652392737\n",
      "Episode: 649, Total Reward: 26.1515354280387, Epsilon: 0.036033175291307735\n",
      "Episode: 650, Total Reward: 28.976653048461106, Epsilon: 0.03585300941485119\n",
      "Episode: 651, Total Reward: 26.81668491604979, Epsilon: 0.035673744367776934\n",
      "Episode: 652, Total Reward: 17.577890445893797, Epsilon: 0.03549537564593805\n",
      "Episode: 653, Total Reward: 17.443827532806967, Epsilon: 0.035317898767708356\n",
      "Episode: 654, Total Reward: 7.9455072641622895, Epsilon: 0.03514130927386981\n",
      "Episode: 655, Total Reward: 28.675868940606467, Epsilon: 0.03496560272750046\n",
      "Episode: 656, Total Reward: 27.04297514103258, Epsilon: 0.03479077471386296\n",
      "Episode: 657, Total Reward: 28.476320815931196, Epsilon: 0.03461682084029365\n",
      "Episode: 658, Total Reward: 26.343121841944907, Epsilon: 0.034443736736092176\n",
      "Episode: 659, Total Reward: 29.277426923678295, Epsilon: 0.034271518052411715\n",
      "Episode: 660, Total Reward: 27.764484959588547, Epsilon: 0.034100160462149656\n",
      "Episode: 661, Total Reward: 23.853082205058143, Epsilon: 0.03392965965983891\n",
      "Episode: 662, Total Reward: 11.711910541279366, Epsilon: 0.033760011361539714\n",
      "Episode: 663, Total Reward: 24.78220522553268, Epsilon: 0.03359121130473201\n",
      "Episode: 664, Total Reward: 1.9796673634682842, Epsilon: 0.033423255248208356\n",
      "Episode: 665, Total Reward: 28.583988814676083, Epsilon: 0.03325613897196732\n",
      "Episode: 666, Total Reward: 22.953554502369677, Epsilon: 0.03308985827710748\n",
      "Episode: 667, Total Reward: 29.042654114319028, Epsilon: 0.032924408985721944\n",
      "Episode: 668, Total Reward: 15.81169580728836, Epsilon: 0.03275978694079333\n",
      "Episode: 669, Total Reward: 27.642519088154028, Epsilon: 0.032595988006089364\n",
      "Episode: 670, Total Reward: 28.64025306644566, Epsilon: 0.032433008066058915\n",
      "Episode: 671, Total Reward: 28.7426358474424, Epsilon: 0.03227084302572862\n",
      "Episode: 672, Total Reward: 11.397338020544762, Epsilon: 0.032109488810599975\n",
      "Episode: 673, Total Reward: 28.015293455864075, Epsilon: 0.031948941366546975\n",
      "Episode: 674, Total Reward: 3.9121841316266566, Epsilon: 0.03178919665971424\n",
      "Episode: 675, Total Reward: 20.526636905557393, Epsilon: 0.03163025067641567\n",
      "Episode: 676, Total Reward: 27.585790047729255, Epsilon: 0.03147209942303359\n",
      "Episode: 677, Total Reward: 12.550717023452782, Epsilon: 0.03131473892591842\n",
      "Episode: 678, Total Reward: 22.78592932830652, Epsilon: 0.031158165231288826\n",
      "Episode: 679, Total Reward: 1.9130006968016178, Epsilon: 0.03100237440513238\n",
      "Episode: 680, Total Reward: 14.262285711740072, Epsilon: 0.030847362533106718\n",
      "Episode: 681, Total Reward: 18.381224693180542, Epsilon: 0.030693125720441184\n",
      "Episode: 682, Total Reward: 25.629662366363767, Epsilon: 0.030539660091838977\n",
      "Episode: 683, Total Reward: 5.6118245353025875, Epsilon: 0.03038696179137978\n",
      "Episode: 684, Total Reward: 3.0126961034751254, Epsilon: 0.030235026982422884\n",
      "Episode: 685, Total Reward: 7.382792247866382, Epsilon: 0.030083851847510768\n",
      "Episode: 686, Total Reward: 16.029429914281806, Epsilon: 0.029933432588273214\n",
      "Episode: 687, Total Reward: 27.54328794820629, Epsilon: 0.029783765425331846\n",
      "Episode: 688, Total Reward: 19.34454280307613, Epsilon: 0.029634846598205186\n",
      "Episode: 689, Total Reward: 6.881620223207071, Epsilon: 0.02948667236521416\n",
      "Episode: 690, Total Reward: 28.60889938214509, Epsilon: 0.029339239003388088\n",
      "Episode: 691, Total Reward: 27.809063993644674, Epsilon: 0.029192542808371146\n",
      "Episode: 692, Total Reward: 20.0408668723341, Epsilon: 0.02904658009432929\n",
      "Episode: 693, Total Reward: 18.177405310262976, Epsilon: 0.028901347193857643\n",
      "Episode: 694, Total Reward: 23.416948950269163, Epsilon: 0.028756840457888354\n",
      "Episode: 695, Total Reward: 14.144421110177474, Epsilon: 0.02861305625559891\n",
      "Episode: 696, Total Reward: 12.347747365353237, Epsilon: 0.028469990974320916\n",
      "Episode: 697, Total Reward: 24.198460304523753, Epsilon: 0.02832764101944931\n",
      "Episode: 698, Total Reward: 24.152932219581622, Epsilon: 0.028186002814352063\n",
      "Episode: 699, Total Reward: 28.173777646279145, Epsilon: 0.0280450728002803\n",
      "Episode: 700, Total Reward: 16.943203811953346, Epsilon: 0.0279048474362789\n",
      "Episode: 701, Total Reward: 26.8175803496163, Epsilon: 0.027765323199097504\n",
      "Episode: 702, Total Reward: 28.676455175377324, Epsilon: 0.027626496583102015\n",
      "Episode: 703, Total Reward: 26.708681690783873, Epsilon: 0.027488364100186506\n",
      "Episode: 704, Total Reward: 26.25260394410675, Epsilon: 0.027350922279685573\n",
      "Episode: 705, Total Reward: 26.478132723427183, Epsilon: 0.027214167668287145\n",
      "Episode: 706, Total Reward: 25.277613025786913, Epsilon: 0.02707809682994571\n",
      "Episode: 707, Total Reward: 16.34577786231011, Epsilon: 0.02694270634579598\n",
      "Episode: 708, Total Reward: 23.353514313646826, Epsilon: 0.026807992814067\n",
      "Episode: 709, Total Reward: 19.478889496877922, Epsilon: 0.026673952849996664\n",
      "Episode: 710, Total Reward: 24.944888612925066, Epsilon: 0.02654058308574668\n",
      "Episode: 711, Total Reward: 24.018503904210956, Epsilon: 0.026407880170317945\n",
      "Episode: 712, Total Reward: 28.908237781483137, Epsilon: 0.026275840769466357\n",
      "Episode: 713, Total Reward: 28.17553488729183, Epsilon: 0.026144461565619025\n",
      "Episode: 714, Total Reward: 27.974985259382414, Epsilon: 0.02601373925779093\n",
      "Episode: 715, Total Reward: 29.242182078767602, Epsilon: 0.025883670561501974\n",
      "Episode: 716, Total Reward: 28.47549706476147, Epsilon: 0.025754252208694463\n",
      "Episode: 717, Total Reward: 25.512572453944276, Epsilon: 0.02562548094765099\n",
      "Episode: 718, Total Reward: 4.285874789928626, Epsilon: 0.025497353542912736\n",
      "Episode: 719, Total Reward: 3.8973432110885096, Epsilon: 0.02536986677519817\n",
      "Episode: 720, Total Reward: 22.167119189722843, Epsilon: 0.02524301744132218\n",
      "Episode: 721, Total Reward: 14.129655703409828, Epsilon: 0.025116802354115567\n",
      "Episode: 722, Total Reward: 28.909180186249746, Epsilon: 0.024991218342344988\n",
      "Episode: 723, Total Reward: 28.208735117418428, Epsilon: 0.024866262250633264\n",
      "Episode: 724, Total Reward: 23.91094103352072, Epsilon: 0.024741930939380097\n",
      "Episode: 725, Total Reward: 14.495920626504159, Epsilon: 0.024618221284683196\n",
      "Episode: 726, Total Reward: 7.679306888774883, Epsilon: 0.02449513017825978\n",
      "Episode: 727, Total Reward: 6.012640067740482, Epsilon: 0.02437265452736848\n",
      "Episode: 728, Total Reward: 10.477725906576547, Epsilon: 0.024250791254731636\n",
      "Episode: 729, Total Reward: 26.052448273337713, Epsilon: 0.024129537298457977\n",
      "Episode: 730, Total Reward: 28.54218224594069, Epsilon: 0.024008889611965685\n",
      "Episode: 731, Total Reward: 25.017556889292933, Epsilon: 0.023888845163905856\n",
      "Episode: 732, Total Reward: 28.809517405062735, Epsilon: 0.023769400938086327\n",
      "Episode: 733, Total Reward: 28.13104199498026, Epsilon: 0.023650553933395897\n",
      "Episode: 734, Total Reward: 16.92876772506644, Epsilon: 0.023532301163728918\n",
      "Episode: 735, Total Reward: 2.813992753232264, Epsilon: 0.023414639657910272\n",
      "Episode: 736, Total Reward: 17.878684614943424, Epsilon: 0.023297566459620722\n",
      "Episode: 737, Total Reward: 27.349186246625763, Epsilon: 0.023181078627322618\n",
      "Episode: 738, Total Reward: 27.44676094536169, Epsilon: 0.023065173234186005\n",
      "Episode: 739, Total Reward: 4.979307687748202, Epsilon: 0.022949847368015076\n",
      "Episode: 740, Total Reward: 28.276524795218847, Epsilon: 0.022835098131175\n",
      "Episode: 741, Total Reward: 6.563381619546497, Epsilon: 0.022720922640519125\n",
      "Episode: 742, Total Reward: 13.177988298869925, Epsilon: 0.02260731802731653\n",
      "Episode: 743, Total Reward: 12.331232545640486, Epsilon: 0.022494281437179946\n",
      "Episode: 744, Total Reward: 27.74487665810084, Epsilon: 0.022381810029994047\n",
      "Episode: 745, Total Reward: 28.47615098275722, Epsilon: 0.022269900979844076\n",
      "Episode: 746, Total Reward: 29.080098648936353, Epsilon: 0.022158551474944856\n",
      "Episode: 747, Total Reward: 12.178820328157205, Epsilon: 0.022047758717570132\n",
      "Episode: 748, Total Reward: 17.183808663724328, Epsilon: 0.021937519923982282\n",
      "Episode: 749, Total Reward: 27.475854494439098, Epsilon: 0.021827832324362372\n",
      "Episode: 750, Total Reward: 13.450291130288923, Epsilon: 0.02171869316274056\n",
      "Episode: 751, Total Reward: 7.616404652974282, Epsilon: 0.021610099696926857\n",
      "Episode: 752, Total Reward: 28.810132005349008, Epsilon: 0.021502049198442223\n",
      "Episode: 753, Total Reward: 28.943612822121864, Epsilon: 0.021394538952450012\n",
      "Episode: 754, Total Reward: 28.379133719100338, Epsilon: 0.02128756625768776\n",
      "Episode: 755, Total Reward: 18.397271407554182, Epsilon: 0.021181128426399323\n",
      "Episode: 756, Total Reward: 24.363286933206936, Epsilon: 0.021075222784267326\n",
      "Episode: 757, Total Reward: 2.7151527517185956, Epsilon: 0.020969846670345987\n",
      "Episode: 758, Total Reward: 15.251160871812969, Epsilon: 0.020864997436994256\n",
      "Episode: 759, Total Reward: 26.609335290908433, Epsilon: 0.020760672449809284\n",
      "Episode: 760, Total Reward: 26.776970014834212, Epsilon: 0.020656869087560238\n",
      "Episode: 761, Total Reward: 27.377431557689665, Epsilon: 0.020553584742122436\n",
      "Episode: 762, Total Reward: 25.29669184523795, Epsilon: 0.020450816818411825\n",
      "Episode: 763, Total Reward: 28.777881372461238, Epsilon: 0.020348562734319765\n",
      "Episode: 764, Total Reward: 23.599111639254115, Epsilon: 0.020246819920648168\n",
      "Episode: 765, Total Reward: 5.712178129353354, Epsilon: 0.020145585821044927\n",
      "Episode: 766, Total Reward: 28.41082997730843, Epsilon: 0.020044857891939702\n",
      "Episode: 767, Total Reward: 28.441540260891323, Epsilon: 0.019944633602480003\n",
      "Episode: 768, Total Reward: 25.151284837962212, Epsilon: 0.019844910434467605\n",
      "Episode: 769, Total Reward: 27.217373572158415, Epsilon: 0.019745685882295267\n",
      "Episode: 770, Total Reward: 27.476795185803475, Epsilon: 0.01964695745288379\n",
      "Episode: 771, Total Reward: 28.92733714331039, Epsilon: 0.01954872266561937\n",
      "Episode: 772, Total Reward: 23.2784976702851, Epsilon: 0.019450979052291272\n",
      "Episode: 773, Total Reward: 26.480575594671816, Epsilon: 0.019353724157029815\n",
      "Episode: 774, Total Reward: 27.845681689455972, Epsilon: 0.019256955536244666\n",
      "Episode: 775, Total Reward: 22.845643737384908, Epsilon: 0.019160670758563442\n",
      "Episode: 776, Total Reward: 28.174715191278644, Epsilon: 0.019064867404770626\n",
      "Episode: 777, Total Reward: 27.93970588763924, Epsilon: 0.018969543067746772\n",
      "Episode: 778, Total Reward: 27.143738668564556, Epsilon: 0.018874695352408037\n",
      "Episode: 779, Total Reward: 12.578372368078899, Epsilon: 0.018780321875645996\n",
      "Episode: 780, Total Reward: 29.142196334470803, Epsilon: 0.018686420266267767\n",
      "Episode: 781, Total Reward: 21.15953215364767, Epsilon: 0.018592988164936427\n",
      "Episode: 782, Total Reward: 26.777120424454687, Epsilon: 0.018500023224111744\n",
      "Episode: 783, Total Reward: 5.812028362474588, Epsilon: 0.018407523107991184\n",
      "Episode: 784, Total Reward: 24.5448887294584, Epsilon: 0.01831548549245123\n",
      "Episode: 785, Total Reward: 15.352131155597343, Epsilon: 0.018223908064988973\n",
      "Episode: 786, Total Reward: 17.877739416269154, Epsilon: 0.018132788524664028\n",
      "Episode: 787, Total Reward: 26.11769928229197, Epsilon: 0.018042124582040707\n",
      "Episode: 788, Total Reward: 25.209587532650303, Epsilon: 0.017951913959130504\n",
      "Episode: 789, Total Reward: 26.384999439426586, Epsilon: 0.01786215438933485\n",
      "Episode: 790, Total Reward: 15.095420110533697, Epsilon: 0.017772843617388175\n",
      "Episode: 791, Total Reward: 11.208987937344176, Epsilon: 0.017683979399301233\n",
      "Episode: 792, Total Reward: 17.295127075297714, Epsilon: 0.017595559502304726\n",
      "Episode: 793, Total Reward: 25.965909140501434, Epsilon: 0.0175075817047932\n",
      "Episode: 794, Total Reward: 28.342503451037533, Epsilon: 0.017420043796269234\n",
      "Episode: 795, Total Reward: 21.997854332577685, Epsilon: 0.017332943577287888\n",
      "Episode: 796, Total Reward: 28.242354127642827, Epsilon: 0.01724627885940145\n",
      "Episode: 797, Total Reward: 13.552397382089053, Epsilon: 0.017160047465104442\n",
      "Episode: 798, Total Reward: 26.15130617893941, Epsilon: 0.01707424722777892\n",
      "Episode: 799, Total Reward: 25.576516505260773, Epsilon: 0.016988875991640028\n",
      "Episode: 800, Total Reward: 26.610148103955957, Epsilon: 0.016903931611681827\n",
      "Episode: 801, Total Reward: 23.285943670783993, Epsilon: 0.01681941195362342\n",
      "Episode: 802, Total Reward: 19.72998655573291, Epsilon: 0.016735314893855303\n",
      "Episode: 803, Total Reward: 28.74265408217537, Epsilon: 0.016651638319386028\n",
      "Episode: 804, Total Reward: 12.919742302018829, Epsilon: 0.0165683801277891\n",
      "Episode: 805, Total Reward: 16.03417002606121, Epsilon: 0.016485538227150154\n",
      "Episode: 806, Total Reward: 4.012650726035844, Epsilon: 0.0164031105360144\n",
      "Episode: 807, Total Reward: 29.075272319369905, Epsilon: 0.01632109498333433\n",
      "Episode: 808, Total Reward: 27.324713306545775, Epsilon: 0.016239489508417658\n",
      "Episode: 809, Total Reward: 24.561493012264133, Epsilon: 0.01615829206087557\n",
      "Episode: 810, Total Reward: 8.796392887120051, Epsilon: 0.01607750060057119\n",
      "Episode: 811, Total Reward: 13.71107365195566, Epsilon: 0.015997113097568336\n",
      "Episode: 812, Total Reward: 4.915425169087101, Epsilon: 0.015917127532080494\n",
      "Episode: 813, Total Reward: 16.1473683162671, Epsilon: 0.01583754189442009\n",
      "Episode: 814, Total Reward: 10.062286903321677, Epsilon: 0.01575835418494799\n",
      "Episode: 815, Total Reward: 18.178373320150534, Epsilon: 0.01567956241402325\n",
      "Episode: 816, Total Reward: 14.714211910956685, Epsilon: 0.015601164601953134\n",
      "Episode: 817, Total Reward: 19.282561353778995, Epsilon: 0.015523158778943369\n",
      "Episode: 818, Total Reward: 7.529711773977925, Epsilon: 0.015445542985048652\n",
      "Episode: 819, Total Reward: 26.57441437544058, Epsilon: 0.015368315270123408\n",
      "Episode: 820, Total Reward: 27.674426489958417, Epsilon: 0.01529147369377279\n",
      "Episode: 821, Total Reward: 14.344094950124042, Epsilon: 0.015215016325303928\n",
      "Episode: 822, Total Reward: 28.608211736292215, Epsilon: 0.015138941243677408\n",
      "Episode: 823, Total Reward: 28.574565226671844, Epsilon: 0.01506324653745902\n",
      "Episode: 824, Total Reward: 28.44441728126399, Epsilon: 0.014987930304771725\n",
      "Episode: 825, Total Reward: 5.830198624774182, Epsilon: 0.014912990653247866\n",
      "Episode: 826, Total Reward: 6.54551563289951, Epsilon: 0.014838425699981627\n",
      "Episode: 827, Total Reward: 4.630199134540435, Epsilon: 0.01476423357148172\n",
      "Episode: 828, Total Reward: 9.15215080067276, Epsilon: 0.014690412403624311\n",
      "Episode: 829, Total Reward: 28.643470917321476, Epsilon: 0.01461696034160619\n",
      "Episode: 830, Total Reward: 13.243779858851845, Epsilon: 0.014543875539898159\n",
      "Episode: 831, Total Reward: 28.773774424838667, Epsilon: 0.014471156162198668\n",
      "Episode: 832, Total Reward: 18.911071115517515, Epsilon: 0.014398800381387675\n",
      "Episode: 833, Total Reward: 27.884033801909577, Epsilon: 0.014326806379480736\n",
      "Episode: 834, Total Reward: 29.074741244701805, Epsilon: 0.014255172347583332\n",
      "Episode: 835, Total Reward: 23.052588817597993, Epsilon: 0.014183896485845416\n",
      "Episode: 836, Total Reward: 13.69592076880469, Epsilon: 0.014112977003416188\n",
      "Episode: 837, Total Reward: 23.59987661498785, Epsilon: 0.014042412118399107\n",
      "Episode: 838, Total Reward: 22.918029527603657, Epsilon: 0.013972200057807112\n",
      "Episode: 839, Total Reward: 27.476165698160194, Epsilon: 0.013902339057518077\n",
      "Episode: 840, Total Reward: 9.053092247595577, Epsilon: 0.013832827362230486\n",
      "Episode: 841, Total Reward: 25.08579120042026, Epsilon: 0.013763663225419333\n",
      "Episode: 842, Total Reward: 28.74413704549551, Epsilon: 0.013694844909292236\n",
      "Episode: 843, Total Reward: 29.042013425454996, Epsilon: 0.013626370684745774\n",
      "Episode: 844, Total Reward: 11.979306717584237, Epsilon: 0.013558238831322046\n",
      "Episode: 845, Total Reward: 9.795930786695715, Epsilon: 0.013490447637165436\n",
      "Episode: 846, Total Reward: 28.94220480100336, Epsilon: 0.013422995398979608\n",
      "Episode: 847, Total Reward: 28.874542856334017, Epsilon: 0.01335588042198471\n",
      "Episode: 848, Total Reward: 12.29591811411271, Epsilon: 0.013289101019874787\n",
      "Episode: 849, Total Reward: 28.79247483288409, Epsilon: 0.013222655514775413\n",
      "Episode: 850, Total Reward: 29.542668510430754, Epsilon: 0.013156542237201536\n",
      "Episode: 851, Total Reward: 27.342301116076317, Epsilon: 0.013090759526015528\n",
      "Episode: 852, Total Reward: 15.428114556071291, Epsilon: 0.01302530572838545\n",
      "Episode: 853, Total Reward: 27.76260325519705, Epsilon: 0.012960179199743523\n",
      "Episode: 854, Total Reward: 12.749287811263367, Epsilon: 0.012895378303744804\n",
      "Episode: 855, Total Reward: 29.377419193038012, Epsilon: 0.01283090141222608\n",
      "Episode: 856, Total Reward: 6.945511271526014, Epsilon: 0.012766746905164949\n",
      "Episode: 857, Total Reward: 29.075365412886427, Epsilon: 0.012702913170639124\n",
      "Episode: 858, Total Reward: 20.61912694254637, Epsilon: 0.012639398604785928\n",
      "Episode: 859, Total Reward: 1.8976877960112224, Epsilon: 0.012576201611761997\n",
      "Episode: 860, Total Reward: 27.81785951456012, Epsilon: 0.012513320603703188\n",
      "Episode: 861, Total Reward: 26.53147522882126, Epsilon: 0.012450754000684672\n",
      "Episode: 862, Total Reward: 25.798417452450327, Epsilon: 0.012388500230681249\n",
      "Episode: 863, Total Reward: 2.8793542947697803, Epsilon: 0.012326557729527843\n",
      "Episode: 864, Total Reward: 28.55019746278927, Epsilon: 0.012264924940880204\n",
      "Episode: 865, Total Reward: 29.07599624003496, Epsilon: 0.012203600316175803\n",
      "Episode: 866, Total Reward: 5.829723666275399, Epsilon: 0.012142582314594924\n",
      "Episode: 867, Total Reward: 29.176795976798815, Epsilon: 0.01208186940302195\n",
      "Episode: 868, Total Reward: 11.046445497634508, Epsilon: 0.01202146005600684\n",
      "Episode: 869, Total Reward: 20.801203564326347, Epsilon: 0.011961352755726806\n",
      "Episode: 870, Total Reward: 22.40983162917628, Epsilon: 0.01190154599194817\n",
      "Episode: 871, Total Reward: 24.585624652739785, Epsilon: 0.01184203826198843\n",
      "Episode: 872, Total Reward: 28.742654252248073, Epsilon: 0.011782828070678488\n",
      "Episode: 873, Total Reward: 9.65261987113668, Epsilon: 0.011723913930325095\n",
      "Episode: 874, Total Reward: 27.477472630652123, Epsilon: 0.01166529436067347\n",
      "Episode: 875, Total Reward: 26.284168206327664, Epsilon: 0.011606967888870102\n",
      "Episode: 876, Total Reward: 17.553554500536734, Epsilon: 0.01154893304942575\n",
      "Episode: 877, Total Reward: 10.033251251620555, Epsilon: 0.011491188384178622\n",
      "Episode: 878, Total Reward: 1.946334030134951, Epsilon: 0.011433732442257729\n",
      "Episode: 879, Total Reward: 28.8422023358395, Epsilon: 0.01137656378004644\n",
      "Episode: 880, Total Reward: 26.94234078048134, Epsilon: 0.011319680961146208\n",
      "Episode: 881, Total Reward: 1.9796673634682842, Epsilon: 0.011263082556340478\n",
      "Episode: 882, Total Reward: 25.085009088061973, Epsilon: 0.011206767143558775\n",
      "Episode: 883, Total Reward: 20.69592060409079, Epsilon: 0.011150733307840981\n",
      "Episode: 884, Total Reward: 28.87693181708188, Epsilon: 0.011094979641301777\n",
      "Episode: 885, Total Reward: 28.477172050978925, Epsilon: 0.011039504743095268\n",
      "Episode: 886, Total Reward: 26.084835882250896, Epsilon: 0.01098430721937979\n",
      "Episode: 887, Total Reward: 29.009179586148953, Epsilon: 0.010929385683282892\n",
      "Episode: 888, Total Reward: 25.050849730859667, Epsilon: 0.010874738754866477\n",
      "Episode: 889, Total Reward: 5.679306992401456, Epsilon: 0.010820365061092144\n",
      "Episode: 890, Total Reward: 25.51927693772931, Epsilon: 0.010766263235786683\n",
      "Episode: 891, Total Reward: 28.979460853185323, Epsilon: 0.01071243191960775\n",
      "Episode: 892, Total Reward: 29.142678391189904, Epsilon: 0.010658869760009713\n",
      "Episode: 893, Total Reward: 4.945512075834562, Epsilon: 0.010605575411209664\n",
      "Episode: 894, Total Reward: 18.39403371709325, Epsilon: 0.010552547534153616\n",
      "Episode: 895, Total Reward: 14.753082295737354, Epsilon: 0.010499784796482848\n",
      "Episode: 896, Total Reward: 4.91264213946585, Epsilon: 0.010447285872500434\n",
      "Episode: 897, Total Reward: 14.098271753441804, Epsilon: 0.01039504944313793\n",
      "Episode: 898, Total Reward: 25.876668724802926, Epsilon: 0.010343074195922241\n",
      "Episode: 899, Total Reward: 5.516785418620819, Epsilon: 0.01029135882494263\n",
      "Episode: 900, Total Reward: 28.741562708419895, Epsilon: 0.010239902030817916\n",
      "Episode: 901, Total Reward: 16.564731784163722, Epsilon: 0.010188702520663827\n",
      "Episode: 902, Total Reward: 17.451102714101324, Epsilon: 0.010137759008060509\n",
      "Episode: 903, Total Reward: 7.7112210544025945, Epsilon: 0.010087070213020206\n",
      "Episode: 904, Total Reward: 22.41886340640846, Epsilon: 0.010036634861955105\n",
      "Episode: 905, Total Reward: 27.81470665585362, Epsilon: 0.00998645168764533\n",
      "Episode: 906, Total Reward: 28.296527741822093, Epsilon: 0.00998645168764533\n",
      "Episode: 907, Total Reward: 28.977125771797102, Epsilon: 0.00998645168764533\n",
      "Episode: 908, Total Reward: 3.7788507982933233, Epsilon: 0.00998645168764533\n",
      "Episode: 909, Total Reward: 28.942991032695268, Epsilon: 0.00998645168764533\n",
      "Episode: 910, Total Reward: 16.1109072125949, Epsilon: 0.00998645168764533\n",
      "Episode: 911, Total Reward: 29.543635626037386, Epsilon: 0.00998645168764533\n",
      "Episode: 912, Total Reward: 27.442821828744115, Epsilon: 0.00998645168764533\n",
      "Episode: 913, Total Reward: 27.776688243251414, Epsilon: 0.00998645168764533\n",
      "Episode: 914, Total Reward: 17.29811948312401, Epsilon: 0.00998645168764533\n",
      "Episode: 915, Total Reward: 22.853554398743107, Epsilon: 0.00998645168764533\n",
      "Episode: 916, Total Reward: 22.55266251728877, Epsilon: 0.00998645168764533\n",
      "Episode: 917, Total Reward: 28.909017979638538, Epsilon: 0.00998645168764533\n",
      "Episode: 918, Total Reward: 29.07693190461667, Epsilon: 0.00998645168764533\n",
      "Episode: 919, Total Reward: 27.8965234378836, Epsilon: 0.00998645168764533\n",
      "Episode: 920, Total Reward: 25.31280101429199, Epsilon: 0.00998645168764533\n",
      "Episode: 921, Total Reward: 28.294488684894315, Epsilon: 0.00998645168764533\n",
      "Episode: 922, Total Reward: 29.509780281353294, Epsilon: 0.00998645168764533\n",
      "Episode: 923, Total Reward: 26.98452764604956, Epsilon: 0.00998645168764533\n",
      "Episode: 924, Total Reward: 29.075858259696655, Epsilon: 0.00998645168764533\n",
      "Episode: 925, Total Reward: 27.647021988147127, Epsilon: 0.00998645168764533\n",
      "Episode: 926, Total Reward: 27.761184595694154, Epsilon: 0.00998645168764533\n",
      "Episode: 927, Total Reward: 27.61179873230676, Epsilon: 0.00998645168764533\n",
      "Episode: 928, Total Reward: 28.607280258931592, Epsilon: 0.00998645168764533\n",
      "Episode: 929, Total Reward: 4.6120336427583375, Epsilon: 0.00998645168764533\n",
      "Episode: 930, Total Reward: 28.47699806342388, Epsilon: 0.00998645168764533\n",
      "Episode: 931, Total Reward: 18.632725307601497, Epsilon: 0.00998645168764533\n",
      "Episode: 932, Total Reward: 28.963302697314315, Epsilon: 0.00998645168764533\n",
      "Episode: 933, Total Reward: 28.575687015176413, Epsilon: 0.00998645168764533\n",
      "Episode: 934, Total Reward: 16.796984671962708, Epsilon: 0.00998645168764533\n",
      "Episode: 935, Total Reward: 27.84396221925792, Epsilon: 0.00998645168764533\n",
      "Episode: 936, Total Reward: 25.76714110494138, Epsilon: 0.00998645168764533\n",
      "Episode: 937, Total Reward: 10.314480523653577, Epsilon: 0.00998645168764533\n",
      "Episode: 938, Total Reward: 20.30982667495378, Epsilon: 0.00998645168764533\n",
      "Episode: 939, Total Reward: 26.784348533373898, Epsilon: 0.00998645168764533\n",
      "Episode: 940, Total Reward: 25.718392274808036, Epsilon: 0.00998645168764533\n",
      "Episode: 941, Total Reward: 25.40792046076487, Epsilon: 0.00998645168764533\n",
      "Episode: 942, Total Reward: 27.41049996996654, Epsilon: 0.00998645168764533\n",
      "Episode: 943, Total Reward: 8.14944665898602, Epsilon: 0.00998645168764533\n",
      "Episode: 944, Total Reward: 24.552123602561178, Epsilon: 0.00998645168764533\n",
      "Episode: 945, Total Reward: 28.283644161342185, Epsilon: 0.00998645168764533\n",
      "Episode: 946, Total Reward: 6.013112269752317, Epsilon: 0.00998645168764533\n",
      "Episode: 947, Total Reward: 27.119748869112325, Epsilon: 0.00998645168764533\n",
      "Episode: 948, Total Reward: 26.57934011790143, Epsilon: 0.00998645168764533\n",
      "Episode: 949, Total Reward: 23.900167491024025, Epsilon: 0.00998645168764533\n",
      "Episode: 950, Total Reward: 29.143140763324663, Epsilon: 0.00998645168764533\n",
      "Episode: 951, Total Reward: 26.900728599911627, Epsilon: 0.00998645168764533\n",
      "Episode: 952, Total Reward: 20.061476628436335, Epsilon: 0.00998645168764533\n",
      "Episode: 953, Total Reward: 27.761732670139757, Epsilon: 0.00998645168764533\n",
      "Episode: 954, Total Reward: 28.542532254389865, Epsilon: 0.00998645168764533\n",
      "Episode: 955, Total Reward: 27.992800160508015, Epsilon: 0.00998645168764533\n",
      "Episode: 956, Total Reward: 25.852138162281157, Epsilon: 0.00998645168764533\n",
      "Episode: 957, Total Reward: 24.686887835703, Epsilon: 0.00998645168764533\n",
      "Episode: 958, Total Reward: 27.18273631862366, Epsilon: 0.00998645168764533\n",
      "Episode: 959, Total Reward: 28.59247863439813, Epsilon: 0.00998645168764533\n",
      "Episode: 960, Total Reward: 26.875195275020726, Epsilon: 0.00998645168764533\n",
      "Episode: 961, Total Reward: 17.962115356255975, Epsilon: 0.00998645168764533\n",
      "Episode: 962, Total Reward: 25.797619951731182, Epsilon: 0.00998645168764533\n",
      "Episode: 963, Total Reward: 28.43270469200678, Epsilon: 0.00998645168764533\n",
      "Episode: 964, Total Reward: 15.364422956183216, Epsilon: 0.00998645168764533\n",
      "Episode: 965, Total Reward: 8.645351103886753, Epsilon: 0.00998645168764533\n",
      "Episode: 966, Total Reward: 29.10869674623009, Epsilon: 0.00998645168764533\n",
      "Episode: 967, Total Reward: 6.245869393291105, Epsilon: 0.00998645168764533\n",
      "Episode: 968, Total Reward: 26.48421457200183, Epsilon: 0.00998645168764533\n",
      "Episode: 969, Total Reward: 22.37541802882321, Epsilon: 0.00998645168764533\n",
      "Episode: 970, Total Reward: 24.020221169008305, Epsilon: 0.00998645168764533\n",
      "Episode: 971, Total Reward: 20.177472517309948, Epsilon: 0.00998645168764533\n",
      "Episode: 972, Total Reward: 27.215010589769125, Epsilon: 0.00998645168764533\n",
      "Episode: 973, Total Reward: 28.99357005664645, Epsilon: 0.00998645168764533\n",
      "Episode: 974, Total Reward: 9.445973384667896, Epsilon: 0.00998645168764533\n",
      "Episode: 975, Total Reward: 29.47679834191504, Epsilon: 0.00998645168764533\n",
      "Episode: 976, Total Reward: 24.750738832297493, Epsilon: 0.00998645168764533\n",
      "Episode: 977, Total Reward: 28.775349507821197, Epsilon: 0.00998645168764533\n",
      "Episode: 978, Total Reward: 24.885009192652397, Epsilon: 0.00998645168764533\n",
      "Episode: 979, Total Reward: 5.845361695807921, Epsilon: 0.00998645168764533\n",
      "Episode: 980, Total Reward: 29.107405435690275, Epsilon: 0.00998645168764533\n",
      "Episode: 981, Total Reward: 22.21865780371544, Epsilon: 0.00998645168764533\n",
      "Episode: 982, Total Reward: 5.496720928509576, Epsilon: 0.00998645168764533\n",
      "Episode: 983, Total Reward: 21.143193524623108, Epsilon: 0.00998645168764533\n",
      "Episode: 984, Total Reward: 28.875959235943995, Epsilon: 0.00998645168764533\n",
      "Episode: 985, Total Reward: 29.141724308380624, Epsilon: 0.00998645168764533\n",
      "Episode: 986, Total Reward: 25.91815614999205, Epsilon: 0.00998645168764533\n",
      "Episode: 987, Total Reward: 24.718328312070426, Epsilon: 0.00998645168764533\n",
      "Episode: 988, Total Reward: 25.451987545094827, Epsilon: 0.00998645168764533\n",
      "Episode: 989, Total Reward: 14.811695819913774, Epsilon: 0.00998645168764533\n",
      "Episode: 990, Total Reward: 27.077406434462898, Epsilon: 0.00998645168764533\n",
      "Episode: 991, Total Reward: 27.845029152970255, Epsilon: 0.00998645168764533\n",
      "Episode: 992, Total Reward: 27.529348479562916, Epsilon: 0.00998645168764533\n",
      "Episode: 993, Total Reward: 4.879162629469796, Epsilon: 0.00998645168764533\n",
      "Episode: 994, Total Reward: 28.375698454268186, Epsilon: 0.00998645168764533\n",
      "Episode: 995, Total Reward: 22.267912975962194, Epsilon: 0.00998645168764533\n",
      "Episode: 996, Total Reward: 27.55020099658371, Epsilon: 0.00998645168764533\n",
      "Episode: 997, Total Reward: 18.695308465085997, Epsilon: 0.00998645168764533\n",
      "Episode: 998, Total Reward: 17.711209464712827, Epsilon: 0.00998645168764533\n",
      "Episode: 999, Total Reward: 26.347580454535652, Epsilon: 0.00998645168764533\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 27885), started 0:00:02 ago. (Use '!kill 27885' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-194bea67c63e9ef8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-194bea67c63e9ef8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir ./logs/custom_dqn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4714ee739adca912176564b3eb00229",
     "grade": false,
     "grade_id": "cell-30ac99abe97e62b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Atari Space Invaders Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf70e6e3c9fe761473c11366c91f40ff",
     "grade": false,
     "grade_id": "cell-b42bead8118e3c9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, `reset()` has returned a valid initial state as a four-tuple. The function `plot()` uses the same colour-scheme as described above, but also includes a yellow grid-square to indicate the current position of the agent.\n",
    "\n",
    "Let's make the agent go upward by using `step(1)`, then inspect the result (recall that action `1` increments the agent's vertical speed while leaving the agent's horizontal speed unchanged)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: A presentation of your results, showing how quickly and how well your agent(s) learn (i.e., improve their policies). Include informative baselines for comparison (e.g. the best possible performance, the performance of an average human, or the performance of an agent that selects actions randomly).\n",
    "\n",
    "Discussion: An evaluation of how well you solved your chosen problem.\n",
    "\n",
    "Future work: A discussion of potential future work you would complete if you had more time.\n",
    "Personal experience: A discussion of your personal experience with the project, such as difficulties or pleasant surprises you encountered while completing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G. & De Freitas, N. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.\n",
    "\n",
    "Oudeyer, P. Y., & Kaplan, F. (2007). Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(1), 26-50.\n",
    "\n",
    "Schaul, T., Hung, A., Pi-Chang, H., & Sutskever, I. (2015, December). Prioritized experience replay. In Advances in neural information processing systems (pp. 4662-4670).\n",
    "\n",
    "Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., & Wierstra, D. (2016). Dueling network architectures for deep reinforcement learning. In International conference on machine learning (pp. 1994-2003).\n",
    "\n",
    "\n",
    "- Mnih, V. et al. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533.\n",
    "- Schulman, J., et al. (2015). Trust Region Policy Optimization. *International Conference on Machine Learning (ICML)*.\n",
    "- Schulman, J., et al. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.\n",
    "- Dosovitskiy, A. et al. (2017). Learning to act by predicting the future. *International Conference on Learning Representations (ICLR)*.\n",
    "- Liang, X., et al. (2018). Deep Reinforcement Learning for Autonomous Driving. *Machine Learning Systems Workshop at NeurIPS*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
